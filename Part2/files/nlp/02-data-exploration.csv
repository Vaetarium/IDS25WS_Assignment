start,end,text
2.8,31.18,"Okay, let's get started. A very warm welcome to the second lecture. Today, as I kind of warned you, we are not going into super technical things. We are still, let's say, at the level where we are, let's say, talking about various topics. As of next week, we will go in, let's say, more concrete algorithms, like, so for example, next week you will learn..."
31.18,60.99,"how to build a decision tree, how to apply regression. After that, we go to support vector machines, et cetera, et cetera. So this is still a bit of a lecture where I'm covering multiple topics. Some of these topics you actually have to be able to do hands-on. So for example, I will explain to you how a boxplot is created. And this would be a typical example of something that you should be able to do."
60.99,89.71,"let's say, as a pen and paper exercise. But let's simply jump into it. So these are the topics that we have today. We talk a bit about visualization, but there is a later lecture that goes much deeper into this. But we mostly talk about preparing the data for analysis. And we look at basic, let's say, descriptive statistics of the data."
89.74,119.42,"So here are the topics. And when you are, for example, doing your assignments, there are many of these things that you are already applying. So, for example, you need to normalize the data. Sometimes you need to use things like binning. Sometimes you need to make sure that your data is balanced. So many practical things we will talk about today that you..."
119.42,148.86,"I'm sure you will recognize that if you do the assignments that you actually need them. Let's continue a bit where we, let's say, stopped yesterday, or I should say Monday. So it starts with data extraction. I explained you what the difference is between a data warehouse and a data lake. I hope you remember the bottles of water. But one of the main things, one of the main challenges is to extract data from existing systems."
148.86,175.68,"And like in this course, we cannot give you really that experience, how that feels in real life. So in the last lecture, I asked you how many tables SAP had, and then I think somebody said 10,000, and it is actually 800,000, right? So these things are much more complicated than you think. Then we talked about the different types of data."
175.68,204.78,"So we will mostly talk about structured data, and we will talk just a bit about unstructured data. Of unstructured data, the thing that we will talk about most is text, right? How to analyze text there. But most of our data will be structured, and I introduced, let's say, this taxonomy in the lecture on Monday, and I briefly repeat it with some examples."
204.78,234.69,"So we have categorical data that is nominal. That means that there is no natural order. So this could be true or false. This could be, I don't know, color. This could be gender. This could be, I don't know, a breed of dogs. And so dogs are not the... So if I have, let's say, one type of dog or another type of dog, there is not a natural ordering there. So that is nominal data."
234.69,261.7,"If we look at ordinal data, that's also categorical data, so not a number, but there is a very clear order. So, I don't know, I'm very happy, I'm happy, I'm sad, I'm depressed, or something like that, right? That could be like a scale where all of you are able to order the words that I'm talking about. So that is, let's say,"
261.7,290.45,"ordinal data, categorical data that has an order. And for example, I think some of you are familiar with the Likert scale. That's a scale that is typically being used if you do questionnaires or something like that. So here for various types of questions, you map it onto five values. So for example, if you look at the first row here, for example, frequency ranges from never to always."
290.45,318.4,"And let's say, I think everybody will agree that rarely is in between never and sometimes, and often is in between sometimes and always. And I can do that for all of these things. So it's not a number. They are still categorical, but there is a clear order related to this. Then we go to numerical data. If we look at continuous data,"
318.4,345.65,"we distinguish between interval-scaled and ratio-scaled. And as I explained on Monday, the difference that you should check whether it belongs to one type or the other one is can you say in a meaningful way twice as much, twice as fast, twice as, I don't know, twice the distance or something like that."
345.65,374.59,"And if you are able to say two times in a meaningful way, then that typically means that there is a zero that has a meaning. Right? So I can say, I don't know, one bag is two times as heavy as another bag. I can say that because it's ratio scaled. That means that there is a clear zero there. Right? So that's the thing that you..."
374.59,404.94,"should bear it in mind. So I hope that this is clear. Then we typically look at, as I said, tabular data, right, where the rows correspond to instances and the columns are corresponding to features. We will also talk a bit about text. We will also talk a bit about, let's say, for example, when we talk about neural networks, we will talk a bit about images, but we'll not go very deep."
404.94,431.09,"In most of the cases, we look at tables that look a bit like this. So our input is we have features. These are the columns. And we have instances. These are the rows. And we have seen you can use many other terms. So sometimes people talk about things like a response variable, et cetera, et cetera. But we have features and instances."
431.09,461.3,"And if we are interested in a very particular feature, we call that the target feature. The feature that we would like to understand. So what you will see is that there are special features like the target feature. Right? And if you talk about the target feature, you typically try to understand the target feature in terms of the other features. So for example, I don't know, if you look at the credit card application process,"
461.3,490.88,"The outcome can be that your application is accepted or rejected. And you may learn, for example, a decision tree explaining under which circumstances typically a credit card gets rejected or accepted. Another special feature that you often see is time. Most of the data that we look at has somehow a timestamp. Something has happened. And time is very special."
490.88,520.46,"So you will have many lectures, perhaps roughly one-third of the lectures that you will see, is where we look at data science questions that involve time. So think about things like time series, but we also talk about things like sequence mining, process discovery, conformance checking. These are all things where data has a temporal dimension. And that's hopefully clear to you that that is very important."
521.04,550.88,"Visualization is very important. So who has seen this example before? That is surprising, right? So this is a data set that is very famous. It's called ANSCOMS Quartet. And this is an example where we have four data sets, right? Indicated in the different columns."
550.88,579.49,"And from a statistical point of view, if we would look at descriptive statistics, they are identical. And let me explain what I mean by identical. If you would look at the average value of x, so here you have one data set, here you have another data set, another data set, another data set, the average of these x values is precisely 9. And it's very difficult now for you to check it for these numbers."
579.49,607.7,"But for example, all of these values are, there are 10 values equal to 8, and there is one value equal to 90. And if you then compute, you will see, yes, the average here is precisely 9. So the average of x is identical. If we look at the average of y, it is also identical. It's approximately 0.5. So if we sum up all these y values,"
607.7,637.71,"then in all four data sets it's exactly the same. Then we can look at things like variance or standard deviation. Then all of these data sets are identical. The variance of X and Y in all of these data sets are the same. Next week you will learn regression. If we apply regression to these data sets where we try to explain Y in terms of X, the result is the same."
638.03,667.5,You will find the relationship between these two. You can also look at correlation. It's the same in all of these three data sets. So many of the standard statistical tests will show these are identical. But now I visualize it. And now you see the four data sets. There are 11 dots for all the different instances. They have an x-coordinate and a y-coordinate. And you can see that they are very different.
667.95,695.44,"This is a small, tiny example showing that if you use visualization, you typically see completely different things than if you are trying to analyze using something, using an algorithm or standard statistics. So that is why later we will talk much more about visualization and we briefly touch it in this lecture. This is another very famous example."
695.44,725.25,"It's Charles Minyard's map of Napoleon's army. As you know, Napoleon tried to conquer, let's say, the Soviet Union. So he started with a very big army in France, and then moved to Moscow, and then they retreated. And if you look at this visualization, there are several things that you can see. So the width..."
725.25,754.7,"of this, I'm not sure how to call it, the width of this pvaat corresponds to how many soldiers are still alive, right? So we start with over 400,000, and then it goes here to Moscow, and then there is a retreat, and in the end only 10,000 soldiers survived, right? And this is visualized by the width of this thing."
754.7,779.92,"You can see the geographic location. The color is indicating the direction. So this beige color is moving towards Moscow. The black color is, let's say, retreating. And what you see here is the temperature when retreating. Look at when this was created. It is very old."
779.92,809.52,"And this is often used as an example because it's one of the first examples of information visualization. And now you should think how many numbers are on this chart. And if you would map that onto a table with numbers, you would need pages and pages and pages to collect the information that you see here, which is kind of compactly represented in a single diagram. And that's the goal."
810.19,839.62,"This is a so-called dotted chart, which is a standard way of, if you look at event data and you have no idea what is in your data set, that you first use it to visualize it. It's very simple. Oops. It's very simple, so on the x-axis you have time. On the y-axis you have the cases. And every dot is corresponding to an event. And the color of the dot here is..."
839.62,868.19,"indicating what type of event has happened. And what you can see here, which you would never see if you would look just at the data or you would look at the process model or something like that, is that you can see that the arrival process is not constant. It's almost constant, but it's not a completely straight line. And you can see periods where there are these gaps of inactivity. So there are certain times where nothing is happening."
868.19,895.81,"And there are points in time where, let's say, just the red and the other color dots are happening, but the light green is kind of missing. It's again something, if you would try to look at statistics, you would look at models, you would not be able to see it. But as a human, you immediately see, we need to investigate this period and this period. So we did similar visualizations."
895.81,919.94,"For example, for the patients lying in the ICU unit here in Klinikum during the COVID period. And then you can visualize the first COVID vella. I'm not sure how to translate it. And the second one. And then you could see that there were clear differences just by looking at the data. This is another example."
919.94,949.68,"What you often see that you don't see is, for example, here you see batching. So cases have been waiting for a very long time. And then on one day, for all of the cases, the final activity is happening. This is the power of visualization. Something that we will come back to later. Let's go more down to earth. And we first start looking at one feature. And later we will look at, let's say, multiple features. And then we also look at things like correlation and covariance."
949.68,978.75,"But let's start with the single feature. So we have instances, we have a bunch of features, and now we would like to characterize one single feature. And as I told in the lecture on Monday, there are a couple of standard books that we are using as a reference. You don't need to buy them, you don't need to read them, but if you would like to go deeper, like I always try to provide the references,"
978.75,1007.86,"And these tables are taken from that book. And if you're interested, you can read about these cases and see more about it. So there are continuous features and categorical features. And these tables are basically summarizing what is in the data. Right? From a statistical point of view, without showing the data. So if we look at continuous features, so we now look at one column."
1007.86,1037.3,"And the column is a number. And if you have a number, then of course you can talk about the number of rows, the number of instances. Some of the rows may not have a value. Very important, right? If you look at things like data quality. So you see what is the percentage of missing, let's say, values. You can see the number of unique values. You will see the minimum, the maximum, the mean."
1037.3,1061.42,"the median, and the standard deviation. So if you have numbers, these are the typical things that you can do. What is the largest one? What is the smallest one? What is the average one? Et cetera, et cetera. So the thing that I would like to talk about, and later when I talk about box plots, you can see why I'm, let's say, elaborating on this now. The mean everybody..."
1061.42,1089.25,"hopefully it knows how you compute it, you just sum up the values and then you divide it by the number of instances. The median you do by sorting all of your values from small to large, and you try to pick the one that is in the middle. So the median can be very different from the mean. So for example, if you look at income, the mean income is very different from the median income."
1089.25,1118.1,"So informally, this is the middle value. And if I have an uneven data set, so for example, I have this data set with five values. It's very easy because it's uneven. There is precisely one in the middle. And I can simply say the median is two. If I have an even number of instances, like in this example, I have four."
1118.35,1148.1,"This is not uneven, so there is not a single value in the middle. There are two values in the middle. And you take the average of these two values. So you have 2 and 3 are in the middle, not a single value. And you take 2 plus 3 divided by 2. So you get 2 and a half. Easy, right? If we go to quartiles, and I will explain in a minute what this is."
1148.1,1178.13,"we are basically applying this computation of the mean in two ways, like a divide and conquer strategy. So, median was the middle value. I just explained it. If we are dealing with a continuous distribution, so think of this as a kind of histogram, but then in a continuous sense, then the middle value, you can also..."
1178.13,1203.15,"approximate this by saying I want to have the largest values, the largest value of the smallest 50% of the values. So if I'm interested in median income, I look at the poorest half of the population and I look at of the poorest half of the population who earns most."
1203.15,1231.44,"If I look at, I could also define that as the smallest of the largest 50%. So I look at the half of the population that is earning the most, and of, let's say, this half, I take the poorest person. And if you have enough data, then all of these things become the same. If it's continuous, then you can think of it like this. So this is this."
1231.44,1258.42,"Next to talking about the median, you can also talk about, let's say, the poorest 25% and the richest 25%. And then next to, let's say, this M value, which corresponds here to Q2, you get Q1 and Q3. And again, I can define it in the similar way, in an approximated way. So if we look at this value, Q1,"
1258.42,1288.91,"That is the largest of the smallest 25%. So I look at the poorest 25% of the population, and I pick the person that has the highest income. And this is equivalent to, if you have enough data, to the smallest of the largest, 75%. Do you see that? That's the way to do it. So what you see is that already with the middle value that's kind of ill-defined,"
1288.91,1316.42,"If it's even, you need to take the average of two values. And of course, with these things, you have all kinds of problems. So if I talk about the poorest 25% of the population, if the number of people is not dividable by four, it's ill-defined what a quarter is. So that's why you need to approximate this."
1316.42,1346.42,"And we basically compute the medium recursively. And it's easy to show that using an example. So now I'm interested in the first quartile, the median, and the third quartile. I have these values. Clearly, I cannot divide this into four equal parts. I have five instances, and I cannot divide that, let's say, in four parts. But I first compute the median. That's the two."
1346.45,1376.14,"And because there is a single value in the middle, I leave it out and I split the rest of the data in the two remaining sets. They have an equal size because of this. And to these two sets, 1,2 and 3,3, I apply the same approach. I now look at what is the median of 1 and 2. And I look at what is the median of 3 and 3."
1376.43,1406.4,And I get these values. So quartile 1 is 1.5. The median is 2. Quartile 3 is equal to 3. So here I took an example where my number of instances is uneven. I can also take an example where the number of instances is even. Now there are two values in the middle. And what I do is that I now again compute what the median is. It's 2.5.
1406.4,1433.65,"the average of these two numbers. But now I do not delete a middle value because there are two. I just keep the two and I keep the three, if you see what I mean. Now, again, I have two data sets, and in that way, again, I compute the median of these two remaining data sets, and I get this. This is a simpler example. If I take a larger example,"
1433.65,1460.74,"So here we have an uneven number of instances sorted. There is one in the middle. I leave it out. That is the median. Then I have two datasets. I again compute the median of these two datasets that remain. And I take the middle value of this there. If there are two middle values, I take the average. So I get this. This is where this is uneven."
1460.74,1489.89,"If I look at the data set where it is even, then I have two values in the middle. I compute the median as 3.5, the average of 3 and 4, and I split it into these equal data sets. So I keep the 3 and I keep the 4. Again, I apply it, and these are the numbers that are there. Clear? You may wonder why I'm telling you this in a lot of detail. If you create a box plot..."
1489.89,1533.1,"The key information is Q1 and Q2, right? So that is something that you need later if you are computing a box plot. So any questions here? Is it clear? Yeah? Sorry, yeah, I see it. I see it. Yeah, yeah. That's clear. Yeah. So this is the smallest of the largest 25% and the largest of the smallest 75%, right? That's what you... No, no."
1533.1,1567.89,"It is the smallest of the largest 25%. So you look at the people that have the highest income, the 25%, and of that I take the poorest person. Largest and smallest are swapped. It is correct. It just should say 75%. Yeah? Okay? Yeah? Other questions?"
1569.68,1599.6,"Now we go to categorical features. Of course, we cannot compute things like the minimum and the maximum, etc. All of these things do not exist. We can still talk about the number of instances, the percentage missing. We can talk about the number of unique values. That includes the value missing. If there are empty entries, that also counts as a value here. Then we look at the most common value."
1599.6,1624.06,"the most common non-missing value, the frequency and the percentage of this most common value. And this is the second mode that is the second most common value that you see here. In a minute I will show an example so that you can see it. So this is a table that comes from the book."
1624.06,1653.52,"This is kind of representative for, let's say, real-life data. So you can see, for example, there are entries with missing information. There are also entries with very suspicious values. So, for example, here you see a claim amount minus 99,000 something, right? So this is what you look at as you look at real data. And then the first thing that you do is that you start creating these descriptive statistics."
1653.52,1680.94,"And then you often already see some issues. So here I'm zooming in. And we can now split the features, so the columns, in categorical and non-categorical. And for categorical features, we give different types of descriptive statistics as for, let's say, the numerical ones. So this is for numerical ones."
1681.42,1710.69,"So for example, if you look at income, there are 500 values. Nothing is missing. There are 171 unique values. The minimum is zero. The first quartile is zero. The mean is that. The median is this. So this is very, it immediately shows an example that median and mean can be very different from each other. This is the third quartile. This is the maximum value, et cetera."
1710.69,1738.91,"So these are the statistics that you see here. What you see here, probably very suspicious, is that income is often zero. Is that a missing value? Or is that really indicating a value that we can use? We can look at categorical features. So for example, if you look at the first column, that is not so interesting. Because for all 500 values,"
1738.91,1763.44,"for all 500 instances is exactly the same value. So the most frequent value is the same for all. If we look at marital status, we can see that 61% of the values are missing. You can kind of see that here. There are four possible values, including the missing values."
1764.24,1795.36,"Of the non-missing values, the most frequent one is married. There is something weird here. That's because this is based on only considering the values that are non-missing. That's why the numbers look odd. So the most frequent value is married. The second most frequent value is single. You can look at injury types. There are four possible values. And you can see that broken limb."
1795.36,1829.01,"is the most frequent one. Just basic overviews that you can generate to show this. Now let's take a look at some visualizations. So this is showing exactly the same data as I just indicated. We are looking at categorical variables, and we look at how they are, let's say, distributed. And you can see, for example, for this feature, most of the things are missing. If you have continuous"
1829.01,1854.66,"variables, then the only way that you can plot them is that you use binning. And binning is a topic that I will also discuss in a lot of detail towards the end of this lecture. So if you have continuous data, it could be that every value is unique and you would not see anything. But you want to see the distribution, and for that you need to use binning."
1854.66,1883.66,"And here you can see the effect of whether you are using many or few bins. Later I will explain to you how this is being done. But in all of the cases, the underlying data is exactly the same. Here I have three bins, 14 bins, 60 bins. And probably here 14 would be the best value because it's still smooth. What you see here is that you easily get outliers."
1883.66,1912.91,"that are very far away. As I said, the topic that we will go back into detail. Then what kind of distributions could this be? If we look at data, we do not really know the underlying distribution. We can only visualize the examples that we have seen. If it looks a bit like this, then it's probably uniform. If it looks a bit like this, it's probably normal. It can also be that it's skewed."
1912.91,1941.46,"to one or the other direction. If things are exponential, it looks like this. So small values are very likely. Larger values are less likely. And you can have, it's called multimodal, where your distribution has multiple peaks. You can all imagine if you plot the data that I was just talking about, that you will get graphs like this."
1941.46,1968.14,"Let's say some repetition of basic statistics, right? A uniform distribution plays a very big role in many parts of this course. Why is this the case? If you, for example, look at the law of large numbers, right? If you add up many independent things, you automatically get a normal distribution, right? So it's the same as that you automatically get an exponential distribution."
1968.14,1998.03,"If you have many, let's say, actors that independent of each other do something, independent in steady state, then the time between two subsequent arrivals is negatively exponentially distributed. So these are distributions that are very natural, and that's why you often see them in the field of data science. So I assume that you know what a normal distribution looks like. So here the standard deviation is constant."
1998.03,2028.35,"Here I'm shifting the mu. Here I'm keeping the mu equal to zero, so the average, and I'm playing with the variance. The bigger my sigma, the more flat it will be. Why do we often look at a non-normal distribution that is if you add up many numbers and you average them, you kind of automatically get..."
2028.35,2056.4,"a normal distribution. So that's why it is very important. And we know a lot of things about a normal distribution. So for example, there are these standard, let's say, things that you can often apply in practice, that if something has a normal distribution, that 68% falls between, let's say, the mean minus the standard deviation and the mean plus the standard deviation."
2056.88,2085.46,"It's something that is very useful. If I then look into two sigma on both sides, it's 95%. And if I look at three sigma in both directions, we are already capturing 99.7%. And if you're doing statistical tests, these are typical things that are being used in the background for this."
2085.49,2114.67,"Try to relate this to things like Six Sigma. I will talk about this in a minute. But often you want your process, think of it as quality control or response times. So you can think of any key performance indicator. You want things to be within certain tolerances. So if you, I don't know, buy a car, then things are being tested, parts of the engine, for example."
2114.67,2143.25,"And some tolerances are allowed, but it has to be within certain limits. And if it's outside of these limits, it is considered to be a faulty product. And you can also apply this to time. For example, I don't know, you want to treat patients within two weeks after diagnosis, then you can create a distribution. And you want to make sure that, let's say, only a small percentage."
2143.25,2170.61,needs more than these two weeks before they start treatment. So you can apply this to any performance indicator. And there is something like there can be a lower specification limit and an upper specification limit. And you want your process to be within these two bounds. And we know that if these bounds correspond to
2170.61,2196.03,"to minus 2 times sigma and mu plus 2 times sigma, we know that 95% is in this category and 5% is outside. That's how we would like to control this system. One can also look at this one-sided. Think of this as response times or something like that."
2196.03,2223.55,"And there is a certain, so things need to happen before a certain deadline. If that coincides with mu times 2 times sigma, then we know that in 2.5% of the examples we will take too much time. And then we need to decide is this acceptable or not. So in quality control, you try to find out what the distribution is."
2223.55,2253.82,"And then you accept that certain instances will be outside of this distribution. So hopefully some of you have heard about Six Sigma. There are like an industry that is typically a term that's being used. And people are saying, for example, my process is running at Six Sigma. And when people say that, in most of the cases they do not have, although they say it, they have no idea what it means."
2253.82,2281.1,"What they know what it means that if things run at Six Sigma, that basically means that just once in, let's say, 3.4 million, you have a deviation. So running in Six Sigma is considered to be very good. You have just a few errors. I don't know, a few chips that do not function or a few..."
2281.1,2310.75,"patients that are not treated properly or whatever, it's just one in 3.4 million. And as I said, people that talk about this, they have no idea how this number is being computed. And I will take a modest attempt to explain to you how this is. But it's very, let's say, baroque, very weird how this is defined. So six sigma, the six sigma is referring to"
2310.75,2340.34,"the standard deviation of the normal distribution. So it corresponds to say, okay, let's set the limit to the average plus six sigma and the average minus six sigma. Right? That is what we set as bounds. And then you would say, okay, if you run at six sigma, you look at what is the probability that you are here at the right side of this line or you are here at the left side of this line. Right?"
2340.34,2370.85,"That's something that you would think. But if you compute that, then you will see this doesn't correspond to this number. That corresponds to this number. What you see here in green, that's 2 times 10 to the power minus 9. And that is a much smaller likelihood than the likelihood that you see here. So what is the interpretation of the people that invented this?"
2370.85,2400.21,"The interpretation is that this mean can shift one and a half, right? And that's why you can see, let's say, this distribution and this distribution. And with this shift, that determines, let's say, what its value is. And if you apply to this shift in one direction or the other direction, then one direction essentially becomes zero, and the other direction becomes much bigger."
2400.21,2429.66,"So if you take this red one, for example, then that corresponds to this 3.4 times 10 to the power minus 6. So if people talk about 6 sigma, there is this reference to this statistical distribution, but there is also some, let's say, weird design choices that are behind it. I'm just telling you, because you..."
2429.66,2459.31,"You may often hear this term if you're doing data science in industry. And also to start you making thinking about distributions and standard deviation and all of these things that they actually are important in this. So Six Sigma corresponds to running, let's say, in this safe way. And this is what I just said. Data quality."
2459.31,2487.73,"So there are often data quality problems. You just saw the table that I indicated to you. Things that you saw on the table is sometimes the claimed amount is minus 99,000, right? That's probably not a value that you should use when you're learning a model. We also saw that income is most of the cases zero. It's probably also not a good thing to use, right?"
2487.73,2514.61,"You often need to inspect the data and then see what kind of quality issues there are. So things may be incomplete. There may be values that are impossible. So for example, dates that do not exist. There may be values that are conflicting, right, that could never exist together. Things may be imprecise, outdated, et cetera, et cetera."
2514.86,2544.74,"We briefly talk about different ways of addressing these issues. Right? And they are listed here. And I will show, let's say, you're using a table. Did this in more detail. So this is a table. Standard tabular data. And the red cells are missing values. So this could be missing values. But this could also be extreme values that you do not trust. Like the..."
2544.74,2574.4,"minus 99,000, that I said earlier. So I mark which cells I consider to be missing or being outliers in some way, and then I need to make a decision. I now need to feed this data to my machine learning algorithm. What do I do? One thing that you could do is that if, let's say, for a particular feature,"
2574.4,2604.5,"some of the values are missing, you simply don't use it. So in the earlier example where you could see that most of the income was zero, probably is not good to use that. If these are actual missing values, you would get, let's say, very funny results probably. So you can remove a feature altogether, which you typically do if many values are missing."
2605.81,2636.26,"Then there is the, so what I should stress, if a value is missing, it is typically not independent of the other features, right? So like in many examples, if I'm interested in a particular target feature, the missing values can still correlate to the target feature. And if I would leave it out, I would basically bias my data."
2636.26,2664.83,"towards a particular group. That is not something you should do. You can also remove records or instances if values are missing. So here I'm focusing on feature two, and I remove all the instances that do not have feature two. Here feature four, the same idea. If I look at feature seven, I would use everything. You could also remove..."
2664.83,2692.98,"instances completely if one of the values is missing. You can also make up values for the entries that are missing. So for example, if the income value is missing, you can simply insert the average income. So you can insert values that you kind of make up."
2693.01,2723.18,"And how good that works, of course, depends how good your estimate is. What we will see later is that we often use, let's say, models again to attack quality problems and to fill in the missing values. There are, of course, also values that you know that cannot exist. It's very easy if you talk about, let's say, dates. Dates are typically manually entered."
2723.18,2752.75,"You often see that in dates there are problems. So you know that these values, although they are there, that you cannot use them, right? Because they do not exist. Colors that do not exist. Natural numbers that are, let's say, fractions, et cetera, et cetera. Then you need to address this. As I said, if you look at dates,"
2752.75,2782.8,"And that is something that I will come back to, let's say, in one of the later lectures. What you typically see is that with manual data entry, day and month is being swapped, right? And if you look at the distribution, in many data sets, you can see that something is incorrect there, right? And that days earlier in the month have a higher likelihood, right? And dates later in the month because of the swapping."
2782.8,2811.2,"These things are very similar to missing values. You get the data value and you cannot use it, and you need to deal with it in some way. Sometimes you are sure, like the data example that I gave, but sometimes, I don't know, if a student is 123 years old,"
2811.2,2842.58,"That could be the case, but it is probably like an incorrect value. That is there. This brings me to the box plots that I talked about earlier. That's a way of visualizing the distribution. And mind you, we are still talking just about a single feature. So we are still very basic. A box plot is showing this. This is a standard visualization."
2842.58,2872.8,"that you will see everywhere, and that, let's say, was invented by John Tuckey in his famous book, and here you can see some examples. And you should be able to compute a box plot, right? Remember the earlier examples that I gave for computing the median, computing the first quartile, computing the third quartile? You need this, what I talked about earlier,"
2872.8,2904.22,"to show a box plot. So a box plot again is very compact but it has quite a bit of information. So there is this vertical bar and that is not the average, it is the mean. Then there is the top of this box and the bottom of this box that is corresponding to the third quartile and the first quartile. The things that we have seen before. Then there is an upper whisker and a lower."
2904.22,2932.48,And they correspond to actual values. So there is a data point that is here. There are also probably data points here. There is a concrete data point here. And these dots that I show here are considered to be outliers. And how is this being defined? And now the kind of ad hoc thing comes. You look at the...
2932.48,2963.7,this value that you have here and you know how big this box is and you take one and a half times the height of this box you add that here and then you get what is called an upper fence and you look at the largest value below the upper fence the same at the bottom whisker so you know this box you take one and a half the height of this box
2963.7,2992.53,"you go that amount below, and then you look at the smallest value inside this area. So what you can conclude here, and I will explain it again in a bit more detail and also with examples, but just for you to get an overview, the fact that there is here this whisker means that there are no values here."
2992.59,3023.1,"The fact that there are no circles means that there are no, let's say, outliers below this situation. But there are these two outliers here. And there can be values here, but we know that we still have a value here that is within this third quartile plus one and a half times the width of this box. So let me again repeat."
3023.1,3052.62,"Slowly get it. You know what the medium is. You know what the first quartile is. You know what the third quartile is. We have shown you how to compute that. Now I can take the difference between the first quartile and the third quartile, and that is called interquartial range, IQR. So half of my instances are within this blue box that you see here."
3052.62,3081.23,"So this is half, right? That's like the center of my data. And what I now do is I take this third quartile and I add one and a half times the height of this box. And then I get an upper fence. I get a lower fence by taking a look at the first quartile, doing minus one and a half times this interquartile range, and then I get a lower fence."
3081.94,3110.7,"Note that this one and a half is kind of arbitrary, but that's like what is done in standard literature. Just like the six sigma, that there was a one and a half shift, that's just based on experience. That seems to be a meaningful value. So then you have the lower fence and the upper fence. And now below the upper fence, you look at the largest value below this threshold."
3111.44,3139.49,"And that's the upper whisker. Here you look at the smallest value above the lower fence. So that means that there is no data here, and there is no data here between the whisker and the upper fence. All other points I draw as explicit dots, and they are the outliers. They are further away."
3139.49,3167.06,"then one and a half times this interquartal range above the, for example, third quartile. Clear? So this is the thing that you need to do. So what I'm describing here is completely deterministic. It's completely well defined, but it depends on the first quartile and the third quartile, which is not so clear, and that's why I explained that earlier."
3167.31,3192.88,"So as I said, we have the earlier challenges, so the data may not be dividable by 4, etc., etc. So that's why we use the convention that I explained to you earlier. Let's take an example. So these plots are generated using Excel. It's something that you can simply do, like a column with numbers, and then you can ask Excel to draw a box plot."
3194.19,3224.5,"How is this box plot now being computed? So I have these values here. If you count properly, you will see that there are 11 instances. So that means that we have a value in the middle. That is our median. So the median is here at 5. And we can see that we split this into these two data sets. Again, we look at the middle value. So we get Q1 is 2."
3224.5,3252.67,"Q3 is 8. So now we know how high is this box. It is 8 minus 2 is equal to 6. So now 2 and 8 are, let's say, corresponding to this and this. Q1 and Q3. And we know that our IQR is 8 minus 2 is equal to 6. The medium is 5. So the upper fence..."
3252.67,3283.92,is 8 plus 1.5 times 6 is 17. Our lower fence is 2 minus 1.5 times 6 is minus 7. Right? So these are the fences. And now again I look at what is now the largest value below the upper fence and what is the smallest value above the lower fence. And then you will see that in this example this is corresponding to 10 and 0.
3283.92,3312.1,"So note that these whiskers are very far away from the upper and lower fence. And that's why we do not see any outliers here. There is no outlier. Everything is within this upper fence and this lower fence. Yeah? So if I now modify it a bit to force it to create an outlier, this is still the same, still 11 values."
3312.1,3340.53,"But now this value is not 10, it is equal to 20. If we compute Q1 and Q3, it is exactly the same. It is not influenced by this 20 that I have here. But what you now see, 20 becomes this outlier. So how does this work? Again, if you look at the upper fence and the lower fence, because Q1 and Q3 did not change because of the 20,"
3340.66,3386.58,"The upper and the lower fence are still exactly the same. But what you now see is that this value, this 20, is above the upper fence. And that's why it is considered to be an outlier and drawn separately. So this is the way that it works. I can show another example. So now you have... Whisker. Whisker."
3386.77,3422.4,So here you can see that the upper fence is 17. That's somewhere here. And we know that there are no values between this value and the upper fence. Here the lower fence is minus 7. But there are no values between 0 and minus 7. And that's why the whiskers are corresponding to actual values. You look puzzled. So first you compute the fence.
3422.4,3451.89,"It's completely defined based on this IRQ and the Q1 and the Q2. Then you get these bounds and within these bounds you start looking for concrete values that exist. Between this 2 here, between 2 and minus 7, we look for concrete values."
3451.89,3486.93,"And zero is the smallest of these values. If there would have been a minus two, right, this whisker would have been as minus two. Sorry? We are trying to give a one-shot visualization of what the distribution is. So what we can now see is, so where we were successful in is showing that all of our data is between zero and nine."
3487.44,3519.81,"All of our data is within 0 and 9, except this outlier. And if we look at the values that are between 0 and 9, half of them are in this blue box. That's a goal. This is another thing. Here I have multiple 8's in the middle. Like again, I use the way of computation that we had before. So Q1 is 6."
3519.81,3547.98,Q3 is 9. Note that the values are now completely different. I compute what is the IQR. That is 9 minus 6 is equal to 3. So now the upper fence is equal to 9 plus 1.5 times 3 is 13.5. And 6 minus 1.5 times 3 is equal to 1.5.
3548.14,3579.01,"So these are the fences, the things that you do not see. But now we look at the smallest value above 0.5. The smallest value above 0.5. And this is this two. That's why this whisker is here. We also look at the upper fence. So the upper fence is 13.5. So we look at the largest value below 13.5. That's equal to 10. And that's why it is here. And here..."
3579.01,3604.56,"We see that there is just one outlier and this is the value zero. Hopefully it explains things. Yeah, clear? So you have now seen, although our examples are very small, that this is a way to detect outliers. So these are the dots that we draw separately."
3604.59,3633.73,"And you can think of an outlier. So this minus 99,000 would appear here definitely as an outlier. And once you have detected an outlier, you need to deal with it. And you can treat it as a missing value by removing, for example, the instance. Or you can kind of change it into another value. So if I visualize this, here we see the outliers."
3633.73,3658.19,"above the upper fence and the things below the lower fence. And we can either simply remove these instances. We do not trust them. The value is too way off. Or we somehow try to move them into, let's say, a value that is somehow in the distribution that we would like to see."
3658.54,3692.64,"So instead of moving it here, you could also move it to, for example, the average or the median value, if you want. Yeah? Clear? I need to make sure that I keep speed, because I think I will have a problem. So let's take a look at two features. We just looked at one feature. Now we look at two features, where we also start looking at things like correlation. Again, we have a data set. We have two numerical..."
3692.64,3723.07,"features, and I think everybody has an intuitive feeling of what is correlation, positive correlation, negative correlation, no correlation. Here is the data set that you have seen in the book. So what you see here is, for example, you have height and weight, and you can look at is there a correlation between these two values. If you create a so-called scatter plot,"
3723.07,3753.47,"you will see this. So every dot is corresponding to an instance, and you just plot it with an X and a Y coordinate based on the data. And what you can clearly see, hopefully, is that there is a clear positive trend here. That means that there is a positive correlation. If you look at age and sponsorship earnings, you can see that there is a negative correlation. So if these basketball players are older, they typically get"
3753.47,3783.73,"a smaller sponsorship. You can also look at age and height, and then there is not a very strong correlation, perhaps a slight positive correlation. So you visualize that using, so if you have multiple features, you can show a so-called scatterplot matrix, where you simply look at all the different combinations."
3783.76,3810.7,"And you try to see, okay, can I now see certain patterns? For example, here you can see that there is a positive correlation. Here you can see that there is a negative correlation, just by showing it. Note that this information and this information is exactly the same. It is just mirrored. You swap the x and y dimension, but it's exactly the same."
3811.28,3837.3,"So as you often see, if the thing that you compute is symmetric and you put it into a table, everything above the diagonal is providing you information. Also, these cells, if you would look at the correlation between a value and itself, it is equal to 1. So there's no point in showing that. Because you know what it is."
3837.94,3868.14,"You can combine this now with categorical features, let's say in all kinds of ways, right? So you can mix and match. So here I showed you that you cannot look at a single scatter plot, but multiple ones. The same thing is, of course, you can look at multiple bar charts, multiple pie charts at the same time by just, let's say, playing with the variable. So this is the whole data set."
3868.27,3899.31,"If we look at everything, and here I'm splitting the data set into, let's say, the basketball players that have a shoe sponsor, that have a shoe sponsor, and those that do not. And now if I compare these two, we do not see a very clear signal. So we are now looking at one categorical variable, and we are seeing how that is influencing, let's say, another categorical variable."
3899.54,3925.42,"There are some differences, but they are not spectacular. If you look here, then you see a clearer difference. So again, we look at shoe sponsor, yes or no. And we look at where people are, let's say, playing the field. So typically, the dissenters are the tall guys, the guards are the small guys."
3925.42,3953.54,"And what you see here, that the small guys have much more shoe sponsors than the taller ones, for example. This is showing you the relationship between different categorical variables, which we cannot capture as easily in a number as, for example, using correlation. You can also look at relative frequencies, and I hope that all of these pictures are explaining themselves."
3953.54,3982.64,"So, for example, here if you look at, so all the things now add up to 100%. Here we look at the career stage and we look at the fraction that has a shoe sponsor or not. And here it's, for example, 50-50. And here you can see that most of the centers have no shoe sponsor. Just a few of them have. But if we look at the guards, as we have seen earlier, like most of the guards have a shoe sponsor."
3984.37,4009.87,"Just showing you how you can play with these different diagrams. It is not rocket science, but by combining things. You can also, so far we have been looking at categorical variables. You can also apply binning. And after you apply binning, you can use the bins kind of as categorical variables. So I can do the same thing."
4009.87,4039.79,"So here we see the three positions, and here we can see the age distribution. So you can see that the forwards are typically, I don't know, younger than the guards or something like that. These are the things that you can see. Here you see something that is a bit easier to interpret. So you can see that the guards are typically smaller than, for example, the centers, which are typically above 2 meters."
4040.98,4071.17,"Just showing you, I take a table and I can visualize like a thousand things that provide different insights. And of course, I can also, you have now learned how to create a box plot. And I can simply take a categorical feature and I can use that categorical feature not to create one box plot, but to create one box plot per feature value. So for example, here you can see the overall age distribution."
4071.17,4102.11,"And here you can see the age distribution over the three different positions. We can also do that for, let's say, where people are playing. And if I put these two next to each other, then you can typically see that the relationship between age and position is weaker than the relationship between height and position. So here you can clearly see that guards are smaller than..."
4102.11,4152.69,"than centers. Yeah? Clear? Crash course in, let's say, statistics. So this is how you compute the mean. Everybody knows that. This is how you compute the variance. Hopefully also everybody knows that if you have a thing. So who can explain me why is there minus 1 and not... So we divide by n minus 1. And why don't we divide by n? Yeah. That's..."
4152.69,4182.45,"So the reason is that this A here is not the real mean. It's an estimate. And because this is an estimate and we are applying it to the same data, the likelihood that this is closer to the other values is, of course, bigger. Right? Because this is not the real mean. So if this would be the real mean, you would have to divide by N. Right? But if this is an estimate,"
4182.45,4210.96,"then these values tend to be closer to the things that were in your data set. So there is a bias, and to remove this bias, you do the n minus 1. Okay, so you know your statistics. I'm very happy. So you can take the square root, and then you get the standard deviation that has a clear intuitive meaning. So how far are you typically off from the center of your distribution?"
4211.12,4241.39,"Then covariance is typically a bit more difficult. So covariance is defined using this formula. The n minus 1 is again there for similar reasons. So here the intuition that you should get, if my a value is higher than the average and my b value is higher than the average, then I multiply two positive numbers."
4241.39,4270.26,"and the effect is positive. If I multiply two positive numbers, I get a positive number. If this value is below the average, and this value is below the average, so both are below the average, then I have two negative values. And if I multiply two negative values, again I have a positive value. So in other words, if they agree being above or below,"
4270.26,4297.23,"I'm looking at two different variables, two different features, A and B. If they agree one is higher and the other is also higher than the average, or one is lower and the other one is lower, then that is positive. It becomes negative in the cases that I plotted here, where, for example, this is above the average and that is below the average. Then they're going in opposite directions. So that's the intuition."
4297.23,4327.09,"So the intuition of covariance is very simple. If I have two features A and B, if they agree, if one goes up, the other goes up, or one goes down and the other goes down, we get a positive number. If they disagree, so if one goes up, the other goes down, then we get a negative number. So that part is easy to interpret. The part that is more difficult to interpret is that this scales from minus infinity to plus infinity."
4327.09,4354.69,"So then it becomes difficult how to interpret. So if the covariance is 5, then your interpretation is failing there. So you normalize that by dividing it by the standard deviation. And if you divide it by the standard deviation of A and the standard deviation of B, then you get it scaled back to minus 1 and plus 1."
4354.69,4382.27,"the intuition behind it. So if things are independent, it will be approximately one. If I look at the correlation between a feature and itself, right, it will have value one. If I look at the correlation between a feature and minus that feature, I get the minus one, right? So that's the intuition."
4382.27,4410.54,"So here I'm showing you the plots, but I think this is a maximum positive correlation, maximum negative. Here you can see there is zero correlation, and here you can see some examples where it is weaker. I showed this with this one outlier here, and you can see that that is influencing the correlation value that I have. So again, if I look at the plots, then..."
4410.54,4437.76,"of the plots that I showed earlier intuitive. This has a correlation that is very high. It's almost 0.9. Here you can see, let's say, a weaker correlation, where it's approximately that value. You can see it a bit, but it is not super clear. Also, if you have many features, how do you look at them in one glance?"
4437.76,4465.17,"you simply generate a table of all of your features with all possible correlation values. In one glance, you can see whether there are things that are remarkable. So for the data that I showed you earlier, as I said, on the diagonal that is not interesting, it's always 1. The correlation of a feature with respect to itself is 1, so it has no information, and it is mirrored."
4465.17,4495.23,"So the correlation between height and weight is the same as the correlation between weight and height. If I swap A and B, it has no effect. So because of that, like we know that there is always one here, it's not interesting, you can kind of combine this as follows. So I show scatter plots, and I show this, and the reason is that if I mirror it, it's exactly the same information."
4495.23,4525.86,"Due to symmetry, I'm capturing all of the things. Okay, let's continue. Sorry that I'm a bit in a hurry because I'm conscious of time. And there are still quite a few things that I need to cover. Again, we are looking at the situation where we get a data set. We get a data set. And now we have addressed our outliers."
4525.86,4556.24,"Our strange times, strange ages, et cetera, et cetera. We addressed our missing values. We did everything. Can we now directly feed it to a machine learning or a data mining algorithm? And the answer is that may be dangerous if the scales of the different, let's say, features are very different. Sometimes our data is also too large, so we need to take a sample."
4556.24,4583.41,"or our sample may have a bias in it. So these are still things that we need to address. So just imagine you now have a data set and you would like to prepare it for analysis. Then to show normalization, I'm using, let's say, this artificial table. And I hope that you see that if you feed this into a regression algorithm, for example,"
4583.76,4613.26,"you will get completely random results. And the reason is that the scales are very different. So here, it moves between 0.97 and 0.99. So it's a small value, and the range is very small. Here, I have a huge number, but small variation, relatively speaking. In absolute sense, the very..."
4613.26,4643.55,"The variation here is much bigger than here. Here, again, we have small values that are between minus 1 and 1, but the differences are relatively big. And here we have big numbers with also relatively big differences. If we now feed this into, for example, regression or a neural network or something like that, these dimensions will have lots of effect. If we are computing the error,"
4643.55,4673.57,"I don't know here, the error is always very small, right? If we always say 0.98, the error is also always just 100, 0.1.1 at the most, right? Whereas for the other ones, it would be much bigger. And later, we will compute things like an error, taking into account these numbers, right? So that's why you typically are normalizing your data, and you should be able to apply three"
4673.57,4702.69,"Let's say standard methods. Min-max normalization, standard score normalization, and decimal scaling. They're very simple. I can explain them all with, let's say, one slide. So min-max normalization means that you start with a data set that has an arbitrary minimum and an arbitrary maximum, and you simply scale it linear to a predefined interval. Right?"
4702.69,4733.02,"If you apply this formula, then all of your values will be between low and high. And you will have at least one value that has a value low. And you will have at least one value that has a value high. So you force it to go in this way. So this is the minimum in your data set. That is the maximum. And I hope that everybody can understand in this formula that this is the way to scale it basically to a fixed interval."
4733.02,4760.66,between low and high. So typical values for low and high are 0 to 1 or minus 1 to 1. Just like we scaled covariance earlier to a value between minus 1 and 1. Another normalization is taking into account the standard deviation.
4760.66,4790.03,"So here you look at the difference between the concrete value and the average. And by doing this, I hope that you see that you normalize something in such a way that the average becomes equal to zero. So you can see values above the average or below the average just by looking at the sign. Note if I'm doing something like this. Even if I scale it between..."
4790.03,4817.71,"minus 1 and 1, it could still be that my average is 0.8. Right? That would still be a possibility. So this is a different type of scaling. And this type of scaling attempts to squeeze the data into something that looks like a normal distribution. Right? That's the thing. So the minus A makes sure that the average becomes 0."
4817.71,4847.5,"and dividing by the standard deviation ensures that you get, let's say, a standard deviation equal to 1. That's the basic idea. So this is explained here. So if you apply this formula, you can think of this, okay, you have a normal distribution with a mu and a sigma, and you scale it back to a standard normal distribution with a mu of 0, so mean of 0, and a standard deviation equal to 1."
4847.5,4878.56,"That's the intuition. Yeah, so that's very powerful. This one is a bit more exotic, but that you can still, let's say, interpret the numbers. This is basically shifting the comma. So if you have a number, you can, so for example, I'm talking in millions, and now I say a million is just like a single unit. You're playing with where you put the comma, right? So you can shift the comma."
4878.56,4907.23,And you try to shift the comma in such a way that you're using the interval between minus 1 and 1 in a maximal way. So you shift the comma in such a way that all of your values will be between minus 1 and 1. And you're shifting it as tight as possible. This is much easier to explain if I just show you examples. So if I have these values.
4907.23,4934.29,"Minus 966 and 78. So we need to move the comma. And moving the comma is multiplying or dividing by 10 to the power something. So if I divide that by 1000, these two numbers, I get this and this. So we get values between minus 1 and 1. And it's tight. Right?"
4934.29,4962.93,"I could also have divided by, I don't know, 1 million, but then I would have just used a small part of the interval between minus 1 and 1. Yeah? Then, like if I take a look at some other examples, so we have this and this, and after normalization it will look like this. So we shift the comma for all the different values in such a way that we get something that is between minus 1 and 1."
4962.96,4993.01,"in a way that is as tight as possible, to keep as much information as possible. Note that with the other ones, the mapping to the original number is not so easy, right? And if you do this, then that is easier. That's why sometimes people do this. This is an example. I don't want to show it now in detail, but I would suggest you that you take these numbers. That's something that you should be able to do. Take these numbers and..."
4993.07,5020.37,"see what the effect is of these different scaling things. So you can play with this itself. It's the same as with the let's say box plots. It is not rocket science, but at the same time you really need to understand what you're doing. Otherwise it will not work. Binning. I already mentioned this, so you can think of binning a bit like you have a continuous variable."
5020.37,5045.46,"In most cases, if you have real continuous data, for example, if you would use a random generator and generate numbers between 0 and 1, no two numbers will be the same. Unless, I don't know, you use all the precision of the machine. No two random numbers will be exactly the same. Unless you wait for a very long time."
5045.46,5074.27,"So that means that if I try to plot this and I show like this as a scatter plot, everything is unique. So I would not see anything, right? So that's why you do binning, right? And you're trying to recreate this distribution, let's say, in a visual manner. And then if you have too few bins, like you can think of this as underfitting. If you have too many bins, you can think of this as overfitting."
5074.27,5104.11,"Because there will be bins that just by accident have no values. Although, according to the distribution, that there could have been many values, but were just by chance. In terms of binning, the first type of binning is equal width. So that means that what you have on the x-axis, you have a lower bound and an upper bound. And every interval that you look at on the x-axis has the same size."
5104.18,5132.67,"That's what you see here. So the width of every bar is the same. That's one type of binning. It's probably the thing that you're most familiar with. However, it is also possible to make the width variable. So you make the width variable, but you want to ensure that in every box, in every square, in every rectangle,"
5132.67,5163.58,"that the number of instances is the same. And if you do that, then you get something like this. So this is a white box that is low. And that is the case because there are fewer values, let's say, in this interval compared to this interval. So that's another way. Both are, let's say, correct. And the thing that you should see is that you can use both methods. And in both of the methods,"
5163.58,5191.28,"like the area of every bin reflects the number of items in that bin. So what you see here is that here the surface area of every bin is exactly the same. Here it is not. But here we can see that in this bin there are much more values than in this one, for example."
5191.28,5219.49,"And here we know the number of values in this bin is exactly the same as in all the other bins. But they both express the same thing. Give me five minutes, and then I will try to do it. Sampling, you can have a lot of data, simply too much to analyze, or your data may have a bias."
5219.49,5248.06,"This would be what is called top sampling. So you have a huge data set and you take a sample starting from the beginning. And this is very dangerous if your data, like it has a meaning, right? So for example, if your data is temporally sorted, this may not be representative for the whole period. Another type of sampling is that you do a random sampling."
5248.06,5274.99,"You take random samples, and then you may, so if there would be something like concept drift, that things are changing over time, you would be able to address this. Stratified sampling is also, let's say, taking a sample, but making sure that the sample has the same distribution as the overall data set."
5274.99,5303.01,"And here with the shapes, I'm kind of indicating that. So in the overall data set, the number of males is one-third and the number of females is two-thirds. Then that should also be the case in my sample. So here I want the sample to be corresponding to the whole data set. Of course, you can also imagine that if I have a data set like this, I would like to de-bias this."
5303.01,5332.24,"My number of male samples instances and the number of female instances would be exactly the same. And there are two ways to do that. If one set of instances is larger than the other one, there are two obvious ways to do it. Of the largest set, you can remove part. So here I'm removing, let's say, half of the females from my data set to make it balanced. That is 50-50. That's one thing."
5332.24,5362.45,"Of course, the drawback of this approach, if we think about, let's say, machine learning, we are throwing away, let's say, one-third of our data. That may not be a very smart idea. Another thing to do is that you simply duplicate instances to make it biased. For example, if you look at something like intrusion detection or something like that, you have a data set where most of the samples correspond to no intrusion."
5362.48,5391.41,"Very few samples correspond to an intrusion. But if I would then train a data model, it would probably say there is no intrusion. And it would be right in most of the cases. By doing this, by replicating the intrusion instances, I become better at detecting intrusions. Because otherwise they would just disappear."
5391.41,5418.29,"Let me stop here. So the things that I did not discuss, but that you can look at, was the, just one minute, please. So was the fact and fair principles. You can read about it. Things like fairness, et cetera, will be discussed later. I also showed you some more interesting visualizations, but they are completely self-explaining. So I think you will not have a problem."
5418.64,5434.83,"Next week I will not be here, but there will be normal lectures. So there will be a lecture on decision trees and a lecture on regression. They will be given by two of the instructors. I will be in Uruguay."
