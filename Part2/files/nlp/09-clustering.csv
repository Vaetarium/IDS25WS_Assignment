start,end,text
2.74,31.15,okay let's get started so welcome to lecture number nine already so as i said we are now done using the let's say supervised learning we now go into unsupervised learning so today we will talk about clustering you will learn let's say four very concrete techniques that you will be using hands-on but also
31.15,61.2,"where we can ask, let's say, pen and paper exercises. And before I dive into that, like in the last lecture, at the end, I had, let's say, some timing problems. So there were a few slides that I did not show. I do not want to, unless I'm done early today, I do not want to go into this in detail because then I probably create another problem. That is the, we ended with talking about concept drift."
61.23,90.9,"So concept drift means that you're observing a process and that the process is changing over time. So for example, if you would look in something like healthcare, you will see that, I don't know, the number of people smoking and the people getting cancer, et cetera, is not a stable thing, as these graphs show. So often if you have a data set, it is very important to be able to analyze, is my underlying data changing?"
91.12,116.46,"And in the last lecture where I didn't have time to elaborate on that, there is something that you only need to know at a very high level, that is the earth-mover distance. So the earth-mover distance is basically a way of comparing two distributions. So you observe a process, for example in February, and then you observe the same process in March."
116.53,144.27,"And then you look at the distribution of the cases. You can just look at any feature and you look at the distribution. And using the earth mover distance, you can basically get a number between how different are two distributions. That's something that you only, at this point in time, need to know at a high level. The thing that is more detailed is the formula that you see there, where you are..."
144.27,172.35,"comparing, let's say, two distributions that are categorical. You can also have been using binning. And then you compare, let's say, the frequency of, let's say, a certain categorical attribute or numbers in a particular range. And you compare them between two different time periods. And if you apply that formula, then you get a number."
172.35,198.83,"And if that number is close to zero, if it's precisely zero, then there is absolutely no change. If it's smaller than 0.1, we say there is no significant drift. If the number is higher, and for example above, let's say 0.25, we say that there is a significant drift. So this is a way of measuring two different distributions."
198.83,228.4,"If things are unclear, post a question in the forum or come to me afterwards, right, then I can explain this in more detail. Because now I would like to dive into the topic that we talk about today, that is clustering. I briefly start talking about unsupervised learning, but that should be clear to you. And then I dive into these concrete methods that I mentioned before."
229.9,251.1,"Data science is a very broad field and there are many different names for exactly the same thing. So I'm mixing, let's say, content from different books. Sometimes I'm using screenshots that come from a particular book in this space. Please be more quiet. Thank you."
251.1,281.63,"So for example, I will often be talking about instances, but if you go to another book, people may talk about objects or tuples or something like that. I talk about a feature, but in other books it will be called a variable attribute. So probably if I talk, I'm very sloppy in the terms that I'm using, but I hope it is clear to you. So don't be confused with it. These things are exactly the same thing. So unsupervised."
281.63,310.5,"We have been focusing on this problem, for example, using neural networks, decision trees, regression, SVMs. That was all focused on we have a target attribute, and now we go to the situation where we no longer have a target attribute, but we want to find patterns in our data. That's the basic idea. That makes it a bit, let's say, more difficult, but at the same time also probably more interesting."
310.5,340.7,"You find things that you were not explicitly looking for, unlike a neural network, which you're trained to do a very particular task. So these are the unsupervised methods that we will talk about today. We just talked about clustering, but in the next weeks, you will see things like frequent item sets, association rules, sequence mining, process discovery, time series, et cetera, et cetera. So lots of unsupervised methods."
340.7,368.96,"But today we focus on clustering. So let's start with, let's say, a bit of the basics. So if I'm showing a picture like the picture that you see here, like I'm basically assuming, okay, we have two features. One feature for the x-axis and one feature for the y-axis. And in this two-dimensional plane, I can easily show what are the points that are close to each other. So this is very easy to imagine."
368.96,398.02,"Of course, it becomes more difficult if my number of features that I'm using is hundreds or perhaps even thousands, then you cannot simply draw it, right? So this is just to create a mental image, but at the same time it is very misleading. And we want to find instances that are close to each other. And these groups of instances that are close to each other, they are far away from the other groups."
398.02,425.55,"We want to find patterns like this. So let me start with, let's say, several examples that you can all imagine what this is. So you can look at Spotify. There are some numbers there. And now you can basically choose what are my rows and what are my columns. So this means that user 1 played song 1 four times. I can also swap the..."
425.55,451.15,"the rows can be songs and the columns can be users. But I now look at it in this way, so now we have different users and now we would like to find users that have similar interests, right? And then one way to do this is to apply clustering and here I'm showing it again in let's say two dimensions, but the hypothesis if you are doing something like this"
451.7,482.03,"It doesn't work with the simple techniques that we talk about today, but I'm trying to explain to you what the goal is. That you would find, I don't know, that there are users that just play metal, there are users that just play dance, there are users that just play disco, and there are a few users that have a taste that does not fit in one of these categories. They are in the middle. So this would be Spotify. You can also go to Amazon."
482.03,511.71,"And again, you have like a matrix where you have the different products and you have the different customers. If you would try to do clustering here, that would probably not work, right? Because the dimensions are huge. But you can imagine that you can cluster products, right? That you can first cluster them, that you have a few less columns. But now you want to find patterns that customers that typically buy products together. And if you do that..."
511.71,538.75,"then you may find out that although you were never looking for it, that there are, let's say, products typically bought by families. So there are many people that have the same, let's say, shopping pattern. At a high level. You can go to a supermarket, and this is where, for example, in the next lecture we talk about frequent item sets. And frequent item sets, the first use cases come from this area."
538.75,567.94,"where you are interested in finding out whether there are certain things brought together. You can again, so again, every instance is a vector, right? Every instance is a vector. So now I have points in an n-dimensional vector space, and I try to find something like this. I try to find clouds of, let's say, instances that are close to each other."
567.94,594.72,"and these different clouds are far away from each other, right? That's the goal. You can look at cars, right? And for every car, you can record a bunch of attributes. I'm showing you this example to show that it can also be categorical. If you have a categorical feature, you would do a one-hot encoding. So, if I want to translate this first row into a vector,"
594.72,621.41,I need to have a separate column for every car. And this would then be encoded with lots of zeros. And there would be one car for the brand Porsche. And the rest would be zeros. So this is just to encode you that it can also look like this. And the goal is again to encode it as a vector with numbers. We have seen using things like one-hot encoding that this is always possible.
621.41,651.6,"And if you have such a vector space, the goal is to find what are typical types of cars that are very similar to each other. So now you have seen some applications. And highlighted here in green are, let's say, the different methods that I will talk about in detail. So we look at, let's say, techniques where up front we decide we want to have K clusters."
651.63,681.18,"And let's try to find, let's say, K clusters, fixing it before. There are hierarchical methods that have the advantage where you do not need to decide up front, I'm interested in K clusters. You basically use a threshold and you can play with this threshold going from one big cluster where everything is in the same cluster to the other extreme case where every cluster is a single instance. And you can look at anything between."
681.18,704.83,"There are two approaches, bottom-up and top-down. We will just talk about the bottom-up approach. Then all of these approaches are assuming that we have a nice, let's call it a Euclidean space, that the distance is very clear, that you have shapes that are like clouds."
704.83,730.37,"But with DB scan, you can also see that there could be shapes in your data that you have one cluster that looks like this and the other cluster that looks like this, right? Which would not be supported by the other methods. The last topic, self-organizing maps. I already mentioned that in the lecture on neural networks. I will now show you, let's say, how that technique works. And it's also using the term neurons."
730.37,761.7,"But it fits much better with clustering than it fits with neural networks. So what's now the goal? The goal is that we have instances represented as a vector of numbers. You can plot these in an n-dimensional space. You can use distance measures like, for example, Euclidean distance. So you have different distance measures in this n-dimensional space."
761.7,791.47,"groups of instances that are close to each other and that the, let's say, distances between these different groups are far apart. So intuitively, if you would look at something like this, you would probably agree that if I said k is equal to 3, I'm interested in 3 clusters, that this would be a cluster, this would be a cluster, and this would be a cluster. That's the basic idea. And we are going to make this operational."
791.54,821.76,"So one of the things that we need is we need to have either a similarity measure or a distance measure. Sometimes we talk about distance, sometimes we talk about similarity, but they are exactly the same thing. But one is the inverse of the other. So two instances, i and j, are represented by a vector, and we can talk about the similarity between these two vectors."
821.76,846.99,"where we say we have a high value if things are close to each other, and we have a low value if they are very far apart. And often similarity is expressed in something between zero and one, and then one would basically mean that they have perfect similarity, so they are identical. And zero would be somehow the worst case that is possible."
847.66,876.26,"then there is this similarity or distance, and that is the other way around. So if I have a similarity measure that lies between 0 and 1, I can create it, I can turn it into a distance measure by taking 1 minus the similarity. Vice versa, a distance could be, I don't know, an absolute value. Think of it, the distance from, I don't know,"
876.26,906.03,"Aachen to Munich is, I don't know, 600 kilometers, right? Then I could just look at what is the maximal distance between any two cities in Germany, and I would divide it by that number, and I would also get the value between 0 and 1, right? So I can always normalize these things. So typically, right, that's on one of the slides, typically we try to normalize these different features. You remember that also from other approaches."
906.03,933.2,"So for example, if we are feeding things into a neural network, and we have one input neuron that has a value that ranges from, I don't know, minus 0.1 to plus 0.1, and we have another feature that ranges from, I don't know, 0 to 1 million, that that will have an effect on the learning process. So typically you also normalize these things, right? But that's kind of preprocessing that we talked about earlier."
934.45,963.98,"So if you talk about, you can also talk about similarity and distances when you talk about categorical variables. So if they are equal, the distance is zero and the similarity is one. If they are different, the distance is one and the similarity is zero, right? If we encode it on an interval from zero to one. Then there are, of course, if you look at numerical features after normalization, there are, let's say, probably the thing that everybody..."
963.98,993.6,"kind of immediately aspect, is the Euclidean distance. When I was talking about the distance from here to Munich, I was talking about Euclidean distance. But there are other distance notions that you can use, and these I will explain step by step. If you have such a distance measure, then there are many ways in which you can define this, and you will see several examples, but before doing this,"
993.6,1023.17,"It's important to distinguish between, let's say, ways of measuring distance that are in metric space and distance notions that are not in metric space. And something is a metric space if it satisfies these four properties. The first three you probably all agree with, that they are completely trivial, that you would like to have them. So, for example, that distance cannot be negative."
1023.17,1052.34,"I look at the distance of one instance to itself, it should be zero. So it's always positive and only zero if it's the same. Symmetry, the distance from here to Munich is the same as the distance from Munich to here. So these are very obvious. The thing that is more tricky is the triangle inequality. That is basically saying that the distance from here to"
1052.34,1081.39,"Munich cannot be larger than the distance of going from here to Cologne and from Cologne to Munich. So the distance of going from I to J cannot be longer than the distance of going from I to K and from K to J. Which sounds very logical. If you think about Euclidean distance, this obviously holds."
1081.39,1110.99,"But there are many distance notions used in data science that do not satisfy this property. And I will show you an example of that in a minute. So these distance measures that I show you here, they all are in metric space. So they all satisfy the four properties that I said before. And the general form is the Minkowski distance. That is this formula."
1110.99,1140.62,"Note that we take, in every dimension, we take the absolute value of the difference, and we raise that to the power h. We sum up all of these things, and then we take the root with the base h over this. So this looks very weird to you, but if you look at Euclidean distance, where h is equal to 2, that's why it's omitted, it's a standard thing, then this is something that you all recognize."
1140.82,1165.39,"But you have to realize that this is a special case of this general Minkowski distance notion. So this one you know. That's the standard Euclidean distance. H is equal to 2. H is equal to 1 is also very easy to interpret. Think about New York, right? And you look at the distance of going from one location to the other location."
1165.39,1195.23,"The distance is not the Euclidean distance, that's not what you have to walk, because then you would walk through buildings. But if you have a city with a grid structure, right, the way of going from point A to point B is that you need to go in the x direction and then in the y direction, right? That's therefore called the Manhattan distance. And if you look at this formula, h is equal to 1, so this root, let's say, disappears."
1195.23,1224.66,"And we just sum up the absolute values. Do we see that? That is simply the Manhattan distance. Interestingly, you can do that for all kinds of values of h. And if you let h grow into the limit, then you're only focused on the dimension where the distance is the biggest. So you're basically measuring in what dimension is the distance the biggest. Yeah? That's the idea."
1224.66,1254.16,"So here they are listed. So just think of, let's say, this point 1, 2 and this other point 3, 5. So what is the Manhattan distance? The Manhattan distance says that we have to go 2 in this direction and 3 in this direction. We can also go 3 up and then 2 in this direction, but that's equal to 5. That's what in Manhattan you would actually have to walk to go from here to here."
1254.16,1281.5,"Because you cannot go through buildings. If you are flying, you can fly over the buildings and there is this distance. That's h is equal to 2. That's 3,6 in this example. If you let h go to infinity, then you're basically looking at what is the dimension where the distance is maximal. And you take this one."
1281.5,1310.46,"So this one has distance 2. This one has distance 3. So you pick the tree, so this is what comes out. Just to make you think about distance notions. And these are all satisfying the properties that I explained to you before. They all have these four criteria that define a memetric space. This is what I said earlier. So you typically want to normalize features."
1310.46,1340.21,"In data science, it's always very unpleasant if you have features that have completely different ranges, because then implicitly they become more important. You may also exclude deliberately features, right? Using unsupervised learning, you are interested in finding things that you do not know yet, right? So that sounds very, let's say, counterintuitive, but you want to leave out certain attributes."
1340.21,1367.47,"So, I don't know, if I want to cluster people, just think of your own example. Sometimes things very well fit together, but you already know that, so you deliberately leave that out. That's often the case. You can also deliberately give weights to different features that you consider them to be more important."
1367.47,1395.86,"So this is pre-processing, leaving things out, adjusting the weights, and then you can look at the thesis notions. One of the things when we talk about NLP and text mining that you will see is that you can represent basically, and you see that here in the bottom corner, you can basically represent any document by the bag of words. So what is the bag of words?"
1395.86,1421.07,"You simply, for a document, you say how often does a word appear. Or you look at web pages. So on a web page, how often does every word appear? So that means that you can represent any document or any web page by a vector. But then you have the following problem if you're using Euclidean distance or Manhattan distance."
1421.07,1447.66,"You then have the problem if I have like one document talking about a certain topic, and I have another document that is basically two times that document, so I simply copy it twice, then the distance between these two documents is very big, right? Because if you think about this as a vector, you basically made the vector two times as long if you copy the same text."
1447.66,1477.71,"Although the content is more or less talking about exactly the same thing. So Euclidean distance, if you think about this as vectors, the Euclidean distance, the Manhattan distance, Minkowski distance, they all have the problem. If one vector is much longer than the other, by definition, they will be very far apart. And this is often undesirable. So that's why in many applications you see,"
1477.71,1506.72,"that we are using cosine similarity. And cosine similarity is this formula. Remember when I was talking about support vector machines, I showed you exactly the same formulas, right? So you see that many things are repeating. So the definition is that you take the dot product of two vectors divided by the length of these two vectors. So you normalize it."
1506.72,1536.94,"First take the dot product and then divide it by the product of the length of the two vectors. And this is exactly the same as the cosine between these two vectors. And again, if you look at this image, I have this. So what is the cosine? That is basically this length that you see here. So if you have something that has a zero degree angle, the cosine is equal to one."
1536.94,1567.14,"If I have a 90 degree angle, the cosine is equal to 0. So if things are orthogonal to each other, the cosine is equal to 0. So here, if they are orthogonal, it's 0. If they are pointing in opposite directions, it's minus 1. If they are pointing in the same direction, it is equal to 1. Now think about this document vector. This makes a lot of sense."
1567.14,1596.18,"If I think of vectors describing a document, even if a document is two times as big, but talks about exactly the same concept, we want this similarity to be equal to one. Do we get that, the intuition? So this is, let's say, a widely used measure. And now I show you a counterexample."
1596.18,1624.75,"Showing that this similarity, I can turn it into a distance measure by taking one minus the similarity, as we often do. Then I have a distance measure. And if I take this distance measure, I will show to you that it is not in metric space. It's not a metric."
1624.75,1653.76,"I create this one, so we have this point, 0,1, 1,0. And there is this point, you may wonder why it looks so strange, but I normalize it already, so this B vector is also, so all the three vectors have a length of one. That makes it easier to do the computation. And normally, if A is Aachen and C is Munich,"
1653.76,1683.7,"then it can never be that the distance of going from Aachen to Munich is larger than the distance of going from A to Cologne and from Cologne to Munich added up. Right? That's what you expect if it's Euclidean distance. If you look at cosine similarity, which I just argued is widely used and makes a lot of sense,"
1683.76,1712.27,"So normally, this should be the other way around. But I now show you, given this example, this inequation holds, and it's actually this. So if you look at the angle, for example, between A and C, it's 90 degrees. So the similarity is equal to 0, because it's orthogonal. 1 minus 0 is equal to 1."
1712.27,1741.62,"I can also compute, let's say, the cosine similarity between B and A. This is 45 degrees, this is also 45 degrees. And if I fill out these numbers, then I get these numbers. On the next slide, I show you that in detail. So here I simply copied, let's say, the formulas that you showed before. And if you now look at this picture, so the cosine of 45 degrees is 0.7."
1742.1,1769.68,And we talk about distance. So we take 1 minus the cosine of 45 degrees. It's approximately 0.3. This is also approximately 0.3. And now we have the strange situation that going from A to C is more than the sum of going from A to B and from B to C. Do we see that? Using this example.
1769.68,1806.03,"Fill out the values, and you can see that this is not a metric that's violating this property. Yeah? It's something to be concerned about, right? Yeah? No, because we are looking at cosine similarity, which is the angle between the different points. Right? So the point is that you are constantly thinking in terms of Euclidean distance, and I'm trying to train you."
1806.03,1833.54,"That if you look at applications like this, right here, it would not make a lot of sense to look at the Euclidean distance between the vectors that you have here. Right? Because if I would just multiply a vector by two, it would have a huge distance between it, which would not be accurate. Right? Yeah? So you need to..."
1833.54,1864.5,"unprogram your thinking of always thinking in terms of Euclidean distance if you look at these spaces. So you can use any distance measure, right? And later when we talk about NLP and text mining, we will basically just use this one. We will not use Euclidean distance. There is another, let's say, similarity measure that you can use, or a distance measure, that is the Jacquard distance."
1865.3,1896.26,"So this is if you have two sets and you want to know the difference between two sets, you can first define similarity. That is the intersection divided by the union. So that is the percentage of overlap that I have that varies between 0 and 1. I can also take 1 minus this value and then you get this and you get the distance notion. This is again a metric."
1896.26,1922.32,"So this again satisfies the four properties that you have. So what we have now seen, we have seen things like Euclidean distance, Manhattan distance, etc. We have seen cosine similarity, but you can also, if you talk about sets, you can also look at this Jacquard distance. So these are different ways of measuring distances. In the remainder,"
1922.32,1951.92,"Like in most of the things, you can basically see it more like Euclidean distance, right? But you need to understand that there are also other ways of doing this. So we start now with concrete techniques that you should be able to do. So if I give you like a table with numbers in it, you should be able to apply k-means. And k-means starts with a value k. k is the number of clusters that you want to end up with."
1951.98,1981.2,"And the way that it works is that you have this n-dimensional space. And if k is equal to 3, then you basically throw three darts in this n-dimensional space. Completely random, right? I throw a dart there, I throw a dart there, I throw a dart there in this n-dimensional space. And what I then do is that every instance has a closest dart, right? I've been throwing these three darts."
1981.2,2011.94,"And for every instance, there is one that is closest. I assign every instance to the dart that is closest. Right? So I'm coloring it. So let's say that I have a green dart, I have a blue dart and a red dart. Right? I throw them. I color all the points based on what dart they are closest to. Then I have these colored dots and I basically take the average"
2011.94,2041.46,"of these dots, for example the dots that were all closest to the green dart, I color them all green, I take the average of them, so in a vector I just add up all the vectors and then I divide it by the number of vectors that I have, and that is the middle point. And I take my green dart and I put it in exactly, let's say, the middle of these green points. After doing that,"
2041.46,2070.7,"Right? I did that. Then I repeat the game. So my dart has probably shifted. And again, I look at all the points. What are the points that are now closest to the green dart? And there may be points that were first red, and because of the repositioning of the green dart, they change color. And I repeat this until it stabilizes. So that's the idea of, let's say, k-means."
2071.6,2101.23,"This is showing this. I've been throwing a dart here, a dart here and a dart here. The cross here you cannot see, but the cross here and here you can see. I just threw these three darts. Now with the circles I indicate which are the ones that are closest to my dart. Then I reposition the dart, so you can best see that here. All of these points that are in this cloud are closest to this dart."
2101.23,2128.19,And now I reposition this dart to the middle and then it ends up here. And I continue doing that. I repeatedly do that until nothing changes anymore. So this would be the final clustering where these are my three darts. These darts are called centroids. They are the middle of the cluster. That's the basic idea.
2128.19,2157.94,"I have some more pictures and some more examples to show you. So this is happening in an n-dimensional space, right? So I always show it in two dimensions, but that's very misleading. Later I will even show examples with just one dimension, just to make it easy. Typically this is happening in a high-dimensional space and you need to decide, let's say, how you evaluate the quality of a cluster."
2157.94,2185.04,"Like what kind of distance measure you are using. So here there is this distance measure that's being used. And here the error that is typically considered in two k means is this expression, which is basically the sum of all the squared errors. And an error here means how far is a point away from the centroid. Because like a..."
2185.04,2214.43,A cluster would be easiest if all the points would be exactly on top of the dart. So the quality is high if these distances inside the cluster are small. So P is an element of this cluster and CI is the centroid. So I simply take a look at the distance and I square it. And this looks very much like the error functions that you've seen in the past.
2214.43,2243.5,"This is the algorithm. It's basically applying what I just said. So you randomly pick points and you continue to reposition the dart or the centroid until it stabilizes. And of course, I think many of you have an intuition, this is not deterministic. But the approach works in such a way that if you use this"
2243.5,2272.13,"this quality thing that you are hitting a local minimum. So when it stabilizes, you reach a local minimum, right? And you cannot improve from that anymore. And there may be a global minimum that is better. So typically, if you are using Python and you implement this, one of the parameters is how often do you throw these initial darts? It's completely random."
2272.13,2302.61,"And by doing this more, like the likelihood that you find the global minimum is higher. Because you just try it multiple times and you're using this to evaluate which is the best one. So what are the decisions? The decisions is we need to decide on k, the number of initial clusters. We need to decide on the initialization. Just think of it as completely random. And we need to decide on what is the distance notion that we are using."
2302.64,2331.2,"And you've seen several examples before. And this leads to a local optimum. So you can prove that it is a local optimum if it's stabilizing. I should rephrase it differently. If I initially throw the darts and I continue repositioning them, I'm decreasing my error."
2331.2,2358.51,"So I'm monotonically decreasing the error. That's something that you can prove. And then at some point in time it stabilizes that nothing changes anymore. And that is a local minimum. It's not a global minimum. And I can show you, let's say, a super simple example immediately making the point. So we just look at a one-dimensional data set with numbers 1, 2, 3, and 4."
2358.51,2389.34,"Now it's not two-dimensional, it's just one-dimensional. And I look at these numbers and now I can look at this cluster and I can look at this cluster. So the average of 1 and 2 is 1.5. The average of 3 and 4 is 3.5. So these are my centroids. Note that 1 and 2 are closest to 1.5. 3 and 4 are closest to 3.5."
2389.34,2417.31,"So it stabilizes in this situation. So if I randomly have thrown darts at these positions, I'm immediately done. But I could also have randomly thrown darts at, let's say, positions 2 and 4. And if you now look at these numbers, SO1, 2 and 3, they are closest to 2, right? There's also like a similar distance, but let's forget about that in a minute."
2417.31,2451.2,"And the other one is four. So this is stable. I cannot improve it. No point is closer to another centroid. It's very loud. So you cannot, let's say, improve on this situation. So this is again a local minimum. So I have two local minima where I think that both of you will see that these are different in quality. So if I'm using this quality notion,"
2451.2,2480.02,"the squared difference, the criterion that I showed you earlier, then you see that this first one is better than the second one. Again, what was the quality criterion? That was this formula, the squared distances. So what is the guarantee that we have if we apply the algorithm and we continuously move darts? We have the guarantee that we reach a local minimum."
2480.02,2511.3,"But it doesn't have to be a global minimum. So this is visualizing this in a more interesting thing. So initially, k is equal to 4. So here is a dart, here is a dart, here is a dart, here is a dart. This is the initial situation. And note that these dots here are purple because this dot is closest to this one. This one is brown because it's closest to this one."
2511.3,2540.06,"These are all green because they're closest to this one. So this is the initial situation. And after doing this, I reposition the centroids to be the average of, for example, all the purple things. So you will see that this will move this way because these points are kind of pulling, let's say, this centroid in this direction. So after repositioning, it looks like this."
2540.06,2571.33,"It's probably clearest for the green ones. So these were all the points that were closest to the initially thrown green dart. Some of them are very close, like for example these two. Some of them are very far away. And after coloring the instances, I reposition the centroid to the middle, which is here. I recolor it."
2571.33,2601.44,"And now you can see that these points here were first green. They're now not green at all anymore, right? Because they are now closer to the red one, which moved much less. And I repeat doing this, right? So again, you can see that slowly we are repositioning things. So here you see multiple iterations. And you can see that here we have hit a local minimum, not a global minimum."
2601.44,2632.02,"So you see here the problem. It stabilizes in this situation where this is the purple cluster, this is the brown cluster, this is the green cluster, and this is the red cluster. And I hope that you can see that you could improve this by making this one cluster and making these two clusters. If you would look at our quality notion, that would be better."
2632.02,2663.01,"I don't know, are using a Python package to do this, why you typically try this multiple times, right? And take the best one. So I think this is a bit superfluous, but I think that you got the idea, right? That you try to find, let's say, such a local minimum. And what you saw in the example is that you can end up with centroids that are kind of in between clusters. Because your choice of K was wrong."
2663.01,2692.78,"or because you initialized it in a way that was very unfortunate, right? That is stabilized with things that are just in the middle of clusters. So just in such a way that you can do this with pen and paper, right? And here, as I said, it's very weird because I just use one dimension, right? But that's the easiest way to explain it. So these are all instances in a one-dimensional space. So vector of length one."
2692.78,2719.28,"And now we want to apply k-means. So we could throw the dart here at 2 and we could throw the dart at 13. And then we would have such a situation. And this situation is what we say converged. Like every point here, every blue number is closest to centroid 2."
2719.28,2748.37,"And every purple number is closest to centroid 30. Right? So all the points are closest to the centroid. So it is converged. Right? I do not move the darts anymore. But the sum of errors is equal to almost, let's say, 200. So the sum of squared errors. Yeah? This would be a possible end situation."
2748.37,2776.61,"But I could also have thrown my darts initially, I don't know, for example at 4 or so. And then it would go to 3.5 and 14. So now this would be the allocation. And what you now see is that I'm cutting it here. And if you look at this, then you see that the algorithm is not done yet. Because 9 is now closer to 3.5."
2776.61,2806.93,"then 9 is to 14.76, which is the average of the purple ones. So we will move this 9 to this cluster and we recompute the cluster. What you should look at here, here the error is 189. If I go one slide back, although this looks like a very unfortunate cluster, it's also not a local minimum yet. This was a local minimum, but the value is higher."
2807.25,2836.61,"So this is a stable situation. The algorithm will end here. The sum of squared errors is 196. But if I would have thrown the darts differently here, I'm not done. I can still improve over this. But I'm already better. Which shows that we are talking about the local minimum and not the global minimum. So as I said, the algorithm is not finishing. So the 9 will move to this part."
2836.61,2864.62,"The 10 is closest to this one, but the 9 is closer to the 3.5 than it is to this centroid. So we will move the 9. Then we end this situation. Now the average of these values is 4.6. The average of 10 and 25 is 70.5. And we are still not, we have improved. That is guaranteed by the approach. So our error has improved."
2864.82,2893.79,"But we are still not done because 10 is closer to 4.6 than it is to 17.5. So we move this one here and in the end we end up with this cluster where 25 is a cluster consisting of just one element and all the other elements are in this cluster with centroid 5.5. And note that as is guaranteed, the sum of squared errors"
2893.79,2922.64,continue to go down. And this is like the minimum that we cannot get out of. Do we see that? Let me modify the example a bit. I changed the 9 into a 10. Now we have this situation. And again I can apply the algorithms. So again you see the numbers. So in this situation we have already converged.
2922.7,2950.74,"So these two tens are closer to 15 than they are to the 3.5. So this is an end situation, but I hope that you understand that this is very unfortunate, right? Because we are kind of separating values very close to each other, right? And if you look at the centroids, this centroid 15 is kind of in the middle where there are no other instances."
2951.38,2984.19,"So I hope that these numbers examples show you a bit like what the problems are. Yeah? Yeah? And you constantly assign them to the local things. And I'm showing you different initial situations that lead to different results. So this is a problem. And one, let's say, possible improvement is K-medoids. And this is addressing this problem. Look at this dart."
2984.19,3012.91,"The centroid is equal to 50, which is the average of 10, 10, and 25. But it is relatively far away from all the different points. So K-medoids is doing something very similar. The only point is that our centroids are no longer, let's say, the middle of a cluster. Our centroids have to be concrete instances. So the center of a cluster..."
3012.91,3040.02,"cannot be in an area where there are no instances. The center of a cluster can only be an existing instance. And doing that helps. So with the other approach, there was the necessity to have an error function where we take the squared errors. And why were we doing that? Because if you use the squared of the errors,"
3040.02,3070.61,"you can guarantee that it is monotonically decreasing until you hit the local optimum. If you use another distance notion other than the square, you cannot guarantee that. Here, with K-medoids, it's more flexible. Right here, I can simply take the sum of all the distances, which is more intuitive because this approach works differently, right? So we don't need that guarantee. But again, we pick K. But now,"
3070.61,3098.74,"the centroid, which is called the medoid here, is an instance. It's not an arbitrary value that is an average of things. So if I apply it to this method, now our centroid is not, let's say, some random dart, but we start by simply picking a number of instances. In this example, k is equal to 2, so we pick two instances."
3098.83,3124.94,"So I could pick this instance as a medoid and this instance as a medoid. Again, I assign all the instances to the closest medoid, it is called now. So 25 is closest to 8. 3 is closest to 1. And here I sum up the error. So what is the error here? This has error 0, error 1, error 2. So this is 3."
3124.94,3155.57,"And here if I take 8 as a medoid, then I compute all the different distances. So that's 0 plus 1 plus 2 plus 17 is equal to 20. And my total sum is equal to 23. And this is already, let's say, a stable situation. Because I assigned everything to the closest point. So how does the technique now work?"
3155.57,3183.23,"Instead of, I don't know, repositioning darts or something like that, I look at can I now swap a medoid, which is an instance, by another instance. And I just swap and I look whether things are improving. So here I did two swaps. So I went from one to two and I went from eight to nine."
3183.23,3212.86,"And then again, I can compute all the distance and we can see that it's now better. So how does the algorithm work? And there are different variants of it. We have a number of MEDOIDs and we have lots of points that are not a MEDOID. And I simply look at what happens if I pick an arbitrary instance that is not a MEDOID. I pick an arbitrary MEDOID and I make the instance that was not a MEDOID a MEDOID and vice versa."
3212.86,3240.77,"So we simply look at the swap. And here you can see that here I actually did the two swaps, but we can see that it got better because it was first 23, now it is 20. I could make another swap. For example, I could replace, let's say, 9 was first a Medoid, and I can replace that by 10. You can see that the quality is the same. If I would make another swap, for example, that 10..."
3240.77,3270.85,"is no longer a medoid, but we swap it by 25, then we get an error of 21. So it's worse. So the lowest two are these two, and they have the equal value. So that is the goal of the algorithm. So in the end, like it would end with this situation, which, let's say, is the optimal cluster in this case. That's the basic idea."
3270.85,3301.78,"So here is the algorithm. So let me again say it in words. So we have randomly picked a number of metoids. These are concrete instances. But the number of clusters, the number of metoids, is much smaller than the number of instances, of course. And what we do is that we try to replace one of the metoids by an instance that is not a metoid. And we simply do the swap."
3301.78,3331.76,"And then we see whether things get better. If things get better, we do the swap. If things do not get better, we try another swap. And that is how you could, let's say, iteratively do this until you have reached a situation where no swap is improving your result. And what you now immediately see, this is much more expensive than k-means."
3331.76,3361.3,"K-means was very easy to compute. Here you can see that if I have, I don't know, 10 meadowides, so K is equal to 10, and I have a million points, I have 10 times a million possible swaps that are possible. So this is, in principle, it can be very expensive. So this is describing the algorithm, and there are multiple variations possible."
3361.3,3388.93,"But again, I think there's an animation here. So again, we pick K being the set of clusters. We randomly pick a number of instances as our metoids. And then we try these swaps and see whether we can improve. But why does this now work? Because you think this is an incredibly stupid approach, right? You're basically trying. And the reason that this is still..."
3388.93,3416.1,"let's say, providing a reasonable performance, is that basically you do not need to look at everything. You only need to look at if you're making a swap, most of the points will stay untouched. There are only these four situations, and in most of the cases there will actually be no change. So you only need to look at if I'm swapping..."
3416.1,3444.53,a medoid with a normal instance and I'm swapping that I only need to look in the neighbourhoods of these two points because only in these neighbourhoods something can change. That's visualized say here and so the dashed lines are after swapping so P was before closest to OJ. Now OJ is replaced by O random that is now the new medoid.
3444.53,3474.37,"And now P was moving from cluster J to cluster I. So here you can see that it is reassigned. Here, like OJ was swapped by O random, and I have a point P, and it moved from OJ to random, so it stayed in the same cluster. Here nothing has changed. And here there is another point that used to be assigned to cluster I."
3474.37,3521.63,"that now goes to cluster J because this MEDOID was moved from here to here. Yeah? Like this is a brute force approach, right? So here you're basically considering swaps, and what you are suggesting is like a greedy approach, if I understand you correctly, because you decide what to swap. That may work, but it can lead to exactly the same situations that I indicated before. So if you become greedy,"
3521.63,3552.82,"you may miss opportunities to improve. You would run into the same things. So there are two differences. The approach is different, that's one thing, and we no longer have the situation where the centroid can be in the middle of nowhere, where there are no points around it. If there is no swap possible, that would improve the error. Sorry?"
3553.94,3591.22,"There is no swap possible that would improve the error. No, no, but you only take swaps that improve the situation. So you take a swap and you check does it improve the situation or not. If it improves the situation, you take it. Otherwise, you try another one. But there will be a situation where you can no longer improve. Right?"
3591.22,3619.34,"But the trick why this on the one hand sounds very stupid, on the other hand it kind of works, is that you only need, like most of the instances are not influenced at all. Only the instances that are somehow close to the things that you're trying to swap. So you do not need to compute your whole thing again, you only need to compute the difference. And you know, right, right, so here with respect to P."
3619.34,3648.72,"I know whether this is going to improve my situation or not. So you do not need to do everything again. You can compute it very locally. That's the idea. And there are, of course, lots of heuristics to guide. You can basically do any number of swaps. And there are heuristics to decide what are now good ones to avoid, etc. We need to move on. Sorry about that. Good questions. It shows that you're thinking about it."
3648.72,3679.2,"If I apply this to my example that I showed before, initially kind of the points are at more or less the same positions that I had before. Now they correspond to instances, not random darts, but they are approximately the same situation. So this is a very unfortunate starting situation. But still, if I apply K-medoids in this example, I cannot move things to the middle of nowhere."
3679.2,3708.56,"Right? So I cannot move, let's say, a dart in the middle between these two. So this will converge to the situation that you see here at the bottom, which is kind of what you would like to have. Do you see that? So now we have four metoids and we identify these two clusters. Here you can see they're next to each other. Right? So this was what K means."
3708.56,3736.91,"returned in this particular example. And this is what K-medoids returned. And I think everybody agrees that this is better. Also, it doesn't matter what quality measure you take, like the one on the right will always be better. Clear? The problem of the approach that I just indicated to you is that I need to decide K. If I go back here."
3737.07,3766.48,"to this situation, if I pick k is equal to, I don't know, 3, I will get something that is not what I would like to have, right? If I would take k is equal to 5, I probably also have two, let's say, two clusters that are actually more or less almost the same, right? So very close to each other. So agglomerative hierarchical clustering is addressing that problem."
3767.06,3796.77,"And the approach works as follows. You look at the points that are closest to each other and you first merge the points that are in the closest distance. And you repeatedly do that. So you're basically letting things clutter together that are close to each other in terms of a distance, right? It's probably something that you could have come up with. So you can do this top-down and you can do that bottom-up. And so here..."
3796.77,3826.1,Every instance is a cluster and I take things together if they are close to each other. And then I let that clutter together. Top down is a bit more difficult and it's not the situation that we are considering here. So this is the hierarchical approach where first every instance is its own cluster. Then you take instances together in a bigger cluster and you take clusters together.
3826.1,3851.92,"And if you do that, you get a hierarchy that looks like this. So here you see the leaves, right? So all the leaves, these two colors look the same, but they are different. So we start with every instance in its own cluster, and we take things together as long as they are close to each other. The idea is pretty straightforward."
3851.92,3882.96,"Lots of Python libraries are supporting these types of things. And you can combine that with, I don't know, heat maps and all kinds of other stuff. But each time you see shapes like this in the output, then this type of clustering has been used in the background. So also when things come together, that has, let's say, an interpretation. So this means that here we take, if we..."
3882.96,3911.33,"allow a certain distance, then here these two instances come together. And we need to be more flexible, and then this and this comes together. I'm talking about these things, but there was one, let's say, tiny element that I did not mention, but that's very important. Before we were always talking about the distance between two instances."
3911.33,3939.54,"In our first iteration, that is also the case. So D and E are two instances, so we can have a distance measure. But if I want to take this together with this, I have one cluster consisting of two instances and one cluster consisting of one instance. So we need to have a distance notion, not between two instances, but we need to have a distance notion between two clusters."
3939.86,3968.05,"So you can think of a cluster as a cloud of instances. And if I have two clusters and I need to define a distance measure, there are these four obvious choices that you can take. The first obvious choice, if I have this cluster and I have this cluster, the distance between these two clusters is defined as the distance between two points in the two clusters that are closest to each other. So that would look like this."
3968.72,3998.29,"Instead of the minimum, I can also take the maximum. That's what you see here. So the distance between two clusters is determined by the two instances that would maximize the distance between the two. We can use a similar thing like in k-means. So we compute the centroids. So this is the average of the different, let's say, points in the cluster. And we compute that distance."
3998.74,4028.24,"Or we can do the pairwise comparison. So here we are looking at any pair of points where one point comes from one cluster, the other point comes from the other cluster. We look at all the combinations. We add up all of these distances. And we divide that by the number of points in this cluster and the number of points in this cluster. So this would be the average distance between two points."
4028.62,4058.88,"Without going, let's say, to the middle. Is that clear? These are the four ways in which you can compute that. The rest of the algorithm is very simple. In any situation, you have a bunch of clusters. You just determine which are the two clusters that are closest to each other. You have a distance notion between all of the different clusters. At any state, there are two clusters."
4058.88,4085.01,"that are closest to each other, and you first merge these. And after you've merged that, you have one cluster less, and you continue to do that. So you start with the number of clusters equal to the number of instances, and you end in the end with one cluster where everything is in the same cluster. And that leads to this three-like situation that I showed you before."
4085.58,4114.61,"And it has this very, let's say, desirable property that you do not need to decide your k up front. You can just compute the entire tree and you can basically walk through the tree and look at the different points. Is that clear? If I go back perhaps to this situation. So I can cut this dendrogram at this level."
4114.67,4154.88,"Then I have two clusters. I can cut it at this level. I have one cluster. I can cut it here. I have three. I can cut it here. I have four. I can cut it at this level. I have five. So the quality notion that you always have is the, like what is the, for example, the average or the sum of all the distances between the instances and the point in the cluster."
4154.88,4181.76,"But there is of course a trade-off, and that trade-off is very visible here. There is a trade-off if you have as many clusters as the number of instances, the sum of all the distances will be equal to zero. So there is a trade-off between the number of clusters and, right, where less clusters is better for interpretation."
4181.76,4210.37,"But also the average distance to the center of the cluster is another criterion. And these two are competing. If I increase k, I get more clusters and my distances get lower. If I decrease k, it's the other way around. And what you see here is that you generate a diagram like this. And then you should imagine that think of this as a slider. That you have a slider capability."
4210.37,4249.87,"And I can basically, for example, stand at this point and I can inspect the situation with two clusters. I can move it to this. I see the situation in three clusters, et cetera. And what you will see is that the more clusters you have, the better the quality gets. But there is a trade-off. So it's a very good question. There's a slide later. The question is basically how do you now interpret this?"
4249.87,4278.53,"And for example, if you think about the centroid, independent of what you are using, you can always compute the centroid, like the middle of the cluster. And the middle of the cluster is the average vector that is there. So the center of the cluster gives you some ideas of what it means. That's the way. And it's unsupervised. It's up to the user to decide what is meaningful and what is not."
4278.53,4306.19,"That also makes it difficult. So let's keep the speed. So that was the third method. Now we move to the fourth method, that is dbScan. And dbScan addresses the problem that none of the techniques that I showed so far would be able to identify this as a cluster and this as a cluster."
4306.51,4333.47,"Because you always have this distance notion, and none of them would identify this. If you think of these shapes, they are a bit funny, but these are results that dbscan returns, that you get an intuition of what happens. So it would look at this as a cluster, and it would look at this as a cluster. Now think if I would use k-means, for example."
4333.47,4363.5,"you probably have like, I don't know, this as a cluster and this as a cluster. Right? So blue and red dots would end up in the same cluster. So what you would like to have depends on what you want to have. So you're basically now looking at instances that are somehow connected to each other. And you're not looking at the absolute distance because the absolute distance between a dot here and a dot here is pretty large."
4363.5,4388.69,"But this dot here is connected to this dot because there are these other dots connecting it. It's a completely different approach, but also a technique that is widely used. And I'm now going to explain to you how this works. So there are two parameters. There is one parameter that is like the scope."
4389.3,4419.73,"So this is called fixed neighborhood side, indicated by this epsilon here. And these are these circles that you see here. So if you look at the point, there is this notion of having a neighbor. And for every point, like there is this circle around it with a radius equal to this epsilon. And you look at, if I look at this point, then this is like its area. And I look at..."
4419.73,4446.83,"How many other points are in this space? So if we look at n, like it's the only point, let's say, in this radius of n. If I look at, for example, point A, that is this circle, and you will see that there are four points within this circle defined by A. Clear? So we have points. Every instance is a point."
4446.93,4477.3,And we look at the radius around the point and we look at how many other points are there. And then we try to connect them. And what you visualize here is that this would then in the end become a cluster and N would not be a cluster. So how does this now work? So these are the definitions. It's a bit tricky in the beginning. So point Q is directly reachable from point P.
4477.33,4508.32,"If point Q is within distance epsilon from point P, and P is a core point. And I'm making this statement here that this is an asymmetric definition. And that means that, let's say in this example, P needs to be a core point. And a core point is a point that has at least the minimal number of points around it. So a core point is a point with..."
4508.32,4538.93,"sufficient instances around it. And we say another point is directly reachable, let's say, if it is within this distance, epsilon. And we can now apply this recursively to talk about something is reachable. So we start from a core point. So we start from a point with sufficient neighbors. And we look, is there now a path to another point that only goes through core points? But the last one,"
4538.93,4570.66,"does not have to be a core point. If we decide a core point is in, we still also take all the instances that are in the environment, even if these instances do not have sufficient neighbors anymore. If points are not reachable from any core point, they are outliers. These we do not try to add to a cluster. This is, again, sketching the situation."
4570.66,4596.46,"So here the minimum number of points is equal to 4 and all of these circles are indicating what the epsilon is. So for example if I look at point B, point B is not a core point because in the space defined by B there are only two points. Point C is also not a core point because if I look at C and I look at this radius there are only two points."
4596.66,4625.57,"So the yellow dots are not core points, the blue one is also not a core point. But for example, A is a core point, because if I look at A, like the radius in A, there are four instances in between it. So A is a core point, and now if we talk about reachability, we see that from A we can reach B, because the points in A and the points in between are core points, and B is not a core point."
4625.57,4657.26,"So I can go from A to B, but I cannot go from B to A. Do we see that? Because A is not the core point. So we also start in the core point, and we can only walk through core points. So at the boundaries, we may have these points like B and C. So this definition was, let's say, asymmetric, as I indicated before. But if we talk about..."
4657.26,4687.04,"let's say whether things are reachable from a core point, then that definition is symmetric again. That's a basic idea. So if we apply this type of clustering, where for every core point we try to see what other core points are reachable, and we continue doing that, and for all the core points we also look at the things that are in the surrounding."
4687.04,4716.83,"Then we get a cluster that has the following two properties. That all points in the cluster are connected, meaning that there is a core point from which these two other points are reachable. And any two points that are connected through such a core point are part of the same cluster. So these are, let's say, some desirable properties that you would like to have. So this is the algorithm. And the algorithm..."
4716.83,4745.36,"again does not guarantee an optimal solution and it's also non-deterministic because we simply pick points to start with. So we pick a point and if the point is not a core point, it is considered to be an outlier. If we randomly pick a point initially, an instance, and it is a core point, we grow a cluster around it."
4745.36,4775.17,Because that point has many neighbors and we look at all the neighbors and we again look at what are neighbors that have many neighbors and you continue doing that and you start adding these things until it stops and then you start looking at again another point that you did not consider yet. So this is non-deterministic because which point you put at the beginning can be there. Perhaps I can show that here. Suppose that B
4775.17,4803.68,"Suppose that there would be another cluster here of strongly connected points. Then it would depend, do I start now with A or do I start with one of these, to which B is added. So initialization really matters. Yeah, clear. So that was the fourth method. That you need to understand how it works if we give you a situation."
4803.68,4832.75,"that you should at least be able to walk through some of the steps. The part where I will be a bit more vague, but hopefully is very, let's say, intriguing to you, is something called the self-organizing map, which fits perfectly in the topic that I'm now explaining to you. So, the question earlier was like, how to interpret these clusters, right? And that's the difficult question that you need to address."
4832.85,4861.09,And the idea of self-organizing map is that you get such an interpretation. And let me simply show you how it works. So we have a two-dimensional grid. They are called neurons. That's why I briefly mentioned that when we had a lecture on neural networks. So these are called neurons. And every neuron is represented by a vector. So these nodes...
4861.09,4893.58,"You can think of the neurons and they are indicated by a vector. And you can think of this as a centroid. And the instances, and so all of our instances have exactly the same dimensions at this point. So every point is a vector that looks exactly the same as the input. So we have a two-dimensional grid and every point is a vector and also every instance is a vector."
4893.58,4920.98,"And now we do the following. For every instance, and so we randomly pick an instance, and we look at, for this instance, what is the node that is closest to it. And again, assume that all the nodes are having random values. So you can think of all the nodes as random darts in this n-dimensional space."
4921.07,4948.27,"For every instance in my input, I just pick the one that is closest. And if I start with a random situation, then one of them will be the closest. And this is called the best matching unit. So again, every instance, I assign it to the node that is closest. Every node is represented by a vector."
4949.2,4979.86,"And if I find for an instance what is the closest to it, I update the network. So if for a particular instance I find that this node has a vector representation that is closest to my input, so if this one is closest, I update it and I bring it closer to the instance. So if I have an instance, I just look at what is the node that is closest."
4979.86,5009.5,"And I basically adapt that node to be closer to my input. And then something very interesting happens. So you're updating the vector representations of all of these nodes. And the node that is closest, you update that the strongest. But also the surrounding nodes, you update them, but you update them to a lesser degree."
5009.5,5039.41,"To visualize this, every instance has an impact on this, let's say, grid of nodes. But it is impacting the node that is closest to the instance as much as possible. And please note, it's not a distance notion, let's say, in this space. It's a distance notion. Every node is a vector. Just wait a second. There's a picture that shows it."
5039.89,5065.65,"I take one instance and I update all the vectors corresponding to these nodes. I update this the strongest and if they are further away, the update will be weaker. Then I take the next instance and I again update the nodes. I continuously do this until it stabilizes. Over time, to make it stabilize,"
5065.65,5094.51,"Like the ups days that I make, I make them smaller and smaller to avoid that I'm, let's say, iterating between two different instances. So this is describing the process that I just indicated. So again, you take an instance and then you update the nodes that is closest and the nodes that are around the closest node to a lesser degree. And then something..."
5094.51,5121.3,"very interesting happens that it's very difficult to visualize. So I took these pictures from Wikipedia. What you see is that this two-dimensional grid is kind of wrapping itself around the instances. But it's still in a two-dimensional grid that you can interpret. So in a way, this self-organizing map is wrapping itself around, let's say, the data."
5122.03,5152.85,"And of course this is very difficult to imagine for you, but just accept that and in a second I will show you an example. So what we now get is that every of these cells here has a bunch of instances that are closest to it. And what you can see is that what I'm showing you here is where the color is dark, the distance between two neighboring nodes is the largest."
5153.55,5183.66,"So what you now see is that here the distances are very small, here the distances are very small, but here there is a change. And what is a change? Each of these nodes has a vector representation, and I can simply look at when does a node have lots of values that are very different from surrounding nodes. And then we see that the self-organizing map found something that apparently this, you can think of that as a cluster, and you can think of this as a cluster."
5184.43,5212.67,"Very intriguing, right? So we find the cluster here, and we find the cluster here. And for every instance, we can decide, okay, what is now the node that is closest to it? So for every instance in my input, I can assign it to one of these cells. I can also reverse it. I can click on one of these nodes, and I find the instances that are closest."
5212.67,5241.07,"I hope that you can see this is a form of clustering. That is again very different from what you have seen. And this provides for, let's say, extremely interesting visualizations. So here I've identified these two clusters. The ones in the middle I'm not so sure about. But I can now color these cells based on any computation over the instances that are assigned to it."
5241.26,5271.58,"So think of this as, I don't know, every student is assigned to a cell, and now based on what kind of courses the students took, etc. But now the color of the cell, I can use that for clusters, but I can also use it for, for example, measuring what was the duration of how long did the student take to finish his curriculum."
5271.58,5301.78,"Once I have this grid, I can project any visualization on it. It can be categorical features. It can also be numerical features. If we have found these two clusters of students, then we may find the students in this cluster are much faster than the students in this cluster. At any point in time, I can click on a note and see what are the instances that are underneath it. This is a very difficult view. I understand it."
5301.78,5330.74,"hopefully helps you. And this is pre-Trump. But this is an attempt to try to visualize the voting behavior of the members of Congress in the US. And how did this work? So every member is represented by a vector. And the vector is basically saying whenever there was a law and people had to vote on the law,"
5330.74,5357.82,"they could say yes or no. Right? So every member in Congress is represented by a vector, and the vector is indicating did they vote against a particular law or in favor of a particular law. So this is how you should visualize this. Now I use self-organizing maps. The only thing that I need is this vector. Now I play this game, and now..."
5357.82,5389.52,"this two-dimensional space starts to wrap itself around these over 500 Congress members. And if I do that, then I find, so this is again showing this distance, so after I apply this, I apparently find two groups of Congress members that have, let's say, similar behavior. Like here, the differences are the largest."
5389.94,5420.21,"And now I can, let's say, show you something. So this is here, I'm done. I have my two clusters. But now I can use any of the attributes to color all of these cells. And what you find, for example, so these are the clusters that are being found. You understand that there is a separation here because the darkest part is here. And if you now project, so I did not use the party."
5420.21,5447.68,"to which party people belonged, but you automatically find, so here I'm coloring the different cells based on whether they are Republican or Democratic, and you can see this clustering, although not using the party, converges to clusters that have these parties. And I can project now any, so I can look at any law where there was a vote on."
5447.68,5477.28,"And these people, behind these cells, there are Congress members, and these voted yes, and these voted no. So you sometimes see that there are groups like that. So here you can see that several, let's say, Republicans were voting for legislation where the majority of the Democrats were in favor. And I can look at any law that has passed, and I can look at, let's say, particular patterns."
5477.28,5510.16,"So here you can see that just a few, I always forget whether what is yes or no, but you can see that here, like there was one big majority and a smaller minority, and you can see that this minority was in the Democrat Party, not in the Republicans. Does this make sense? Yellow is that it is mixed, because behind every cell, there are Congress members behind it."
5510.16,5539.98,"And it could be that behind the cell there are ten Congress members where, I don't know, four voted in favor and six voted against. And this yellow color, as you can see on the color scale, as I said, I constantly forget what is yes and no, but one is yes and the other is no. And this means if it's yellow, then it means it was 50-50 of the Congress members that were assigned to that cell."
5543.79,5571.58,"The X and Y, that's the interesting thing, they mean nothing. They mean absolutely nothing. But what you can see here is you can see interesting outlier patterns because you deliberately did not pick X and Y. And that makes it difficult, but you get this, let's say, flexible visualization on top of this."
5571.58,5608.72,"Let me rephrase. If I would normally show you instances, and I have a million instances, or in this case I would have 555 senators, and I would visualize them, I would not see anything. And now you can see something. So typically if things are sparse, then for example this cosine similarity works better than the Euclidean distance."
5608.72,5638.78,"Later we will talk about embeddings, and typically if you have spaces which are very sparse, you first apply an embedding and then you start doing these types of things. So there is a pre-processing step. The dots inside the cell, behind every cell there are zero or more Congress members. But you cannot see how many there are in this visualization."
5638.78,5669.87,"But just for your understanding, there are 500 Congress members behind every picture. But how many they are, you need to basically click on it and see what is underneath it. And I suspect, I'm not 100% sure, that it could even be that these ones that do not have a dot, that actually there is nothing behind it. That in this dimensional space there is like a gap there."
5671.25,5681.52,"Okay? Sorry that I've been running a bit late, but it was good to, I think, also come to this point that you understand why. See you next week."
