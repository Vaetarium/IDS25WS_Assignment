start,end,text
3.15,32.27,"Yeah, so time flies. Already lecture 10. So we are, let's say, almost halfway. The lecture series already. Because I think that, if I'm not correct, I think there are 22 lectures. So today we continue with unsupervised learning. So in the last lecture we looked at clustering. One of the things that I did not say at the end, but you will see that many, let's say, slides are ending with..."
32.27,61.7,"let's say, pictures of cocktails. And the thing is that I want you to think about how you can combine these various techniques. So last lecture, we saw clustering. So how can you use clustering? You can use clustering to kind of put instances in groups that you analyze separately. So if you have identified three clusters, then you could add that maybe surprising."
61.7,91.68,"And you get the feel for the data. And then for each of these clusters, you could again use visualization. You could also turn, if you have found that there are, for example, three clusters, then you can label all of your instances with the cluster attribute. So you get an additional feature, which is the target feature. And once you have that target feature, you can again use supervised learning for that. So what defines now?"
91.68,119.47,"the cases that are in a particular cluster. So I want you to think about combinations of all of these techniques. Today we start frequent item sets. That is finding what are combinations of values that happen frequently together, but also representations. If you have found out, okay, these are interesting frequent item sets, you could again use that for clustering. Or you could again use that"
119.47,147.63,"for SVMs or regression, etc. So I want you to think about it in this way. So this is what we do today. I briefly say something about pattern mining. Depending on the data, you can identify different patterns. Then I define what are frequent item sets. And then you will see two algorithms, a priori and FP growth, where a priori is relatively easy."
147.63,176.83,"And FP growth is relatively complicated, right? So there you really need to pay attention that you understand how that works. These algorithms, I think, are very interesting because they show you mechanisms that you cannot only apply to item sets, which is the focus that we focus on today, but the same principles can be used to find patterns in graphs or find patterns in text, find patterns in images."
176.83,206.19,"So the principles are very general, but we focus today just on these frequent item sets. Again, as in many lectures, I show you a bit where we are in the various books, and this book is kind of the main book that we are using if we talk about unsupervised learning. But as I promised at the beginning, I try all of my slides to be self-contained, that you only have to go to these things if you would like to dive deeper."
206.77,235.9,"Okay, pattern mining very broadly. So you have data and you want to find patterns in it. And today we focus on frequent item sets. In the next lecture, we will focus on association rules. And an example of an association rule would be that, I don't know, customers that buy diapers also buy beer. Right? So it's a rule that if people have this, then they also do that."
235.9,264.75,"Or if you would bring that to, let's say, study programs, people that take the course IDS and take the course BPI typically also take the course advanced process mining. So you want to find patterns like that. And that's when I talk about Wednesday, I will explain this. There are also patterns in sequences. So all the things that I talked about before, I was not talking about an order."
264.75,291.12,"For example, the order of courses. If you think about sequences, then, I don't know, customers that do this and that, after they have done, let's say, A and B, they typically also do C, right? And that's the topic of sequence mining. Then there are partial orders, and you can go to graphs. So you can look at all kinds of, let's say, data structures, and you would like to find patterns in them."
291.98,317.3,"There are two main challenges, and today we focus very much on the first one. That is the challenge that you have. Typically your data is large, right? It can be that you have millions of instances. But even if your data is relatively small, that you have like just a couple of hundred of instances, if you have many attributes..."
317.3,344.08,"then the number of possible combinations is exponential or even factorial. So even when your number of instances is relatively small, the set of potential patterns is huge. And I will give you some examples of that. So for example, if in computer science there are just a couple of hundred of students, that is relatively limited."
344.08,367.73,"But if you look at the number of combinations of courses that you can take, the number of possible combinations of courses is way higher than the number of students that we have, right? And if you go to patents, you end up in these problems. So when I talk about, for example, the a priori principle, that is a way of dealing with this problem, like a very general way."
368.66,398.35,"The second thing, and we will not dive into that today, but we will do that when we talk about association rules, is how interesting is a pattern. So if you have a large number of potential patterns and you find thousands of patterns, how do you now decide what are the things that are interesting? And we will look at ways of, let's say, quantifying this in such a way that you can look for the patterns that actually..."
398.35,424.88,"like have value for you. So as I said, when we are talking about these things, we will talk about item set sequences, event data, and time series, right? These are topics that you will see later in this course. Today, we just focus on item set data. But I would like to give a bit of a preview of these other types of data, just when we dive now into item sets that you can see."
424.88,455.31,"Later we will look at other types of data. For example, event data. So what is event data? That's very common data where you have, let's say, a bunch of features and you have a timestamp and every row in such a table corresponds to an event. If you have such data, you see behavior and in this behavior you would like to identify patterns. So again, what do we see? Every row is an event, something that has happened."
455.31,484.14,"Events are ordered based on their timestamps, and you typically group them based on some feature. So for example, if this would be study behavior, then I may want to take all the events belonging to a particular student together. And I see that they belong together, and I will exploit that in my analysis. That's what I just said. So then you could have data that looks like this."
484.14,513.52,"So we will look at finding patterns in event data later. Then there is time series data. It looks very similar, but the nature is very different. So we have timestamps. And for every timestamp, we have a bunch of features. But these features are numerical. They are not categorical, like take this course. But they are, let's say, some measurement that is being taken."
513.52,543.49,"And the timestamps that we have, have like a regular distance. So for example, if you think about, I don't know, software to monitor, let's say, in an ICU in the hospital, like every, I don't know, one minute a measurement is taken, right? And then you get the time series. And it's not surprising that the measurement is being taken, right? It is planned that it's taken and you measure the value. If you look at event data,"
543.49,570.86,"The fact that it happens means something, right? So it looks very similar, but the nature is different. Today, we will make it much simpler, so we will not consider a time component. We just have, let's say, columns that correspond to items. And when I say items, you can think of courses, you can think of products, right?"
570.86,594.51,"Think of, you can think of songs, you can think of movies. But the thing is that all of these columns are somehow comparable, right? So this could be movie one, movie two, movie three, right? Product one, product two, product three. So they are very, let's say, comparable. Every row is a transaction."
594.51,624.43,"And you can think of this, I don't know, a customer going to a supermarket and buying a bunch of products. And as I said, all of these features may refer to a product. It could be a disease, a song, a course, an error code. But they are comparable somehow. And then every cell is indicating whether for this transaction, whether this item is present."
624.43,652.64,"And there are two situations. We can look at it as booleans. It's present or not. Or we can think of it as numbers. Right? So, I don't know, a user has played a particular song on a particular day. Right? That would be like a boolean. Yes or no. But if you go to a supermarket, then, I don't know, a person could have bought, I don't know, ten bottles of beer. And then there would be a number ten here. Right? So..."
652.64,679.47,"These are the two situations that we will look at. And then the best way to, although the application is much broader, you can apply it to education, to healthcare, I don't know, to movies, to whatever. But the typical example that we will look at is that we look at somebody going to a supermarket and buying a bunch of products. And here you can see it."
680.18,709.3,"And here you can see that of the same product, a person has bought, let's say, multiple items of it. And this data can be mapped onto a table like this. So now somebody has been in the supermarket and has been buying these products. And what you see here are quantities, but as I said, you can also think of it as Boolean values. It's present or it's not. So that is..."
709.3,738.99,"The type of data that we will look at all the time, so if you have numerical values, of course you can turn it into Booleans, but if it's Booleans, of course you cannot turn it into the other way. That is the basic thing. And as I said, this is very broad, although we often talk about, I don't know, let's say a shopping basket, what is in it. You could think of restaurant bills, where you can see what people have been eating."
738.99,767.46,"You can look at patients, what kind of diseases people have, students, what kind of courses, repairs, what are the components that are being replaced. I don't know, Spotify users, the song played, et cetera, et cetera. So these types of data are omnipresent. You just need to look for it. So this is what we are now interested in. And today we just focus on something very simple."
767.46,796.53,"We look at the set of items and we want to decide whether it is frequent. And the challenge that you will see is that you have, let's say, lots of possible combinations. And that's the challenge, how to deal with this efficiently. So, for example, this is an item set. And if it's frequent, that means that many customers have been buying this set of products."
797.42,827.54,"And we will use that as a basis in the next couple of lectures to look at more complicated structures like association rules, sequences, etc. But we start very, very basic. So let's be a bit more technical. So I is the set of all items. And then any subset of this would be an item set. So here you can read, I don't know, beer, diapers, whatever."
827.66,857.25,An item set is a set of these things. A transaction is a non-empty item set. We can only see a customer if the customer has bought at least one product. Otherwise it will not be in our data set. A data set D is a collection of these transactions. And of course there can be two customers that are buying exactly the same set of product. There can be two customers.
857.25,886.69,"that both buy diapers and beer. So that's why we have a multi-set. So technically, our data that we use as input is a multi-set of sets of items. And why is this the case? In this first abstraction, we do not care how many products of a certain type people have bought. We just look at is it there or not."
886.69,914.48,"But we care about that multiple customers may buy exactly the same set of products. And that's why we have a multi-set of a set of items. So that's the setting. So to visualize it, now D, what I said earlier, D is a multi-set of a set of items. So because it's a multi-set, we have these square brackets here. So we have a square bracket here and at the end."
914.48,944.85,"And then this is a multi-set of sets of items. And now you can see that there are two customers that have been buying milk. They are exactly the same. And if a D would be a set, if I would have here the symbol P, that would not be possible. So we have multi-sets because two customers, and here you can see an example, may buy exactly the same product. Clear? So that's why we have this representation."
945.3,975.92,"So if I look at this table, this table is represented by this mathematical construct, a multi-set of sets of items. So we can look at this where this is a set of items. And if you look at this example, so for example the first customer, we just have Bitburger and Penne."
976.05,1006.11,"And we lose this information that the customer bought two. And that is because there is a P here. We look at the set of items. It's also possible to replace this symbol by a multi-set. So now we have a multi-set of a multi-set of items. And now what you see here, there is this two here that is indicating that we are recording. This first customer was buying Bitburger and Panne."
1006.11,1036.72,"but he bought two beer, right? And here there is two melo, et cetera. Clear? So these are the two representations that we can use. Yeah, yeah, yeah, yeah, yeah. So like in multi-set, you often use a superscript to indicate how often something has happened. So if I go back, perhaps, oops, what did I do now? That's not good."
1042.13,1071.34,So for example here I have two sets of milk. I could also represent this that I would drop this set here and I would write a 2 at exactly this position. That would mean that I have two of the same. It's the same principle. So now we can talk about support and this looks complicated but it is super simple. We just look at how often does a subset now appear. And you should look at this symbol.
1071.34,1098.7,"So we are interested in, let's say, looking at transactions in our database where at least X is included. So this is, in other words, talking about the fraction of customers because I divided by the number of transactions. So this is the fraction of customers that bought at least X."
1099.18,1127.74,"And if I look at support count, I'm counting them. So again, D is our database. It's a multi-set of sets. And now I look at how many customers have been buying, let's say, the elements in this set X. And they should have bought everything that is in X, but they can have bought more. And again, if you understand the mathematics, you just need to look at this symbol."
1127.74,1159.47,"X is a subset of A. So it may be that this customer has been buying more products. But at least X should be in there. And now we are interested in the instances, like in the number of instances or the fraction of instances that is meeting a certain threshold. So we set the threshold and we are now interested in all sets that are frequent."
1159.54,1190.32,"That's the basic idea. And we always have, like, we can look at a relative notion where we get a number between 0 and 1, and we have this support count where we do not divide by this, and we get an absolute number that is between 0 and the number of transactions that we have in our database. Clear? Just to get used to the notation. So if we go back to our example, then the support of Merlot, the set consisting of just Merlot,"
1190.32,1220.53,"is three out of four, because we have four transactions, that D is consisting of four elements, and three of the four elements contain Merlot, this one, this one, and this one. If we look at the support of Bitburger and Penne, it is one out of four, right? So again, we have four transactions, and now we look at the transactions that have Bitburger and Penne in it. So this one,"
1220.78,1247.15,Has this included? This one not. This one not. Because Bitburger is missing here. And this one is also not. So only one out of four. Do we get it? So everything in this set needs to be included. Not just a subset. Everything should be included. Yeah? Clear?
1247.15,1277.1,"And again, we can look at a relative notion and we can, let's say, count the absolute number. That's very easy. So our problem statement is now very simple. And that's what we talk about the rest of the lecture. We have a database where we have a multi-set of sets of items. And we want to find all the item sets which are frequent. We now want to find..."
1277.1,1304.34,Think about the supermarket. We now want to find all sets of items that have been bought at least a thousand times together. And we set a threshold in such a way that our number of patterns that we find is still manageable. And we can play with this. And that sounds like an incredible trivial task. You just have...
1304.82,1333.39,"Like a database and you just want to count how many customers have been buying at least one product. And if you would ask that for one item set, that is very easy. You just walk through the database and you can answer the question. However, the number of candidates is very high. Because if I have m unique items and I look at the non-empty sets of items, this is my number of candidates."
1333.78,1358.32,"So to check it for one item set is easy. One pass through the database and you're done. But you need to check it for 2 to the power m. So if you use a brute force approach, that is never going to work. If m is, let's say, a larger number, which it typically is. So that's the problem. So think about the supermarket."
1358.32,1384.37,"You can, depending, so Aldi has less products than Rewe or something like that. And Dutch Albert Heijn has even more items. But like a modern supermarket has in the tens of thousands of different products that are there. So if you look at the item sets that can be generated, it's 2 to the power 90,000, if that would be in this case."
1384.85,1411.98,"Let's assume that we have a supermarket, let's say Rewe, that has 50,000 different products. So what is the number of candidate item sets? It's 2 to the power 50,000 minus 1. So for you it is difficult to imagine that number, but here I wrote it down for you. So this is the number."
1412.14,1438.83,"So I'm showing you this, how absurd if you do this naively, you will never be able to do anything meaningful, right? You have no idea where to start. Yeah? So if you do frequent pattern mining, you always have this problem that the number of candidate patterns is huge and you cannot just try all patterns and then you're done. You need to do it more smartly, right? And I hope..."
1438.83,1463.1,"This sticks in your mind that you need something like that. So that will not work. But of course, most of them are not frequent. If we set the threshold properly, most of them probably never happened. There are not so many people in the world. So it's impossible. The number of..."
1463.1,1493.01,"possibilities of going to a supermarket is much larger than the number of people in the whole world, right? So you will never see all of them. So you only see a tiny subset. So you need to do this, let's say, smarter. I start with some definitions and then I go into two algorithms that actually do that. So a first observation for you to look at is that if we look at the number of closed item sets,"
1493.04,1521.1,"If we only store these, we can derive the frequency of any item set. Let me explain how this works. So an item set X is closed. If there is no proper superset, so there is no other set that is larger than it, strictly larger, that has the same support. So this means that X is closed."
1521.46,1549.01,"that if I want to add one more item, the frequency will always go down. You need to realize if I look at the number of customers buying beer, it's higher or the same as the number of customers buying beer and diapers. So if I add an item to my item set, the frequency will go down normally. It cannot increase. It can only go down."
1549.04,1578.56,"And closed item sets are pushing the boundary. So you look at an item set and you cannot add any element to it without reducing the frequency. That's the setting. So this is the formal definition. And the interesting thing is now that just storing the closed item sets, you are able to decide for every item set how frequent it was. So this is the way."
1578.56,1605.62,"of, let's say, making this more compact. Then there is a definition of closed frequent item sets. These are closed and frequent. So they are above our threshold. And if I want to add an item, it doesn't matter which one, my frequency will go down. So that's closed frequent item sets. And then there are the maximal frequent item sets."
1605.62,1635.34,"And if we have set this as a threshold, so we are interested in the patterns that at least meet this threshold, making the item set larger will always lead to a reduction of the number of transactions. It can never lead to an increase. And maximal frequent item sets are pushing against this boundary. So these are sets that if I want to add another element, I always make it..."
1635.34,1660.88,"infrequent. These are item sets that are still frequent, but if I add one item, it doesn't matter which one, I go over this border that I said before, and it becomes infrequent. Do you see that? I can show you some examples, but the thing that you hopefully get is that"
1661.23,1686.5,"Right, these item sets, maximal frequent item sets are also closed, but not all closed item sets are maximal frequent, right? Because there can also be closed item sets here and there can be closed item sets here. Yeah, so let's assume an example. So we have, let's say, 100 items, A1 until A100."
1686.5,1717.3,"We have a database where 10 customers have been buying A1 until A50. 10 customers have been buying everything. And I set this as my threshold. So we want that the support is 25%. So we set this. So how many frequent item sets are there? This is the number, right? Because I have the threshold that is very low."
1717.3,1748.46,"So for example, if I take the item set A100, there are 10 customers buying already that. If I look at A99 plus A100, there are 10 customers, so I meet the threshold. So basically any subset is frequent. So this is showing an absurd number of frequent item sets. But now we look at the number of closed frequent item sets, and there are only two."
1748.46,1779.22,"So this is a closed frequent item set and this is a closed one. And these are the only two. And with these two closed item sets that I have here, I'm basically able to derive all of the other frequent item sets. Because if I know the frequency of these two closed item sets, I can determine the frequency of any item set. And this is why this is important."
1779.86,1808.43,"So the support of this item set here, A1 until A50, is 20 over 20, right? Because we have 10 customers buying this and 10 customers buying this. So everyone is buying A1 until A50, right? So we have a support of 1. If we look at B, the entire set, then we can see that these 10 customers have been buying this, these 10 customers not."
1808.43,1838.21,"So we have a support of 0.5, both meeting the threshold. Just look at this item set. If I'm adding now A51, my support immediately goes to this value. So hopefully by these examples you understand the number of closed frequent item sets is much smaller."
1838.21,1865.9,contain all the information that we need. We can determine the frequency of any item set. There is one maximal frequent item set and that is this one. That is pushing as a boundary and I can no longer extend this because I've used all the items. Let's take another SO here. I'm increasing the threshold.
1866.45,1897.42,"Now there are 2 to the power 50 minus 1 frequent item sets, and there is only one closed frequent item set, and there is just one maximal frequent item set. To show you, there are many frequent item sets, but we can represent them in a compact way. Clear? Let me make it more difficult. We now have 10 customers buying the first 99 products."
1897.52,1927.33,"And 10 other customers buying the last 99 products. So A2 until A99 are bought by all customers, if you look at this carefully. But just half of the customers have something involving A1 and just half of the customers have something involving A100. Again, the number of frequent item sets is very..."
1927.33,1957.78,"high, right? So as long as we do not do A1 and A100 and we look at any subset, we are okay because it's bought by all of the customers. So again, many frequent item sets. There is just one closed item set, that is this one, because if I add A1 to this, right, my frequency goes down. If I add 100, A100 to it,"
1957.78,1986.58,"my frequency also goes down. So one closed frequent item set, and this one is also maximal. These things are a bit artificial, but the thing that you should get from it, there are many item sets possible, but if you start looking at the closed ones, it is relatively limited often, right? Because you look at the things that are pushing the boundary, and..."
1986.58,2011.34,"Like if you think about the techniques that we show later, they are, let's say, exploiting this in some way. So this visualization may help. So here are all frequent item sets that may be huge. Then the number of closed frequent item sets is already much larger. And the number of maximal frequent item sets, they are also closed and frequent, so they are also there."
2012.14,2052.21,"And the interesting thing is now if you know the green ones, you can compute the frequency of all the other ones. So that is what the algorithm should think about, how they can exploit this. And we will see a bit of that. But there is this observation that you do not need to look at everything. That there are more compact ways of storing this."
2053.3,2080.53,"Okay, so we now have two algorithms. The first one is relatively simple. The second one is very complicated. So the first one is based on the a priori principle. So it's based on this paper of 1994. For a long time, it was one of the most or the most cited article in computer science. Now there are other articles."
2080.53,2106.22,"But it is using an approach that is exactly solving our problem. And why is this paper influential? Because it talks about a principle that is not only applicable to item sets, it's applicable to many different things. And here I try to visualize, let's say, this general principle. If I have a bigger pattern that is frequent..."
2107.22,2134.5,"then if you have defined things in a particular way, then all of the sub-patterns in it are also frequent. If this is frequent, then all the parts of it should also be frequent, and this should also be frequent. If there are many customers buying A, B, and C, then there must also be many customers buying A and B."
2134.5,2165.87,"And there must also be many customers buying A, which is trivial, right? You can apply this in the reverse direction. So you know that if there are few customers buying A, I know that I do not need to bother about these, right? So if an item set is infrequent, then anything containing it is by definition also infrequent, right? And that's what these algorithms are exploiting."
2166.13,2195.12,"And that saves a lot of work. So this is formally describing this. So again, this is our input, a multi-set of a set of items. We have a threshold. And we know if A is a subset of B, that the support of A must at least be the support of B. So if I make the item set bigger, frequency can only go down."
2195.34,2224.05,"So that means that if A is a subset of B, and the support of B meets the threshold, then the support of A must also meet the threshold. And I can of course reverse this in the other direction, and this is actually the most important observation. If A is a subset of B, and the support of A is already below the threshold, then the support of B will also be below the threshold."
2224.05,2251.39,"So in other words, if we already know this, then we do not need to consider B, right? Because it can never be, let's say, above the threshold. So that's the mechanism that is being used. And it is used in two ways, right? And if you think about it, it's very simple. So it is using, let's say, the notion of leveling. So LK..."
2251.39,2277.94,"are all the frequent item sets that have a size K. So we start with item sets of size 1, then we go to item sets of size 2, item sets of size 3. So LK are all the item sets of a certain length, and this is what we need to compute. And note that, for example, if I take L, L is 1, it is very easy."
2277.94,2306.45,"For every item, I just go through all the transactions and I count how many customers have been buying that product. So to compute L1 is very easy. For every product, you walk through the database and you look at what is the percentage that I've bought this item, and you add them to this set. And the set cannot be bigger than the number of items that we have."
2307.15,2335.89,"So that's the first step. But now we look at, let's say, going from LK to LK plus 1. And there are these interesting observations. So for any A element of LK, there are, of course, two subsets that contain one element less, such that the union of these two is again A. It must be."
2335.89,2366.61,"If I have a set of length K, I should be able to find two sets that have one element less, in such a way that the union is there. And I will also give you a way to construct this. For any A element LK, and I have, let's say, two, in such a way that A is the union of these two, and these both have, let's say, one element less,"
2366.61,2398.29,"then they must be in this set. So this gives us a way of if we have A prime and A double prime, then based on this, we can construct the A. And this is a mechanism to construct any A. And what hopefully makes it easier for you, if I just assume that all of our items are sorted, as I can just, I don't know, alphabetically sort all the different items. So in any set,"
2398.29,2426.22,"like the ordering is based on the lexicographical ordering of the product names, for example, then if I have a, then I can look at a prime that is the same as a without a k, and I can look at a double prime that is the same, but without a k minus 1. Whenever I have a set,"
2426.22,2454.35,"I can look at the set without the last element, and I can look at the set of the one not without the last element, but the one before last element. Right? And I can apply, of course, this in the reverse direction, because if I now take a look at the union of A prime and A double prime, I get A. So I gave you a way of deterministically constructing A."
2454.35,2484.34,"from a single prime and a double prime, right? So if I can give you a mechanism to construct this from things that have one element less, I can simply use that. So I know that any item set of length k is, let's say, composable of, let's say, a prime and a double prime."
2484.34,2514.69,"And that's what is written on this slide. So this gives me a way of if I'm interested, so I know all the item sets of length k minus 1, and I now want to compute the item sets of length k, I know that I can create all of my candidates without actually looking at the database. So I can construct all candidates of..."
2514.69,2542.21,"length k from the elements of k minus 1. And so any a of length k can be obtained by combining these two. Of course, if I take an a prime and an a double prime that are frequent, I do not know yet whether these are frequent. But what I know is, if they are frequent,"
2542.21,2570.42,"it is constructible in this way. So the implication is only one direction. So CK is the number of candidates, right? So if we construct an A in this way, we know that we are covering all potentially frequent item sets, but we do not know yet whether they are frequent. Do we get this? Step one, yeah?"
2575.86,2606.16,"No, because in our construction, we select them in such a way that they are identical apart from their last element. So that's a deterministic way of how to create them. So this way we get all the candidates. And the thing that you should note is I generated candidates without looking at the database."
2606.26,2636.83,"Note that the number of candidates of length k is much larger, probably. But I'm creating possible candidates. So this is one step where we are using a priori. Now there is the other step where we are using a priori. Again, without going to our database, we know that if we look at a candidate,"
2636.83,2664.46,"length k, we know that if we remove one element it has to be frequent. So if we have a is an element of ck, we can look at k item sets that have one element less and we can verify whether they are frequent. If one of the subsets of a is infrequent, we know that a cannot be frequent."
2664.62,2695.94,"So we can do this test. So for any element A, we look at whether it is frequent. If it's not, we can remove it. And again, the thing that you should do is that we are now talking about item sets of length K, and we still did not look at our database. We did not do this expensive operation of going through our entire data to check whether it's frequent. So this is step two."
2695.94,2725.33,"So in step 1 and step 2, we are using this a priori property. And then only in the last step, we need to check whether these candidates are actually frequent. And then we need to walk to the database, and then this is what we compute. So we now moved from item sets of length k minus 1 to item sets of length k. And now, of course, we go to k plus 1, etc."
2726.26,2760.91,"Do we get this? Right? So you should be able to do this with pen and paper. Yeah? No, no, no. Because it's like we have a constructive approach, what you see here. Right? So we know that these two are frequent. But if I now, for example, remove A2, right, combined with AK, I do not know whether that is frequent."
2763.41,2791.92,"So think about it. We only know it for these two subsets, but it should hold for all subsets. And for example, it could very well be that the set without A2 but with AK, that that one is infrequent. And that's a mechanism to do it. Later there is an example, it's a bit hidden, but where you can see that it actually applies. Yeah? Clear?"
2791.92,2818.45,"Okay, so this is now the algorithm. I'm not going to spell it out, but it is going through all of the different phases. So we first create candidates of length k based on the frequency item set of k minus 1. So we use a priori once. Then we, let's say, remove individual elements, check whether it's still frequent."
2818.45,2848.69,"And only in the very last step, we go to the database and check whether it is actually frequent. And this example is not so elegant. It's taken from the book. So here, this is our database. So these are our item sets. So it's a multi-set of items. Just forget about this column. That's just an identifier, for example, of the customer."
2848.69,2878.72,"So we have a multi-set of items and we have a threshold set. To be honest, I'm not completely sure about the threshold. I think the threshold is two, right? So now we look at all the items of set one that are actually frequent. So the first step is quite trivial. And we find that I1, I2, I3, I4 and I5."
2878.72,2912.59,"have a frequency of at least two, right? So for example, is there an item that, so all are still in there, but it could be that if I would have an i6 here, it would have already disappeared here. So these are all the frequency item sets of length one. Now I create candidates, so this set that you see here is created from all combinations of these sets."
2913.58,2943.12,"What I now do is that I see whether I can prune this. I check whether they are actually, so I count how frequent they are. And then I end up this set. And this is the set of items with length 2. And now let's say the key step comes where the question was earlier. As I said, it's a bit hidden here. But this is my L2."
2943.12,2971.18,"And I want to go to L3. So how can I do that? I need to take, let's say, all item sets here that are differing in the last element. So I can take this one with I1 and I3, et cetera, et cetera. So I look at the item sets that I get like this."
2971.31,3000.1,"And now note that based on this, you would expect, for example, a possible combination involving the item sets that we have here is I2, I4, and I5, right? But I2, I4, and I5 cannot be frequent because I4, I5 is missing in here. Do you see that? So although based on this, you would think..."
3000.1,3030.37,"It is the case. But you just create, let's say, all item sets of length 3 based on what you see there. And then you remove one of them, and it should be in this table. And in this table, we do not see all the combinations. So these are then the candidates, and we actually count how frequent they are, and this is what you end up with at the end. The example is not so elegant, but I hope that the principle is clear. That's a priori."
3030.37,3062.91,"And I hope that you understand that this mechanism can also be applied to graphs, pictures, etc. It's the same idea. So there are various optimizations possible if this doesn't perform well. Note that in the end, for every item set, I have to go back to my database. So here we would look at these bigger sets."
3062.91,3090.42,"but I think it's not continued here because then we would immediately see that that is not possible. And so at some point in time, the next set of candidates will be empty. That automatically happens. And so in the worst case, it would end that you have all the items, right? Like in one of my first examples. Yeah?"
3090.42,3119.79,"So that is the general a priori principle. I will come back to this later, also when I talk about sequences and partial hours, etc. But the basic idea is that if I have a pattern that is included in a bigger pattern, then if this one is frequent, then also this one has to be frequent. If this one is infrequent, then also this one has to be infrequent. And I'll just show you a vague picture."
3119.79,3150.85,"but you can think of graph sequences, any mathematical object. If you define this problem, if you ensure that this holds, then you can apply the same principle. Yeah? I'll skip this for the interest of time. All right, as I said, I will come back to this principle later. Then, as I said, the algorithm that I just showed to you was relatively simple, right?"
3150.85,3180.75,"But it has one big drawback, and the drawback is if in the end I find 1,000 patterns, I have went through my entire database 1,000 times. Now I show you an algorithm that allows you to go to your database only twice. You have two passes through your database, and you have all the information. So that's why..."
3180.75,3210.61,"But you build a data structure that is pretty big, right? That's the thing. So if this data structure fits into memory, this is much faster, right? Because you do not have to go back to your data set anymore. And so here you see some graphs from these original papers. And they show you, let's say, that things that are completely infeasible, right, if you're using a priori, become feasible if you're using FPGraph."
3211.25,3240.91,"Yeah. Okay. So how does it work? Again, the setting is exactly the same, right? So we have a database that is a multi-set of sets of items. And we are interested in item sets that meet the threshold. And we are going to solve this by building a so-called prefix tree. It's called an FP tree."
3240.91,3270.0,"And that contains, let's say, all the information that is needed to compute all the frequent item sets. This is a high-level description of the algorithm. So why is this relevant? So you only need two passes through your data. So the first pass is needed to identify for every individual item how frequent it is. Because we need to have a fixed ordering of all the items."
3270.06,3299.57,"what I called before A1 until A100, you should now think of them as being ordered, where A1 is the most frequent one, A2 is the second most frequent one, etc. If they have exactly the same frequency, we just fix some ordering. So we have ordered all the items based on their frequency. If things have the same frequency, we just pick a fixed ordering. That's important."
3300.3,3328.8,"If items are infrequent, so if an individual item is already infrequent, we can just remove it from a database, right? It can never, so if one item by itself is already infrequent, it will never be an element of a bigger pattern that is frequent, right? So we can simply ignore these things. So based on that, so I did the first pass through my data."
3328.8,3357.26,"And in the first pass through my data, I decided on, let's say, an ordering PDH. So these are the ones that are frequent. So I sort all the instances on these three items. So there could have been other items that were infrequent, and I just removed them. But remember, we have sets."
3357.46,3384.42,"The elements in the sets are ordered, so you can think of all of them as sequences. And based on these sequences, you can build a prefix tree. It's easier when I just, let's say, apply it. So we start with an empty tree, and now we have a transaction PD. And I know that P is before D, so I draw it like this. The second element..."
3384.42,3414.77,I look in my database and I look at the second transaction. It's again PD and I update my graph like this. So I'm updating the counters. Now I have an item set that consists of PD and H. And again I put them into a sequence and I extend the graph like this. And now you understand why I'm doing this. So if I'm now adding an item set just consisting of H.
3414.77,3446.58,"I do it like this. So I always take the ordering based on this. And if I just have H, then the tree is splitting like this. This is the basic ID. And I have a larger example to show how this works. So again, if I add this first instance consisting of P and D, we extend the tree like this. We have another PD."
3446.58,3477.9,"then we extend it like this. We now have an instance that consists of P, D, H. We build a tree like this. And we always, if we, let's say, look at such a sequence, we update all the nodes by one. If I now add H, then we have something like this. Note that if I would now add an element just consisting of P, I would change this number into four. If I would have an element,"
3477.94,3508.03,"consisting of P and D, then this would be 4, and this would become 4. If I would have an element D, I would have a separate branch here to create a new node D. So we are looking at the sequences, and that's why it's called the prefix tree. So if we have just P, as I just indicated, so now I skipped a bunch of steps, but suppose that we are in this state, and we add an item just P,"
3508.03,3537.3,"increment this counter. If we now add PD, then we increase this one and we increase this one. That's not the case here, but if we would now have an instance D, there would be a separate branch. Because we always need to look at the prefix. So this is the result that comes out in the end. So hopefully you understand how this is being built."
3547.86,3576.67,"So, yeah, I see your question. So, suppose that we have D. First, we order it based on the sequence. But if we have an item set just consisting of D, we start here, and we look for a branch, D. That branch does not exist, so we would create a new node here called D, and it would have frequency 1. Right? If I would have an item set..."
3576.67,3606.1,"DP, right? I would not take this branch because I need to sort it. I need to first take the P and then the D. So an item set PD or DP would always lead to this being increased and this being increased. If I would have an item set PH, right? I would first go here, P, and then there would be another node here, H. Right? So..."
3606.1,3645.87,"pH would go here. Based on the frequency. Yeah. So you, that's the case indeed. And then we just always start in the root node and we try to extend the tree along that particular order. Right? And that's why here if we already have, I don't know, P and then do H, then we would go like this. Right? Yeah?"
3646.35,3676.27,"So this is the FP tree. And now the interesting thing that I hope that you kind of get, and this is related to the notion of closed item sets that I talked about earlier, this tree can be used to reconstruct our original data set. So based on this tree, I can reconstruct the original database. So it's a compressed form of the data that we looked at earlier."
3676.27,3713.54,"You should just, so if you go here, you go to P and then to D. If I now take the difference of these two numbers, then I know how many are stopping here. Do you see that? So if I start here and I go here and then I do not continue, I know that there are 40 that just have P. Yeah? No. So, for example, if we look at..."
3713.54,3742.64,"And so we start here with 100, right? So here this is easy, right? That shows that there are nine item sets that just consist of H. Then we go this one. Then we take the difference between these two. And we know that there are 40 ones that are stopping here, right? So there's this P40. Now we go to this one, right? So we know that there are 51 going into this direction."
3742.93,3774.35,"and only one is continuing, so we know that there are 50 stopping here that have P and D, that is this one. And we know, for this leaf node, it's again clear what it is. So, for example, if there is a branch here, then we look at the number here, and we sum up these two numbers to know what are the ones that are stopping here. That's the way it is encoded."
3774.35,3804.69,"Given any FP tree, we can reconstruct the original database. That means that this structure contains enough information to compute frequent item sets. Of course, we now need to think of a smart way to do this. This is still the easy part. Now the difficult part comes. It's important that you understand this."
3804.69,3833.55,"Like very naively, you may think, okay, I just cut off all the branches that have a low... So I know that only one is passing here. So you could say, okay, I just cut off H, right, to look for frequent patterns. But you immediately see that it fails. So if your threshold is 10, you know that there are 10 executions of H. So naively cutting the parts of the tree which have a low frequency..."
3833.68,3857.54,"will not work, right? Because something can be frequent because it has different leaves that together are frequent, but not in isolation as you can nicely see here. So this doesn't work, right? So if we want to cut off age, right, we have to realize that there are 10 instances of age."
3857.54,3883.7,"So if you look at this and you look at, let's say, the support of the different frequent item sets above a threshold of 10, we have something like this. So the tree contains the information, but the naive algorithm will not work. So if our threshold is 10, then this is what the algorithm will return."
3883.7,3911.97,"So we want to get to this, and this is a kind of super trivial example, but now think of a huge tree. So how do we decide this? And again, we have this possible exponential blow-up of looking at all the combinations. And it's clear that we cannot look locally. For example, just at this branch or just at this branch. So how to do that? The idea is..."
3911.97,3939.39,"a conditional FP tree. And what does a conditional FP tree mean? That is an FP tree under the condition that certain elements are in it. So you don't put it into the tree, and I will explain you this step by step, but I just want to give a kind of glimpse at how the general approach works. So a conditional FP tree"
3939.39,3968.8,"is the FP tree that you would have gotten if you require that a certain subset of elements is in there. So it's conditional on that certain things are in there. That's what we would like to construct. And we construct this recursively, in such a way that we deal with the problem that we just indicated. So for this, of course, I need to use a more complicated example."
3968.8,3996.37,"So this is now my data set that I use as input. And in the first pass through my data, I look at the frequency of all the different elements. And you should read these strings as sequences of items. I do not write commas and brackets, etc. But you should think of this as, let's say, sets or sequences of items."
3996.37,4026.19,"So if you look, then you will see, for example, that there is an error here. I made the example more complicated. Oh, no. Or is it correct? No, it's correct. It's correct. So F is in almost all rows, but it is not in this row. If we look at C, there must be one row where C does not appear. It's also in this row. In the other rows, there are C."
4026.26,4054.51,"So this is the frequency, and if we, for example, look at, I don't know, n, n is happening in two instances, this one and this one. So we can do this. Now we set the threshold to 3, and if we set the threshold to 3, these have all become irrelevant. Do you see that? Because they are individually already below the threshold, so we can simply ignore them."
4054.51,4084.75,remove them from our so we do not need to talk about them we can also already remove them here and we sort these based on the frequency and note that there can be things that have the same frequency and it's very important now to store in your mind that this order is very important perhaps I should write it down because it's not on all the slides
4086.13,4120.37,"So we have F, C, A, B, M, P. Just for you to keep track of when there are slides where this is not shown, that you remember what the order is of things. So based on this, I can now remove all the things that are indicated here in red. These are the..."
4120.43,4149.18,"The things that are infrequent, we can simply remove them, right? And then we get this more simple data set, right? So now this is... So we had one pass through our data, right? And this is now the input that we are going to use. Yeah? Okay, so let's build the FP tree, right? And now you have a more interesting example for the people that were struggling before."
4149.18,4176.4,"And note that I sorted things already here in the order. So for example, this one here, fc, these are the instances that start here. They do f, they do c, and then they stop here. And there is one element that is stopping here, and that's exactly this one. If I look at fb, that corresponds to this path."
4176.85,4208.29,"If I look at CBP, right, this one is corresponding to this path. If I look at the second one, right, that is FCABM, right? And it always ends with, and I have, if you look at this sequence and this sequence that is happening twice, and that is this path. Yeah, so here you have a larger example if you did not get the details."
4208.29,4235.95,"before. So this is now the only input that I need, right, for building these conditional trees. So the first thing that I do is these are, let's say, sorted, right, and what you hopefully now understand is that you can now think of this tree as a mountain, and you can think of these things, you know that I love mountains, right, that's why I'm"
4235.95,4266.06,"So what you now see is all of these things, you can think of them as altitude lines. If you look at a map of a mountain, then there are different altitude lines. And these are sorted, and we know while building this tree, we were always respecting this order. So it may be that we have skipped layers, but we know that M can never be below P."
4266.06,4302.58,"Above P, there may be no M, but if there is an M and a P, M has to be above P. Do we get this? So this is now a linked data structure, and that allows me to go from this P to the three instances that contain P. That data structure I will maintain. No, no, no. As I said, there may be gaps."
4302.58,4332.35,"So some things already stop at the M level, right? Or here, there's something even stopping at the B level, right? But if they are on the path, they have to be in a fixed order, right? Yeah? So to visualize this, right, that's why I showed you this picture. You can think of these as altitude lines in a way to make this thicker in your mind, right? So this has this nice structure."
4332.35,4360.46,"And we have the ability to keep this table here. This table is the frequency. And by going through this list, we can find all the M's. We can find all the B's and all the A's, etc. So we keep this data structure. And now we go from the end, from the least frequent one to the most frequent one."
4360.53,4391.23,"So now first we are going to consider a postfix P. So we now look at all the instances that are ending with P. And I highlighted here, let's say, these nodes P green. So P is happening three times. And we add, let's say, the item set P with support tree to our data set. So we have already found one frequent item set."
4391.23,4423.23,"Remember the threshold is 3. So we now have this. And now we want to build a tree that is conditional on P. So we only want to consider the instances that have P in it. So please look very carefully. So we have this, this. So we know that these things have become irrelevant. Because these branches that I'm pointing at here, they do not..."
4423.23,4453.09,"include any instances containing a P. So I start pruning the tree. So again, like this green dashed thing is, let's say, our post fix. These gray things are no longer, let's say, like no instances that are including P are related to that part of the tree. So we do this and we actually update, let's say, all of the numbers."
4453.09,4481.7,"If I go back, so we are now only considering the instances that contain P. So I can remove the parts that are in gray here, but I also need to update these numbers. So here we see P going up, and here we see 1 going up. There were other frequencies before, but I have updated this. I've also updated this table. We can see that..."
4481.7,4513.41,"Now, let's say C has a frequency of 3. Of course, if we say P is already there, we also want to remove, so I'm going too fast, right? So we update these numbers. Is that clear? But what we now see is we see this one is happening twice, this one is happening once. So they are below our threshold, right? So conditional on P, if P has to be present,"
4513.41,4542.64,"it cannot be combined with any item set involving B, right? We know that. So I gray out here all of the things which have become irrelevant because they are below the threshold of 3. So if I have the requirement that P should be in the item set, this is basically what I keep, what I'm, let's say, finding at the end."
4542.83,4571.63,"So in other words, and this sentence I will put that very often. So this conditional FP tree for postfix P is the tree, is equivalent to the tree that we would have gotten if we would have filtered our data first and we would just have looked at the rows that would have like element P in it. If we would first have done that."
4571.63,4599.36,"then we would have ended up with this situation. So this is the tree, just considering the instances that contain P. And now you immediately see, okay, there is another frequent item set that is the combination of P and C, right? Because C has a frequency of tree under the condition that P is present."
4599.36,4623.54,"This may be another way of helping you to visualize it, right? So this was our entire data set. And we look at, let's say, all the instances that contain P. I made P green here. So we can remove this row, we can remove this row, we can remove this row. So then we get this, right? I just kept the rows that had the P in it."
4623.76,4649.71,"And now if you look at the remaining rows, some of the elements have become infrequent. And you can see that C is the only one that meets the threshold of tree. So we end up with this, which is exactly this tree. Right? So I didn't do this game because I do not want to go back to my original data. But the result is exactly the same. Yeah?"
4649.9,4677.94,"Okay, so this helps me now to add C, P with support equal to 3. So we have now considered all frequent patterns involving P. I was building this tree under the requirement that things were ending with P. So we can now remove all P's from our data and also from our tree. It is no longer relevant."
4678.22,4710.54,"So this is now the structure that we have. And now we start looking at the second one. We started with this one, now we start looking at what happens if we look at sequences that are ending with M. It should say postfix here, I should have searched for that. So consider postfix M, so an M at the end, and we basically repeat the game."
4710.54,4737.52,"So I now look at, and I can immediately add M with support of tree, right? Because we have three M's. And now I want to build the FP tree conditional on sequences ending with M. Yeah? I considered already all the sequences ending with P. I now look at all the items, which would have all the item sets that would end with M."
4737.84,4767.15,"So again, I repeat the game. So we are only interested in these ones. These parts are not reachable from one of these two. And I hope that you can see that these numbers need to be updated to 3. So I do that. We now only consider the instances going up from these m's. So this is 3. And there is still this b here. But you can see that b has a frequency of 1."
4767.28,4812.58,"So it is below the threshold, so we can also remove that one. And if we remove that one, this is the FP tree for postfix M. So now we are just looking at the item sets that are ending with M, and this is the tree. And I can repeat again. That's what we are doing. But the semantics of an FP tree with a certain postfix,"
4812.58,4842.22,"is what would have happened if you would have filtered your data based on the items that contain this postfix, right? We are not doing that because we would like to avoid that, but I'm showing you in each time that we can just do that on the tree without going actually to the data, right? I'm showing you the data just to show that it leads to the same result, yeah? It's a recursive way. So now we have something like this."
4842.54,4873.02,"So, and again I'm showing you, so we computed this without looking at the data, but again I can show it to you. So we removed P, because P was below M and did not play a role anymore. We look at the rows containing M, right? I removed the rows not containing M, and I removed P, because P is below M. So now we have this structure, and now we look at all of these."
4873.02,4903.02,"which are the ones that are still frequent, meeting the thresholds. So this B has become infrequent. So this is what we would have had. So if we would have applied this to the data, we would have gotten this data set. And if we would build an FB tree for this data set, this is what we would have gotten. And we did not go to the data, but we get something that has exactly the same information."
4903.34,4933.44,"So what is this encoding? This is encoding all the data sets that have F, C, and A three times. And of course, we should add the M to that, right, to know that. So now first I show you, let's say, this recursion, right, and then later I will make a remark to show that you can do this much simpler. So now, again, I look at this tree, and I go again like I've considered P, I've considered M."
4933.44,4960.08,"Here there is no B, so I can ignore that. So now we should look at A. And this will always be the leaf in this tree. So now I look at A. And now I make, right, we are now considering the postfix AM. So we are looking at the item sets that would end with AM. Right? And we know that AM has a support of tree."
4960.08,4986.35,"because M has the support of 3, and we know that also in combination with A, that is 3, and then we have this. So this is the conditional FP tree for post-fix AM, right? Now I can look at this tree, and I can again see how this would be constructed. So if I would have looked at my data, and I would have looked at, let's say,"
4986.35,5012.11,"From the viewpoint of this postfix AM, I look at all the rows that contain AM. And we can basically ignore all of them. But A and M have to be present. So I'm doing that. And I end up with FC. And that is exactly encoded here. I can repeat it. I can look at C."
5012.11,5043.14,"And now we had, let's say before we had AM, now we add CAM. So this also is a part of tree, right? I can go to F. And, right, so sorry, I now look at CAM. I again show you what would have happened to the data, and we would just have find three instances of F. So this is the tree that we would have gotten if we would have filtered for all the instances that contain C, A, and M."
5043.14,5074.21,"and we are basically ignoring everything below C, right? That is no longer relevant. So then we add this with the support of tree. Like, then I can consider a postfix C, M. I can also add this, and I hope that you see that all of these FP trees, that just by updating the tree, I can do this. And again, if we look at,"
5074.21,5104.54,"The conditional FP tree for C, M, that is this one. And this tree encodes the information if we had projected initial input onto the instances containing C and M and removed everything below C. So this whole part. And this is exactly the tree that we would have gotten. So from a semantical point of view, I hope that you get it. And here I'm..."
5104.54,5132.03,"showing you, let's say, if we apply that operation, then we just see three f's, etc. So we are just now, we were still just here. I hope that you understand it. That there are many, like I've skipped a few steps, but you should also have noted that if you have a sequence of where all the numbers are the same, and this often happens in the algorithm,"
5132.03,5160.19,"I do not need to take all of these different steps, right? But because we know that if we have a conditional FP tree where all the numbers are the same and on the line, that any subset of this will have exactly the same one. And there are many of them. There are eight in this example. So this is stating that because these all have the same number. This is based on postfix M."
5160.19,5192.56,"So we know that M plus any subset of these elements will have a frequency of 3. So without going through, let's say, all of the steps that I showed you earlier, where I did not show all of them, you can immediately conclude that you have these item sets that have a support of 3. You should think of this as a shortcut. Clear? So for example, also a CM has a frequency of 3."
5192.66,5219.31,"Or CFA has a frequency of 3. So any subset has always a frequency of 3. Yeah? Okay, so this is what I just said, right? So what we are doing, we looked at subtracts ending with P. We now looked at all subtracts ending with M. And now we look at B. And note."
5219.31,5248.88,"that we are looking at all, let's say, substrings of this string. So there are 63 combinations, right? So we need to do this in a smart way. So now we have considered this one. We have considered this one. Now we go to B. And I hope that you get a bit of a hang of it, because each time I'm doing exactly the same mechanism. So now we look at postfix B."
5249.07,5279.38,"So anything below B using these altitude lines is no longer relevant. We can completely forget about it. We only look at the green nodes and up. Based on this, I highlight this, I update the numbers, right? And if I update the numbers, you can see that everything immediately becomes infrequent. So if we are considering postfix B, we cannot extend it."
5279.38,5312.32,"Because if we try to add something to it, it immediately falls below the threshold equal to 3. So the conditional FP tree for prefix B is basically the empty tree. I cannot combine it with something else. So we have now considered this part. And now we go to A. Here you can see again the reasoning why this would indeed yield B."
5312.32,5341.71,"But now we want to go to A. So we have now considered P, M, B. And now we go to A. It's the same thing. Everything under the altitude line of 3 we cut away. And we update the numbers. So we remove the part in gray. And we update the numbers going from A upwards. And this is what we get. Let me just go back to you so that you can imagine."
5341.71,5367.6,"So we just look at, there's just one A node, so we just look at up, going up. So we can ignore all the rest, and this is what we end up with. And now again you see the same pattern, so there are C and F, they have the same frequency. And now again we can look at A in combination with any subset of F and C. So we know that A has a frequency of 3."
5367.6,5396.61,"A and F has a frequency of 3. A and C has a frequency of 3. And A, C and F has a frequency of 3. That's what we immediately can conclude. Here is the reasoning. If we would have started from our data and we would have said, okay, A is mandatory. I remove all the rows that do not contain A. And I remove all the elements of A and lower."
5396.61,5430.85,"remove all of this, then the only thing that remains are F's and C's in the rows that contain A. And if I do that, I get FC and it's exactly the same result. So this is what I just said. So for postfix A, we are adding, let's say, these three elements next to just A. Yeah? As I said, this is not easy, right? So now we have considered all of them."
5430.85,5460.29,"And now we go to C. And of course, the higher we get into the tree, the smaller the tree gets. So now we cut away everything below this altitude line of C. And we update the frequencies. And we end up with this. And here is what this means. This is now the tree we would have gotten if C is mandatory. And we would have removed the rest."
5460.29,5489.14,"And would have built a tree based on this. Then that is what we would have gotten. And this is what it looks like. And I hope that you get the hang of, like I'm repeatedly doing the same step. The last step is that I'm at the very end with F. And again we get an empty tree because it's the top level element."
5489.14,5517.84,"And this is what we would have gotten if I would have applied the algorithm. And this is the final result that you see by applying all of this algorithm. It took quite a bit of time to walk through it. But I wanted to show you, let's say, how this really works. So given a data set, you should be able to create an FP tree. And given an FP tree, I hope that you see..."
5517.84,5547.74,"we can determine exactly what are all the frequent item sets without going back to the data. Is that clear? Right? So from the input, the first step is to build an FP tree. The second step is to come up with, let's say, these, let's say, frequent item sets that are here. And you can see that it quickly explodes. But still, this is super efficient in comparison with a priori algorithm."
5547.74,5561.33,"because we never have to go back to our database again. Although I showed you many examples to show the semantics that we preserve the same information. Okay, see you on Wednesday."
