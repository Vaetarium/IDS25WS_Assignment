start,end,text
3.02,28.4,"Okay, let's get started. So welcome to lecture number eight. So in the last weeks we have been looking at many supervised methods. We started with very simple things like decision trees and regression and later ended up with, let's say, backpropagation in a neural network. So you've seen quite a few methods today."
28.53,57.81,"before today. So today we are going to look at how to evaluate these things. And the central message, as you will see, is you can only evaluate how good the model is based on data that you did not use for training, which is very obvious. But you will learn, let's say, various metrics of how to quantify, let's say, the quality. After this lecture, starting on Wednesday, we will go into unsupervised methods."
57.81,85.28,"where it's not so clear how to evaluate, because you do not have a specific target. Let's simply dive into it. Before doing that, as you all know, the assignment has been posted last week, and people always warn you, keep the deadline, make sure that you do it in the group, etc. So please note these things, probably the people in the room."
85.28,112.59,"will not have many problems. But the people that may be watching this video later may have problems. So make sure that you start in time, that you are working in a team that is reliable. We cannot help you. I don't know if one week before you say, my team members, they never responded. What should I do? We cannot help you. It is your own responsibility."
112.59,141.42,"As I said, I think for the people that come to the lectures, these are not the problem cases. It's typically the people that are not here that in the end start asking these types of questions. Note that the swine is important. You need to pass it in order to participate in the exam. That's what was also indicated before. So don't play the game of chicken. I like waiting until the last minute to get started."
141.42,171.76,"It's your own responsibility if that gets derailed. So this is the topic of today. So as I said, we have many different topics. And they are all related to how to evaluate, if you are applying a supervised method, how to evaluate the result. And what you will see is that we will look at things like cross-validation."
171.76,196.7,"We will look at the area under the curve. We will also look at the situation where your data is unbalanced. Suppose that I want to detect intrusions in a network, and intrusions only happen one in a thousand times, and if I have a classifier that always says there is no intrusion..."
196.7,226.4,"you will see surprisingly that the quality seems very good, but of course you're not doing anything, right? So we will see how you can deal with the situations where data is unbalanced, and you will see that there is a lot that comes to quality. Towards the end, we have, let's say, more informal topics. That means that even if you have an excellent predictive model, and I will give you an example of that, so I'm perfectly able to"
226.4,254.58,"to predict something, and I try to influence reality, the moment that I start influencing reality, my prediction is no longer valid. So if you do an intervention, then after that you should always again test the results, and that is what is typically called A-B testing. Okay, so let's talk a bit about the need for evaluation, but I think that's very clear."
254.58,285.38,"So we are constantly looking at the situation where you have a bunch of descriptive features and we want to predict a particular target feature. And we have seen a long range of things, ranging from regression, decision tree, support vector machines, neural networks. You've already seen a lot of things. In this part of the lecture, we are just considering tabular data."
285.38,315.38,"So you're interested, for example, in study results and you would like to predict these other things. So it may very well be, for example, in this room, if everybody has a different birth date, then I can build a model that would link your birth date to the result for this course. That would not be very useful. And this is because it's overfitting the data. It does not help."
315.5,344.9,"If I have another measure that I would, I don't know, count at how many lectures you have been present, and I would say, okay, if you have been at half of the lectures, then I predict pass, right? You could say that that is something that is underfitting, because there are lots of, let's say, characteristics that I'm not using. So this is the problem of balancing between underfitting and overfitting. So I think everybody here will see, okay,"
344.9,372.05,"probably lectures present will have, let's say, some effect on the study result. If your model finds, for example, that your first name has a very strong impact on the study result, you could think, okay, does this make sense? Or are we overfitting something, right? Is there a problem with the data? And the bigger problem that you will have if you start doing this,"
372.05,401.33,"is that if you start using many different features, then the combination of features is always unique. Even if I'm not using things like name, but I'm just, I don't know, using hobby, eye color and city, for example, if many of these combinations become unique and I build a predictive model on that, it's likely to be overfitting. So that's why, for example, when we talked about neural networks, we said that the number of..."
401.33,429.42,"instances that you use for training should be at least 10 times the number of parameters that you have. Otherwise it doesn't make any sense to even try to learn a model. So this is, as I said, this balance between overfitting and underfitting. Let's say fine-tuning your model too much on the example data that you have. That's overfitting. Underfitting is to use"
429.42,459.06,"not enough of the information that you have. And you need to balance this. And the only way to test it is whether it performs well on unseen data. So you can have a decision tree that is very deep and very elaborate to the situation where all the leaves are unique. So every leaf just contains one instance. Then it will be a perfect classifier. 100% accuracy, but it doesn't make any sense."
459.18,488.91,"If I just have a decision tree which has just one root node which predicts the majority class, that's also not something that you would like to have. So this is the setting that we often look at today. So we have training data, you build a predictive model, then you have unseen test data and you use this to evaluate. What you will see later is that sometimes this training data is split into two parts."
489.04,516.02,"it is, let's say, split into a true training set and a validation set, but I will come back to that later. So some of the techniques, they themselves deliberately do not use any data to decide when to stop. For example, let's say, overfitting the situation. And when you remember the lecture on decision tree learning,"
516.02,542.72,"You first build a tree that goes all the way to the bottom where you use all the variables, but then you start pruning it using data that you did not use during training. These are examples of where you already said it. But let's just assume that we look at this very basic situation. So we have training data, build the model, and then we evaluate that you're using test data. And that leads..."
542.72,571.92,"If you look at, let's say, categorical target features that leads to a so-called confusion matrix that looks like this. Suppose that we just look at binary classification. So we have positive and negative. That is the simplest situation. Later we will look at the situation where there are more than two, let's say, values for our target feature. So here, this is like an example taken from the book."
571.95,599.89,"Where we want to predict whether, I don't know, a certain message is spam or not. So ham means that it's not spam. So we have the target. So target is the actual value. And prediction is what our model predicts. And if these two columns are the same, then that's good. If they are different, then we are making an error."
599.89,626.24,"Now we come to these words that you will hear a thousand times today. So true positives, true negatives, false positives, and false negatives. And these are these four cells of this matrix. So a true negative means that we do not predict spam. And in reality, it was not a spam message. So both are negative. So that's a true negative."
626.24,655.74,"If we predict spam and it was actually spam, that's a true positive. So these are the good outcomes. The bad outcomes are where these two values are different. So the target is positive and we predict it to be negative, then that's a false negative. We predict it to be negative, but it was positive. And the reverse, a false positive. So we predict it to be positive, but in reality it was negative."
655.74,682.85,"So this leads to this confusion matrix. And the things on the diagonal, Tp and Tn, when they are high, that is good. If Fp and Fn are high, then you have a problem. So that's the way to see it. So if you look at this table with these 20 rows, then what you see is that we are correct in 15 of these rows."
682.85,710.78,"predicted value and the real value are the same, that's 6 plus 9, and we are wrong in five instances. And this is what is called the confusion matrix. So given any of such data set, you should be able to compute, let's say, the confusion matrix. And the confusion matrix will be used for lots of metrics that I will explain later. As I said, there is this, you could split"
710.78,735.82,the training data into true training data that you actually use to build the model and another data set that you use to decide when to stop or to improve the model. This is all in the training data but it can be split into two things. So that would look like this. The thing in black is that we really use to evaluate.
736.05,766.06,"But the training data, what I call training data before, can be split into a true training set and a validation set. And you've seen already, let's say, some examples of this. So just think of, let's say, where you have a technique where over time you get a more and more refined classification based on your training data. Then it may be that this black..."
766.06,795.92,"is the performance on the training set. So you go through many iterations and you fine-tune all the parameters in such a way that you have a perfect performance on your, let's say, set of real training examples. But if you then use a validation set, you will find out that after a while you may get start to overfit. This depends on the method that you're using."
795.92,825.62,"But this shows that here, if you just follow the black line, so you would just consider the true training data, you would just continue and you would just think it gets better and better and better. So the model gets more and more refined, more and more, it gets bigger, or whatever dimension you can think of. Using a validation set, you then know, let's say, what is the best place to stop."
826.26,865.41,"This is another example that you've seen before in the lecture on the decision trees. So remember that the ID tree algorithm is going all the way to the bottom. You could think of it like that. But if you have a training method that has the risk of going into a lot of details, trying to use everything that is in the data that is possible, then you need to have a mechanism to stop."
865.41,895.58,"And this here, I'm not talking about a specific method. I'm just showing you that it can make sense to use a validation set, to deliberately not use part of your data, to decide what is the sweet spot to stop. But again, with this, you can now think of this as one data, and then there could be a validation set of the validation set. That's what you mean. So you could do that. These are just heuristics that are there to decide when to stop."
895.58,925.22,"I'm not talking about a specific usage. I'm just showing you that if you train, sometimes it's good to, let's say, separate, to not use all of the data to decide how good your predictive method is. I think it is more concrete here, because here I'm referring to an algorithm that you've seen before. So in the ID3 algorithm, remember that we continue splitting the data."
925.22,954.27,"as long as we can. So if there are certain variables that we did not use yet, we continue to split our data, let's say, all the way to the end. So unless your data set gets empty or something like that, this tree has a height that is equal to the number of different variables that you have. Because if you go all the way to the end, in the end you have used all of your columns."
954.27,983.66,"Right? Whether it made sense or not. So then you get something that looks like this. And now the numbers that I'm showing you here are based on the validation set. So not on the training set. And they correspond to the misclassifications. And if you now look at this situation, it would be very stupid to stop at this level. Right? Because we would have 10 errors. But if we continue splitting..."
983.66,1012.66,"These instances, so there may be 100 instances here, but we make 10 errors, and if we continue to splitting, it gets better, right? Because we only have two errors. On the data that was not used for training, right? Because if we would consider the data based on training, splitting more can never make the result worse, right? It may not improve, but it cannot get worse. So if we look at the validation set, and we look at this situation,"
1012.66,1040.46,"Then if we stop here, we have 10 misclassifications. But if we continue to split, we have 11. Right? So here it is not wise to continue splitting here. So that's why we extended this ID3 algorithm with, let's say, the ability to prune. So you simply look at, if I go deeper in the tree, does it get better or worse? And if it gets worse, then we stop."
1040.69,1076.06,"So we would not consider this, and this would be the decision tree that we would return. Here I'm just talking about misclassifications. So I'm not talking about false positives and false negatives. Also the numbers that you see here are just the errors. So it can be that our whole data set contains, let's say, 1,000 instances, and of these 1,000 instances, if I just pick the majority,"
1076.06,1118.96,"I'm making 98 errors. Right? So an error can be a false positive or a false negative. Right? Sorry? We first train the decision tree based on the real training data. Right? So we first do that using the ID tree algorithm. But we have reserved part of our data. For example, 1,000 instances. We kept them out of this."
1119.28,1147.82,"And these 1,000 instances, we now apply the tree to that. And then we just simply count at every, let's say, level how many errors we are making. And here, the decision that this algorithm takes, that if you split the data further and that prevents you from making more errors, you should do it. But splitting further can also lead to more errors."
1148.72,1177.97,"So just think of, I don't know, that I would split it based on all the first names or something like that. On my real training data, it would perform very well. If there are a few people with the same name, on a real training data, that would fit very well. But if I then reserved certain data, the results would be completely random. So it would probably be better to not use that attribute."
1178.45,1205.74,"This is to talk about this topic of validation set, that sometimes this training set is split into two things. Just that you know that if you read a paper or something like that, that this is often mentioned. We now just focus on this part. We just focus on the real evaluation. So if we consider a model, we are considering this model, and not the model before pruning."
1206.19,1235.62,"So then we have cross-validation. So now when I talk about test and training, like training may include a validation set, but I don't care. This was simply all the data that led to the final model. And the black part is, let's say, what is our test data. And k-fold cross-validation simply does the following. You leave out part of the data, the part in black. You train it on the part that is in white."
1235.62,1265.87,"and then you evaluate this based on the remaining set. So if k is equal to 10, which is a typical value, I basically split my data in 10 different parts, and I'm always using 9 parts, and I'm leaving out one of the 10 parts. And then I evaluate based on this. So I get 10 measures of quality. So this is an example."
1265.87,1295.02,"So this is using five-fold. So I split my data into, let's say, buckets of 20% of the instances. And I always use 80% and use the remaining 20% to validate. And I can do that experiment five times. And that may lead... So here, the numbers on the diagonal correspond to the accuracy that I have. So where I'm making the correct prediction."
1295.02,1324.4,"And then you can see that we get these numbers. And now we have, let's say, more information than if we would just return a number, I don't know, 83. We have no intuition what the quality is, because it may sometimes be 83, it may sometimes be 60, it may sometimes be 90. But now we are doing five experiments, and we get an idea of this distribution."
1324.4,1354.83,"And based on this, you can also do statistical tests, et cetera, et cetera. But you get an idea of, let's say, what the quality is. Much better than, I don't know, just do fold one and then stop. Because you could do this k times. This is the same idea. It's called jackknifing or leave one out cross-validation, where you do this to the extreme. So k is equal to your number of instances."
1354.83,1385.68,"So you just leave out one instance, you learn the model based on everything else, and then you evaluate it on the one instance that you used before. It is the same idea, but taken to the extreme, to have as many tests as possible. You can also, let's say, think of something where you add more random behavior. So here we repeatedly k times,"
1385.68,1412.02,"So we do k experiments where m random instances are selected as a test set. So we randomly pick, let's say, elements here that we use for testing, and then we do this k experiment. Because we have k computations, if they all point into the same direction, we can be confident that this is the number."
1412.02,1447.3,"If we do K-experiments and they are all over the place, so sometimes we say accuracy 60%, sometimes 90%, then we know that we cannot be very sure about the quality. Yeah? So, yeah? Yeah, yeah, yeah. By chance you could select the same instances, right? That's random. One of the reasons why sometimes this is attractive to think about this is the following."
1447.3,1474.05,"that this is very dangerous. So if time is evolving, and you would do this experiment, you would have a training set, and you would have a test set, like the distribution here can be different than here. And you can consider this to be an advantage or a disadvantage, depending on what you would like to answer."
1474.05,1503.44,"Random sampling works the best if you think that these instances are independent of each other, that there is no drift over time. But if you assume that there is drift over time, then random sampling will give you results that are probably way too optimistic. Because there are, for example, seasonal patterns, and if you completely randomize, then you're not taking into account this phenomenon."
1503.44,1529.98,"And that can lead to, let's say, optimistic, that's what it says here, misleadingly good results. So you need to think about what you are trying to achieve. If you believe that there is drift, this is not so simple as it seems. Because random sampling would give you an optimistic result."
1529.98,1558.64,"That's the basic idea. And I have some pictures to let you think about this setup. Here I am again looking at training split into a validation set and a training set. And this is the real test. Just as the way that it was before. So if you would do something like this, I do the training, I do the validation, I do the test. Then this test may be, let's say, very..."
1558.74,1587.54,provide you with very good results but if the process is drifting then this classification will probably be worse than this test which was based on let's say a random sampling of the whole period because it can be that there is a seasonal pattern that now something is changing so this is dangerous what I'm showing you here
1587.54,1618.16,"So this is what I just showed. You can also look at it in this way. So you create a model based on 2015, 2016, 2017. You evaluate the model based on unseen data of 2018. And then you make predictions for 2019, hoping that this test was representative for this."
1618.38,1647.09,"You could also use this scheme, where I have training data and validation data, and by doing this, I get an idea if I'm, let's say, training on one period and validating it over another period, how reliable this is. In such a way, so here, my hope is that the performance that I see on the blue part is comparable to the performance on the green part."
1647.09,1677.01,"These are things that you need to take into account. Later I will talk about concept drift. And you can imagine that, first of all, instances are not independent of each other. But also the process that is there at the beginning of your data set may be a completely different process that is there at the end of your data set. And it's not very representative. So here you see a bunch of plots."
1677.01,1704.21,"Like at the bottom, you can see, let's say, where these time series come from. And time series are very, let's say, nice to illustrate this notion of a concept drift. If you look at tabular data, you do not immediately see that. But if you plot it as time series, you can typically see that. So what you see is that, for example, if I look at this one, it has an..."
1704.21,1733.55,"it's clear that the average is going up, and that there are seasonal effects. So if I want to build a predictive model, I should somehow take that into account. If I would simply say, okay, I now learn over this entire data set, I probably predict something around the middle, which would be a very bad prediction. Because there is this up-going trend, and there is this seasonal trend."
1733.68,1763.44,"So of these, only B and G, so this one and this one, they seem to be stationary in such a way that there is no seasonal effect and there is no ongoing trend. In which way you could use data, let's say, randomly. So these are some phenomena. So we have seen cross-validation. We have seen things like concept drift that you should take into account."
1763.44,1792.99,"Let's now simply look at a bunch of formulas that you should also be able to reproduce. The confusion matrix, if we do binary classification, we have, let's say, these green values and these red values. And I can visualize it in this way. So you need to get a bit used to this. But this helps you, in the end, to understand what I mean by this. So this bigger square..."
1792.99,1822.21,"is let's say the set of all possible instances. And the red part is the part where the model says this is positive. So what is here in red is what the model says is positive. This is what is in the real data positive, this square. And this square is what is in the real data negative. So ideally..."
1822.21,1851.95,"In a perfect situation, this red circle would go around this square. So what are the errors that we can make? We have false negatives. So these are things where in reality it's positive, but it is outside of the red circle. That means that we predict it to be negative. There's this other problem that we may encounter. So we predict things to be positive, but in reality it's negative."
1852.62,1882.51,"So again, what is here in green is good, what is in red is bad. And if you look at the confusion matrix, you're basically counting the dots in all of these four areas. That's the idea. So sometimes, but we will not really use this terminology, let's say very extensively, sometimes people talk about type 1 errors and type 2 errors. Type 1 errors are false alarms."
1882.51,1911.66,"So these are false positives. Type 2 errors are missed alarms, where the real instance is positive, but we predict it to be negative. So these are the two errors. Sometimes you will see these terms, we will not use that in the remainder. We stick to true positive, true negative, false positive, false negative. Based on this, these are the formulas that you need to know."
1911.66,1943.15,"There are different ways of quantifying the problems that are there. This one is computing the absolute number of instances. This is counting the fraction of green dots that you have. This is counting the fraction of red dots that you have. The other ones are making it somehow relative. I will walk you through all of them. The ones that I now highlight in green,"
1943.15,1973.22,They should be high. The ones that I highlight now in red should be low. Accuracy is something positive. Misclassification is something negative. So we want that to be low. I think it's very easy. I'm showing you this because at the end of the lecture you will be completely confused and you will not be able to distinguish all of these different terms. That's why I'm trying to make it stick in your mind.
1973.22,2003.31,"True positive rate, also called recall or sensitivity. This is the number of true positives divided by the number of real positives. So Tp plus Fn is what we divide by. So we are looking here at the fraction. We just look at the positive samples. And we look at the fraction of positive samples that we really classify as positive."
2003.73,2036.69,"This is this formula. True negative rate. Now we look on the other side. And we are counting, let's say, the things that are negative, that we really classify as negative. So that is the number of true negatives divided by the number of true negatives plus the number of false positives. Then we have precision."
2036.69,2068.38,"the instances for which we predict it to be positive, and we look at the fraction of the instances that we predict to be positive that are really positive. And then we get this formula. This is called precision. Then there is accuracy. This is basically, as I said before, the fraction of green dots where we are predicting the correct value divided by the total number of instances. Now we look at the negative."
2068.38,2100.5,"So the false negative rate is the number of false negatives divided by the number of false negatives plus the number of true positives. And this is 1 minus the number of true positive rate. False positive rate, this is Fp divided by everything that is classified as negative."
2100.5,2130.96,"We have this relationship, of course. We're basically just swapping the row. So we have all of these, let's say, measures. So this one is easy. It's the reverse of accuracy. So 1 minus accuracy. And that is the fraction of red dots compared to the whole number of dots. Are you still with me? So hopefully some of these things are sticking to your mind."
2131.06,2160.4,"It is very easy to have a model that provides perfect recall. I just classify everything as positive. If I classify everything as positive, I'm never missing a positive instance. So if I say everything is positive, my recall will be equal to 1, but my precision is probably very low. On the other hand, if I predict everything to be negative,"
2160.4,2190.32,"I'm never making a mistake with respect to classifying something as positive while it is not positive. So my precision will be equal to 1. So it's very easy to get into a model where either recall is 1, you just label everything as positive, and it's very easy to also get precision equal to 1, I just label everything as negative. So I have these two extremes. That gives, although these measures..."
2190.32,2219.15,"are ranging between 0 and 1, this creates a bit of a problem. Because if I would look at the average or something like that, you have to really do your best to go below half. That would not make a lot of sense. So that's why typically people do not look at these two measures in isolation, but they combine them in something called the F1 measure. And the F1 measure is the harmonic mean."
2219.54,2248.53,"And what does that, like the harmonic mean is a standard mathematical expression that you can see here. And what is the basic idea? Because it's very easy to get one of them close to 1, right? You want to have a penalty. If one of them goes very close to 0, you want the whole to go close to 0. If I would take simply the average, right, as I said, I would be above 1 half."
2248.53,2279.87,"But that is not the case here. So this is D divided like this. And the intuition that you should have is that the moment that precision or recall approaches zero, the whole will go to zero. And that is much better than taking the average. So here I have a table with, let's say, example values. So if one of them is zero, the F1 measure would be zero."
2279.87,2310.22,"If I would just simply take the average of these two values, it would always be half in all of these examples. If it is balanced, then the F1 measure is exactly the same as these two values. So there is a high penalty if 1 is low. That's the idea. So if you think about this in a bit more detail, but it's just to create some intuition for you what this means."
2310.22,2336.7,"So if precision is equal to recall, I can simply rewrite this expression. And if precision is equal to recall, then F1 is equal to recall is equal to precision. All of these three values would be the same. What we see is that the F1 cannot be higher than the average value of precision and recall. And what you see..."
2336.7,2363.97,"You can hopefully see that this is a desirable property, that F1 is in between the minimum of these two values and the maximum of these two values. So this shows you, let's say, what the intent is of this measure. So if we apply that to our example of spam detection, we can see that precision is equal to 75%. That means..."
2363.97,2393.09,"That 75% of the instances where we predict spam, it is actually spam. That's precision. Recall is 67%. And that means that of all the spam messages, we are able to capture 67%. And these hopefully help you to think about these things. And if you have these two values and you compute the F1 measure,"
2393.09,2423.49,"then it's, let's say, somewhere in between these two values. But the F1 measure is already better than taking the average of precision and recall, as I just showed to you. But we still have this problem. So suppose that 99% is positive in a data set with 100 instances. So we have like 99 positive and one negative. Think of this as..."
2423.49,2452.58,"I don't know, spam detection or something like that, where positive is we do not detect, we do not have spam. If we have this situation and we now compute precision and we always predict positive, then in 99, we always say it's positive. That's 100. And in 99, out of 100 instances, we are right. So we have a precision of 0.99."
2452.58,2484.22,"Very good. If I now look at recall, so these are all the instances that are positive. Are we able to capture them? We capture all of them because we always say it's positive. So this is an example because the data is very unbalanced. It's again very easy to get, let's say, very good numbers by just predicting the majority class. I hope that you can see that there are applications where this would be okay, but if your goal is to"
2484.22,2513.63,"to detect this 1% intrusions, you have a problem. So that's not what you want. So one of the ways to approach this is that you consider all classes to be equal in importance, independent of how many instances there are. So that means that I compute a recall for all the different classes. So how many of the messages"
2513.63,2543.06,"or the instances with having label L, are we able to detect? And I average this over all the classes. It's easier to show again an example. So if we have this situation that I sketched before, so 99% is positive. We have just one instance that is negative. And we would always predict positive. Then we have a recall of one for the positive instances."
2543.06,2573.62,"but we have a recall of 0 for the negative instances. Because of the negative instances, we are not able to catch any of them. So that is 0 divided by 1. And then we average these two, and then you get 0.5. And now it is basically showing you that we are scoring very well on the positive instances, we are scoring very bad on the negative instances, and that's why we get 0.5. Although there are fewer negative instances."
2573.62,2626.64,"Yep. Zero divided by... That would be a situation that would not happen because then you basically would say there are never any in the other class. Right? You could try to come up with a definition, but it's like a non-existent problem. Sorry? These are just different measures of looking at a problem."
2626.93,2652.59,"So I think what you're referring to is that you could also have counted accuracy, right? And accuracy would have been 99%, right? So I could also have done that. But these are measures, and in a minute you will see that a bit, is that you can basically many... So think of a neural network. In the end, a number comes out of this between 0 and 1, for example."
2652.88,2683.49,"And then it's 0.4, and we can decide 0.4 is still positive or not. And that's why I'm introducing this. But precision and recall, if you start thinking about it, it's a very natural mechanism. So think, for example, of you type something in Google, a search query. So I'm looking for a certain thing. Then precision would be,"
2683.49,2712.62,"Of the search results that Google is returning, how many are actually related to the question that you asked? That would be precision. A recall would be, of all the web pages that are talking about the topic that you're interested in, what is the percentage that you actually find? So these notions are very natural if you think about this. So this is, let's say, one way to deal with this."
2712.62,2741.78,"You can also combine these two IDs that I indicated before, and now you can compute the harmonic mean of these different classes. So in this formula, I'm combining two IDs. Every class is important, even if in a class there are just a few instances. That's one ID. The other ID is if one of them is close to zero, the whole should be close to zero."
2741.81,2766.19,"and you can already predict what is going to happen in our example. So we started here, where let's say precision is 0.99 and recall is 1, which seemed very positive. Then we went here, we brought it down to half by averaging over the two classes."
2766.19,2796.06,"But because we are never able to find, let's say, instances in this class, you could also argue that it should be equal to zero. And that's when you use this formula. So here I'm combining harmonic mean with every class is important, even if it just has a few instances. And then it did this. So if you would never predict this one class, then your accuracy would be equal to zero using this formula."
2796.06,2823.31,"Do we understand this? Here, like I also added this, let's say, for the question that was asked before, right? You can think of it like this. This is visualizing this, so this would be the normal mean of values. This is the harmonic mean, and you can see if one of them is going to zero, you're not going to half, as you do here, but you're really going to zero."
2823.54,2852.03,"It is basically visualizing the formula that we had before. At the same time, like if I look at the confusion matrix like this, you could also think not all of these cells have the same meaning. You could also associate costs or profits to it. And the easiest example to think about this is that think about self-driving cars."
2852.03,2880.3,"So a self-driving car has to make a decision to do an emergency stop or not. So you could say that, I don't know, a false positive is that a car makes an emergency stop although there was nobody on the road. That would be a false positive. What is the cost of a false positive? It is annoying, but the costs are limited. What is the cost of a false negative?"
2880.3,2906.16,"is that there is somebody actually crossing the road, but the car doesn't see it. It doesn't detect it. So that's a false negative. It should have been positive. It should have been braking, but it did not. And then somebody dies, right? And I think we all agree that if somebody dies, that the cost of that is higher compared to that the car stops, although there is no real reason for it."
2906.64,2936.16,"So if you take into account this idea, then you can take, so here I'm showing you two confusion matrices, but you can attribute, let's say, values to the different cells. And here, typically values on the diagonal should have a positive value. Values that are not on the diagonal should have a negative value, because they are errors."
2936.16,2963.41,"And now you can kind of attribute values to this. Don't mind about this particular example, but I just want to show you that you have these two classifications. And using costs associated to instances, I can, let's say, compute, let's say, what the overall profit is as I'm adding up all these different numbers that you basically get by..."
2963.41,2991.54,"just multiplying the numbers of these different matrices, and then you would see that the decision tree model would have a better performance. So you're just good predictions yield a positive value, bad predictions yield a negative or zero prediction, and then you basically multiply all of these numbers, and then you can compare, let's say, two instances. In the previous example where we had..."
2991.54,3022.51,"100 instances and one was then, let's say, for example, an intrusion, then we have to decide what is the cost of not finding that intrusion. And based on that, you would decide what is the best way to go. Yeah? Clear? Very briefly, so we are continuously, let's say, looking at this situation where we build a predictive model and then evaluating."
3022.51,3044.24,"Note that if we are doing k-fold checking, we are actually applying the same algorithm k times. So if you do cross-validation, you're not talking about the quality of one model, you're talking about the quality of an algorithm in a particular situation."
3045.2,3074.56,"If you do not have that and somebody just gives you a neural network and asks you what is the quality of this neural network without talking about how it was trained, then you are in the second situation. So if somebody gives you a neural network, you cannot do k-fold cross-validation, right? Because you do not have k models, right? It's something that's very subtle, but it's important that you see that."
3074.56,3104.69,"So, this is, are we now evaluating an algorithm, or are we evaluating a particular model? A complication that we will see later, for example, when we talk about, let's say, process discovery, is that in many situations, you only have positive examples. So, you know that there is a bigger set, but you only see positive examples."
3104.69,3132.32,"If you do, for example, process discovery, you're trying to find a model that is describing, for example, study behavior, but your input are just positive traces, behavior that you have observed. You will never get, I don't know, the traces that could not happen. You only see positive examples. So this is just preparing you a bit for later, that it is not always as clear-cut as it is in this lecture."
3132.32,3165.71,"where we can immediately go for a confusion matrix. Any questions so far? So all the formulas that I just indicated to you, given a confusion matrix, you should be able to compute them. And you should be able to reason why we have these different ways of quantifying this. Another, let's say, something that you will definitely see if you do anything related to AI or machine learning or the data science,"
3165.71,3194.93,"is the receiver operating characteristic curve, typically known as the area under the curve. And I'm going to explain you, let's say, how this works. So again, think of a spam detection mechanism. So a spam detection mechanism, I have a neural network, and that gives me a value between 0 and 1. And I need to make a decision, is it spam or not?"
3194.93,3225.74,"So I may say, okay, if the neural network says 0.9, I predict that it is spam. If it gives me a number 0.1, I predict it's not spam. And then you could put the threshold at half, and you say all the values above half are spam, all the values below half are non-spam. But this 0.5 is arbitrary. I could also have made it 0.6, 0.7."
3225.74,3254.5,"And when I'm increasing the threshold, I'm basically making a trade-off between precision and recall. And the area under the curve is like an approach where we do not come up with a single value. This is the quality. But in situations where we have a threshold, we are basically seeing how good our algorithm is. Not for one setting, but for all possible settings."
3254.5,3282.93,"So this is what I just said. So zero is that you're pretty sure that the class is negative. One could be pretty sure that the predictive class is positive. With 0.5, you do not know. Also think of logistic regression, where we are using the logistic function, where 0.5 would be like the decision boundary. We do not know. 0.6 would be above the decision boundary. 0.4 would be below."
3282.96,3311.7,"But the 0.5 is arbitrary. I could also have said 0.6. And I could apply everything that you've seen before. So this shows you, let's say, the example that I said earlier. And we now have a score. So let's assume that our neural network gives a score between 0 and 1. And if it is 1, we are pretty sure it is spam. For example, here we are pretty sure it is spam."
3311.7,3340.62,"If it's very low, we are pretty sure it's not spam. And now here, I've sorted this on the outcome that the neural network presents, and I put the threshold at 0.5. So anything that has a value lower than 0.5, we predict not spam. And anything with a value higher than 0.5, we predict it to be spam."
3340.62,3371.22,"And we are making errors. For example, here, there are two spam messages that we are not detecting. Clear? Right? But this 0.5 boundary is kind of arbitrary. With exactly the same neural network, or regression, or whatever technique I was using, with exactly the same numbers, I could also say, okay, let's put the threshold, let's say, a bit higher. So if we are moving this red dotted line in this direction,"
3371.63,3401.9,"That means that we get more true negatives and possibly also more... So we classify more things as negative, so this can be good. More true negative can also be bad, that we have more false negatives. We get less false positives and possibly also less true positives. So just think of this example. So if I go back to you,"
3401.9,3431.78,"like 17, because the threshold is below, is above 0.5, we predict it to be spam. We are making an error. But if we put the threshold somewhere here, then this spam becomes ham, and we are correct. So if we change this one line lower, this threshold, we are getting a better prediction. But it could also have become worse."
3431.78,3462.83,"So, for example, if I would have moved this line up, I would start making errors that I did not make before. So, this is illustrating. I moved this line 1 up. I was predicting, for instance, 14 things correctly, but I'm no longer predicting it correctly. I get a false positive. So, this is showing you... So, here I have these scores."
3462.83,3493.12,"That, for example, a neural network generated. Here I have different thresholds. And depending on where I put the threshold, I predict more to be SPAM or less to be SPAM. Right? And for every setting that I choose here, for example, here I have as threshold 0.25. Right? I am getting a confusion matrix."
3493.12,3522.19,"And because I have a confusion matrix, I can compute all of the different things. So again, if you look at this red line, if we pick this red line, then we get this confusion matrix. And if we get this confusion matrix, we can, let's say, compute these values. And I can do that for every threshold that I set. Clear?"
3523.95,3553.73,"So that means that we can play with this threshold value and do not get, let's say, one set of numbers, but multiple sets of numbers, depending on where we set it. So if we are interested in accuracy, then this threshold of 0.5 was the best one, because in 75% of the cases we were correct."
3553.73,3582.58,"will increase the number of misclassifications. But precision and recall may go up and down depending on which way you go. So you can plot this. So this is my threshold value, right? So we had the best accuracy here for half. And I can plot the true positive rate and the true negative rate for the different threshold values."
3583.12,3611.57,"And I hope that you can see that here we were computing all of these numbers. Like every line corresponds to a confusion matrix, we can compute this. And now I'm simply plotting it for the different values that you set here. And what you see is that if you classify more as positive, you go to this end, then you can see that the true negative rate is low."
3611.66,3640.4,"and the true positive rate is high. If you set it to zero, you can interpret that we classify everything as positive, so we do not have any false negatives anymore. I could also go to the other extreme, where I classify everything as negative, and then we know that our true negative rate is high, and we are catching all the negatives."
3641.68,3671.07,"Do the terms now start ticking to make a mental connection in your brain, right, that you understand why these things are there? So this clearly shows that there is a trade-off between these two things, right? And that means that for one algorithm we do not get one value, but we can have like an arbitrary number of values, right? And now I'm going to convert this, like I think I can be brief on this."
3671.07,3702.91,"The threshold, you can also think of this, we make the red circle bigger or smaller. And that's the same as moving the threshold. And then you have an intuitive connection. What happens if I make the circle bigger or I make the circle smaller? What kind of effect does it have? And that's visualized here. So this is the graph that I just explained, which shows you that there is a trade-off. And now I would like to get to the area under the curve."
3702.91,3735.14,"For that, I'm plotting TPR, so the true positive rate. That's what we have here. And on this axis, I'm plotting, let's say, the FPR, which is 1 minus the true negative rate. And now for the different points, I'm showing you the things here. So for different thresholds, we get these points. Let me highlight."
3735.14,3765.63,"Because I see that for some of you this is difficult to see. For example, if we set this, we choose this threshold value. Then our false positive rate is here. And our true positive rate is here. So that leads to this dot. So any line that I pick here corresponds to a dot here. And the reason is this threshold value."
3765.63,3794.1,"is something that is internal to the algorithm, right? We do not care about this. We only care about the performance. And that's why we translate this diagram into this diagram, where any line that I pack here, I have a true positive rate and a true negative rate, and I can plot that onto a point here, where it's a bit confusing because this is now reversed, right? So here we have true negative rates."
3794.1,3825.66,"you should look at the inverse of this. And then we are plotting this. Yeah? Do we see that? Every red line generates a confusion matrix, so I can compute the TPR and the FPR. So this is another line. So if I would cut it here, then this value here is 1 minus this value. So we get 0.1."
3825.66,3865.2,"And this value is equal to that value. So what this shows you for different parameter settings, we get a curve that looks like this. No idea where that comes from. So we get a curve of this. The reason is that now you do not see the parameter. And that's the goal. We only want to talk about the quality because the threshold that I said was something internal to the method."
3866.67,3907.47,"You also have to realize that here I assume that there is one threshold, but you can also imagine an algorithm where there would have been 10 thresholds, for example. It could be much more complicated. So we want to get rid of this dimension, but we are interested in this trade-off. So we basically remove this dimension, and this is showing the trade-off between the two. So here..."
3907.47,3936.37,I'm just looking at slices here that I get under different parameter settings. Every slice here corresponds to one point here. One possible parameter setting. And the reason why we do that is now we get a diagram that doesn't talk about anything that is internal to the algorithm. I do not care what the neural network chose as its threshold value.
3936.37,3969.14,"And that's deliberate, right? Because now I can compare, by doing this, I can compare, I don't know, an SVM approach with a neural network approach with a decision tree approach, right? They all would lead to different dots and would all lead to a different curve, yeah? So for any, let's say, setting of a particular algorithm, I get one of these red dots."
3969.49,4000.26,"So TPR is something that we interpret as positive. So the true positive rate, this formula, just as a reminder. Like we also have, let's say, the false positive rate. This is considered to be something negative. So we want to be up and we want to be in this direction. So this is better and this is worse. So being here is very bad because your score..."
4000.26,4029.74,"You score bad on both dimensions. Being here is good. Do you see that? And for every parameter setting, we get a red dot. And we can do that for any algorithm. So we can have red dots for a neural network. We can have blue dots for an SVM. And we could compare what they look like. That's the idea. So there are these extreme points. So what does it mean that you are here?"
4029.74,4058.98,"that basically corresponds to predicting all to be true. Let's just, let's say, play a bit what this means. So if we predict all to be true, we have no true negatives, right? Because we predict everything to be positive. We have no true negatives. We also have no false negatives. If I fill out Tn is 0 and Fn is 0,"
4058.98,4088.43,"then we see that both of these numbers go to 1. So if I predict everything to be positive, this is the point where we are. If we predict everything to be negative, then again, let's try to think what that means. If we predict everything to be negative, then we do not have any positives, so we do not have any false positives, and we do not have any true positives. Now I simply fill out the formulas again, so if tp is equal to 0,"
4088.5,4121.55,"then this expression is equal to 0. If fp is equal to 0, then this is also 0. So this is everything is positive, this is everything negative. So these are the extremes. And we are typically aiming at the trade-off between these two things. So here, this is the area where you would like to be. Do you understand that? So now this is a very important slide for your understanding. If you are here, right?"
4121.55,4150.19,"then that is extremely bad. Why is this extremely bad? If you're here and I simply swap, I don't know, everything that I predict to be positive, I make that negative and vice versa. If I'm here, I can swap it to go to there. Right? Do you see that? And now this is a very interesting reasoning and that hopefully shows to you why it would be very bad to be under this line."
4150.35,4180.45,"Assume that I have an algorithm that is simply guessing. Assume that I have a data set where a fraction Q is actually positive and the fraction 1 minus Q is negative. It's completely random. Q percent is positive. I do not know anything. When I get an instance, I just throw a dice."
4180.45,4211.06,"And with the probability P, I say it's positive. And with the probability 1 minus P, I predict it's negative. And assume that this is completely random. So I know nothing about the actual label. And I just randomly make a choice. Now we can simply compute, based on the fact that these two things are independent, what is the probability of having a false positive, a true positive, etc. Right? I just multiply the corresponding number."
4211.06,4235.66,"So what is the chance or the fraction of instances being true positive? That means it is really positive and we predict it to be positive. So I just multiply these two probabilities. True negative, right? It is really negative and we predict it to be negative corresponds to this expression. Do we see that?"
4236.69,4265.68,"So I can compute, so these are fractions. If you have n instances, you could multiply it with n to get numbers. But just think of these as fractions. And now I can simply fill out these numbers in the formulas that we have here. So given this, I can now compute the FPR and the TPR. So the TPR is this formula."
4265.68,4293.6,"And so TP is now equal to P times Q, et cetera, et cetera. As I said, I could multiply the top and the bottom with N to get numbers, but I'm just computing the fractions. So I can look at this expression and that yields P. I can also, let's say, do this computation that leads to these expressions. I'm just filling out these values that equals to P. So now we have something very weird."
4293.6,4323.42,"that by just randomly guessing, we get any point on this curve where it is located exactly is based just on the probability that we pick something to be positive. And that matches the intuition. So if we predict everything to be positive, so a likelihood of 1, then we are here. If we say the probability of picking something is positive is equal to 0, then we are here."
4323.42,4351.65,"but we can reach any point in between. So just randomly picking something gives us already an algorithm to get any point on this curve. Do you see that? It's very important that you understand it. And surprisingly, this is independent of Q. So how can we now determine the quality of an algorithm?"
4351.65,4380.91,"And the quality of an algorithm under different parameter settings is how far we are, let's say, above this line. If we are below this line, we can simply swap positive and negative and we get a better prediction. So this is now showing this. So you can think of this as different algorithms being used. I don't know, a neural network, an SVM, a decision tree, whatever."
4380.91,4411.98,"I look at these different curves and then you can see that this black line would be the algorithm that is performing better under different parameter settings. And you would probably pick like a parameter setting that is somewhere here. So now we get to this formula area under the curve, which is basically, note that the dimensions here are 0 to 1. The dimensions here are 0 to 1."
4412.05,4442.0,"So the maximal, let's say, area that we have is equal to 1, right? If it is less than half, we are doing very bad, right? So this formula that you see here is computing, let's say, the yellow surface as a fraction of the whole surface, which is equal to 1. So how to compute this?"
4442.0,4468.46,"Let me walk it step by step. So this area under the curve, I can basically, so these lines are going up. So I take the point where it starts and where it ends, right? And I take half of it, right? So this triangle here is equal to this yellow triangle, right? And we can continue doing this."
4468.46,4493.07,"I compute this surface, I compute this surface, I compute this surface, I compute this surface, I compute this surface. And how do I do that? I take the average of the level where we start and the level where we end and we multiply that how far we move on the x-axis. So this is where we move on the x-axis."
4494.0,4525.74,"and this is taking the average of the height where we start and the height where we end. Do we understand this formula? It's basically computing this. So something that you should be able to do if we give you an algorithm and say, okay, these are the confusion matrices, then you should be able to compute the area under the curve. I hope that you understand that this is meaningful. Clear? So far,"
4525.87,4555.02,"we only consider binary classification. So we talked about true positives and true negatives, false positives and false negatives. But of course, our classifier can also be multiple values. So here you see a confusion matrix, and forget what these things mean, it's taken from the book. So we have four classes, and if we have four classes, our confusion matrix looks like this."
4555.38,4586.88,"And again, all the values on the diagonal you should interpret as good. All the values that are not on the diagonal are errors. So what is the accuracy? The accuracy is, let's say, adding up all these numbers on the diagonal divided by the total. What is the misclassification error? These are all the values that are not on the diagonal. So this one, two, and this one, two."
4586.88,4617.28,"divided by the whole. And again, we can look at this from the viewpoint. So accuracy gives one value, but you can view it from the viewpoint of precision and recall, just as you did before, using these formulas. So precision L means that the number of instances where we correctly predict that it is L, divided by net number,"
4617.28,4647.7,"plus the false positives, so where we're predicting i, but it is not i. Do you see that we can just generalize these two classes? So this is the fraction of instances where we predict the class to be L, and it is actually L. Recall is the fraction of instances where the class is L, and we are actually able to predict that it is L."
4648.27,4678.45,"In a way, it's exactly the same as before, but now we go to any number of classes. Think of L as what we called positive before. But now there can be multiple types of positive. If you look at precision, we look at the instances where we predict a certain class. I don't know how to pronounce this, but we predict this class."
4678.45,4708.85,"We have 15 instances. And of these 15 instances, in 10 times we are right. So we get 10 divided by 15 is this value. So we predict the class. And what is the percentage where we are right that yields this number? That is precision, recall. In reality, it was this class. So in reality, it was this class."
4708.94,4739.49,"And in how many cases where in reality it was this class, we also actually captured it, is 10 divided by 11. So in other words, precision, we look at the columns. If we look at the recall, we are looking at rows and computing the fraction that is correct. Clear? So this is a way to generalize this. So far, we just looked at, let's say, categorical features."
4739.49,4768.35,"You can also look at continuous features, and there I'm showing you things that you've seen before. So this is the sum of squared errors. Remember when we were talking about neural networks, this is what we were using all the time. So this is the formula that I hopefully do not have to explain. So this is the actual value. This is the predicted value. You take the difference and square it, therefore it's always positive. You sum it up and you divide it by 2."
4768.35,4799.5,"And this looks very unintuitive, but we now know why this is the case. Because the derivative of this is very elegant. Because then this two disappears and this half becomes one. That's why we have been constantly using this formula. So again, actual target value, predicted value by the model. And these are the descriptive features that go in. Mean squared error."
4799.5,4828.83,"That is a bit easier to interpret. That is basically you sum up, let's say, all these squared differences and divide it by n. Note that in this one we are looking at the sum. So if I double the number of instances, the error will also be doubled. So it's not a relative notion. This is an attempt to make it more relative. Still you have the squared value here."
4828.83,4858.86,"that has all kinds of mathematical advantages. You can also then take the square root of this, right? Because you are using a square here. This kind of helps to make it more interpretable. This is the root mean square error. And I hope that everybody sees, so this may be the intuitive notion that you would, I don't know, like to think of."
4858.93,4889.82,"But I hope that you understand if I try to minimize this, that is the same as minimizing the other two. Because they are, let's say, monotonically increasing in terms of each other, that if I try to minimize one of them, I'm also minimizing the other ones. That's why we have been using this one, but we can interpret it like that. This one is different."
4889.82,4920.59,"This is the one that as a user you probably would have liked to have. You take the absolute value of this difference and you look at the average error. This is what you would like to have. But you cannot have it because then all of the mathematical tools would fail. Like in this situation, you do not have these nice derivatives that we have seen in the previous lectures."
4921.1,4950.42,"So this may be the thing that we would like to have, but we cannot use it internally in an algorithm. But if we evaluate the result, there is nothing stopping us from using this. So we cannot use it during training, but we can use it during evaluation. So the problem of this formula is that although we are averaging over all the ends,"
4950.42,4980.53,"It is an absolute error in terms of this. And so think of that the target feature would be something like a distance. I can measure distance in centimeters, meters, kilometers, etc. And this would have an effect on this error. So it's an absolute error. So the fact is, if I do not know anything about an application, and people say the absolute error is 10, I do not know how to interpret it. Is that good or bad?"
4980.62,5010.48,"So it's an absolute value. Therefore, often we use the R square, and the R square has the advantage that even if you do not know anything what the model is about, you can still evaluate it. And the reason is that it's kind of a relative notion. So how does it work? So the R square is 1 minus the sum of squared errors, which you know."
5010.48,5042.32,"divided by the total sum of squares. And that's what you see here. And I would really like you to take a look at this expression. So here we have the average of t, and here we have what the model predicts. So you can think of this as a competition between a sophisticated model, for example the neural network, and a technique that just gives you the average. Right? I want to..."
5042.32,5068.67,"I want to create a model to guess your income, like a baseline would be that I would average all the incomes of the people in this room, and that would always be the number that I return. That's a possible model. So by dividing these two things, you get a relative performance. And the one minus is just there because we want..."
5068.67,5097.23,"interpretation that we have is that if the R square is equal to 1, it's good. If it's close to 0, it's bad. So this is a relative notion and you can easily see that if I multiply all my instances with the factor 1000 and I do the same with the model, these expressions would not change. So it's a completely relative notion."
5098.1,5124.74,"This now scales on 0 and 1. So 0 means that we are as good as guessing the average. 1 means that we are not making any errors. Right? So now, like if you look at the R square, you can interpret it. Note that in principle you could be lower than 0."
5124.74,5152.83,"But if you're lower than zero, that means that you're worse than guessing the average. So that is a value that normally you should not see. Or you're doing something wrong. Do you see that? Also why it's relevant. It's scaling, basically, performance to zero and one. And here I'm showing you, let's say, three examples. So the real values."
5152.83,5183.07,"are always the same. The model is predicting different values. I've set up this kind of mini example that the mean absolute error is the same everywhere, right? We are always making, so here we are making a mistake of one, here we are making a mistake of one, here we are, so in total two, here we are making a mistake of two, mistake of zero, mistake of zero, so again we have two as a total error."
5183.07,5211.47,"So here we have a mistake of 1, here we have a mistake of half, here we have a mistake of half. So again, the sum is equal to 2. So the mean absolute error is always the same. But if we start looking at the sum of squared errors and we look at the r squared, then you can see that there are different values. Just for you to get used to the notions that I'm showing you."
5211.47,5241.47,"Cross-validation, I'm just repeating this here, because before we were talking about categorical, if you talk about real values, you can of course do exactly the same. Nothing changes. So in the last, let's say, few minutes, very briefly about A-B testing, correlations, etc."
5241.47,5270.64,"If you are influencing reality, right, then you can no longer trust your predictions. And also, models will pick up on correlations, but correlations do not imply that there is a causality between things. And you've probably seen plots like this before. So here's the chocolate consumption, and here you have the number of Nobel laureates."
5270.64,5295.68,"And there is clearly a positive correlation between this. So if you consume more chocolate, you get better research, is what you could conclude from this or the other way around. The other, let's say, typical example is that if people eat ice cream, there are also more people with sunburns. So I could look at the data set."
5295.68,5321.36,"And you could conclude that if people stop eating ice cream, nobody will get a sunburn. But this is, of course, not true, because there is this hidden variable that is the sun that is influencing the two other variables. So in causal models, you're not focused on, let's say, correlations, but what is the actual connection between these two. So here's another cartoon."
5321.36,5350.82,"Someday I want to get married because studies show that married people are happier, but you could also reverse that, that nobody wants to marry an unhappy person. Correlation and causality, I think everybody in the room has a basic understanding of what that is causing. Often there is a hidden variable that you need to detect to really understand things. Very briefly, A-B testing."
5350.82,5376.37,"I do not want to explain about the details, but this is a study that I did myself together with a bunch of colleagues. Then we were at UWV and we had a very good model where we were able to predict whether somebody was going to raise an official complaint, which is very important because it includes a legal procedure."
5376.4,5405.18,"And our model on the training data was performing perfectly, right? So we could identify the high-risk cases and then we created an intervention that people would automatically get emails trying to avoid them from doing this. And the result was that more people started to raise official complaints. And this is like a typical example, right? Why you need to do A-B testing."
5405.18,5414.93,where you look at the situation before and the situation after. We continue on Wednesday with unsupervised learning. See you then.
