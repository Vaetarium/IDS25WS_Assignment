start,end,text
1.68,27.57,"Question number five of where we talked today about support vector machines. I'm still a bit jet lagged, right? I came from Uruguay and then in the weekend I was giving a tutorial at the European, let's say, AI conference and now I'm here. So the topic of today is a bit in between."
27.57,57.74,"like what you have seen last week, where we looked at very basic techniques that talk about first decision trees and then regression. What I talk about today is very much related to logistic regression, the technique that you have seen last week. And this is kind of in between these simple techniques and neural networks with the topic that we will start with, let's say, later this week. So it's a bit in between."
57.74,85.12,"And it helps to make the transition, let's say, from the basics to more advanced techniques where you also see a bit of the challenges that we are facing. So this is the agenda of today. So we basically have just one topic, support vector machines. We start very simple. So we assume that we have a data set that is separable, where..."
85.12,114.78,like you can think of it we are in an n-dimensional space and there is somehow a plane in this n-dimensional space which is I don't know separating the dog pictures from the cat pictures right or the students that are successful from the students that are not successful so we start with that basic assumption and then explain the machinery step by step and then later we will let's say see because for most data sets
114.78,139.82,"Such a separating plane, of course, does not exist. And then we look at extensions that are, let's say, addressing this problem. So let's simply jump into it with a bit of the things that we have seen before. So we have seen logistic regression. That is the technique that is most related to what we will talk about today."
139.82,168.83,"And the basic idea of logistic regression is that you try to separate, let's say, one class from another class. And it starts with just looking at a random, I don't know, separating line. And if you do that randomly, of course, the performance is very bad. But then there is this mechanism of gradient descent where you know..."
168.83,195.54,"what you should do to reduce the error. So this is of course a separating plane that's very bad. And then over multiple iterations, you start improving, let's say, the line that is separating, let's say, one group of instances from the other group of instances. And the trick why this works is that the problem is defined in such a way"
195.54,224.45,"that you can compute the derivative of the error so you know in which direction you have to walk to reach a global minimal error. And that is why this problem is formulated in the way that you've seen it before. So why is this relevant? Suppose that we are very naive and we would simply say, okay, we have instances."
224.45,253.2,"that have a target feature that is 0 or 1, and we have a model that has as outcome 0 or 1. Let's say instances above this line are 1, instances below the line are equal to 0. If we would make that naive understanding, it's very easy to understand this formula."
253.2,280.18,"because then you simply look at what is the error. But the point is, if we are just dealing with, let's say, values 0 and 1, we do not know in which way we should walk, because the derivative will be equal to 0. So we do not know in which direction we can improve the error. So this is why I show this picture. So if you are in a completely flat area,"
280.21,310.7,"And you should think of the altitude as the error that you have so far. If it's completely flat, you do not know in which direction you should walk to reduce the error. That is why we are using this logistic function that has this formula. And that kind of ensures that we are not in something that's completely flat, but something that has a slope. So that we know in which way we can walk."
310.7,337.87,"such that the error is reduced. Later, if we talk about neural networks, you will see again the same function for the same reason. So at first this function may seem very odd, but the derivative of this function has certain elegant properties. That is why it's used all the time. And also the interpretation is quite easy. If you look at this formula,"
337.87,364.5,"and you look at the value w times d, then this value can range between minus infinity and plus infinity, and this function squashes it between 0 and 1. And you can interpret this as a probability where, I don't know if we are at value 0, so on the separating line."
364.5,393.22,"You can think of this, okay, it's 50% in one class, 50% in the other class. That's the way to interpret this. So how does this help? So this plus and this plus and this plus. So this is on the decision boundary. So then W times D is equal to 0. And if we fill out in this formula 0,"
393.22,420.11,"then you will see that the result is half. So if a point is on this line, then you can interpret it. Okay, it may be 50% plus 50% the triangle. Right? So we do not know. So here there is no error. Then if we are, let's say, on this side of the line and you fill out the formula, you get, let's say, 0.1192."
420.5,449.57,"If you fill out this value. And the interpretation of this is that it is likely to be in the class associated to zero. So we would expect this value to be, let's say, a triangle. But it's not. And if we look at the error, if we fill out this, then it would be... So for this one..."
449.57,478.99,"we would predict it to be on the other side. And the further you are away, the more certain you are about the value. So for this value, this is a big error, because with a high certainty, you would expect a triangle. That's the way to interpret it. So the further away you are from the line, the bigger the error is. And that helps, let's say, the mechanism to smoothly walk down."
478.99,508.99,"to this decision boundary. So that was a brief refresher of where we stopped, let's say, last week when we talked about logistic regression. And now we go to SVMs. So SVMs are, let's say, much more sophisticated. And things are formulated in a certain way that there are opportunities to classify, let's say, a much larger class of examples successfully."
508.99,536.35,"So if this is one class and this is the other class, so these are, for example, the values 1 and these are where the target feature is equal to 0, there are many lines that we can draw to separate these two classes. So I can draw many possible lines. But it's clear that some lines are better than other ones. So here..."
536.35,566.13,"I would argue that the red line is better than the black line. Like the probability that they classify, let's say, unseen instances correctly is much higher if I take the red one. Because it's further away from the points. So you can think of this now as, let's say, a line that you can inflate. And you inflate it as much as possible."
566.13,595.7,"And if you inflate this as much as you can, as I just indicated, what you see is that you cannot further inflate it because on this side there are these instances and on this side there are these instances that are preventing it from inflating further. And the thicker this line can be, the bigger the safety margin is. So the goal is to..."
595.7,625.15,"to separate these points with a very big margin in both directions. And of course you now immediately think, okay, but there may be other points that are exactly in the middle, how to do that? This is, later I will talk about soft margins, and you can see that you can still use this ID, even if there are points in the middle. But now we assume something is completely separable, and we simply try to create"
625.15,656.58,"a line in between them that is as far away from, let's say, the points as possible. Do we see that? So what we now see is that instances are these points. We have, let's say, this one class that you can think of as the class 1. And you have this other class that you can think of as a class 0. And what you now see is that only these points matter."
656.58,683.49,"These points that are very far away from the decision boundary, they play no role for this safety margin. So these points that are on the line that we get if we inflate the line as much as possible, are called support vectors. And I hope that everybody sees the rest of the points do not matter. The rest of the points are easy."
683.49,709.84,"The things that are, let's say, in danger are the things that are as close as possible to the decision boundary. So only the support vectors matter. And if I visualize this, this is what we get. So we spent the whole lecture deriving now, let's say, the conditions to maximize, let's say, the safety margin and how to do it."
709.84,738.1,"Remember that if you talk about logistic regression, a point that is up here is influencing where this line is running. But now points that are here and points that are here play absolutely no role. Because they're very easy. So we should not focus on them. We should focus on the points that are difficult to classify. That's the goal."
753.36,785.58,"So here, if I would remove this point, it would still look exactly the same. If I would remove this, then it could have gone like this. It depends a bit on the other points. But just think of this as a line that you inflate. And in the end, it is stuck between at least two points. No, no. Just one point is enough. It's just the way that I draw it here."
785.58,816.13,"But here I'm only showing the point. So for example, if I would... So here it doesn't matter because of the way that I had drawn it. But just remove this point and remove this point. And the thing would probably be a bit like this, if you see what I mean. So if I would remove this one and this one, then it would be a bit like this. And then you would see that there would only be two."
816.13,845.79,"And of course you need to think this in like a hundred dimensional space, right? So it's a bit difficult. Yeah, so we focus on these support vectors and the intuition behind is that we are focusing on the points that are difficult to classify, not the points that are easy to classify. So if I'm classifying, let's say, pictures of dogs and pictures of cats into dog or cat,"
845.79,875.68,"then like a dog support vector would be a dog that looks very much like a cat. And a cat support vector would be a cat that would look like almost a dog. So these would be the support vectors, the things that are difficult to classify and probably close to the decision boundary where we are not sure. So that's what we would like to focus on. So our own dog, Miffy,"
875.68,900.64,"Like is clearly not a support vector. So Miffy, our dog, would be something like here, where everybody can immediately see that it's a dog. So we do not need to spend a lot of time on classifying Miffy, because that's a clear case. We should spend our time on the things that are close to the decision boundary. That's the idea."
900.64,931.14,"So let's elaborate a bit on this separating plane. So if we have a one-dimensional space, we have just one feature and we would like to separate it, we get a hyperplane of dimension zero. And dimension zero is simply one point. So here I want to separate the white instances from the yellow instances and the separating plane."
931.14,959.65,"is a point exactly in the middle. This is where I have two dimensions, and the pattern that you should see, if I have an n-dimensional space, so I have n features that I'm using, the separating plan has n-1 dimensions. So here I have two features that I use to classify into yellow or white."
959.65,989.66,"And that means that the hyperplane separating this has a dimension of 2 minus 1 is equal to 1. If I go to three dimensions, it becomes very easy, because what you see here, we have three dimensions, so the separating plane has two dimensions. And here I cannot draw four dimensions, but this is what you should try to do in your mind. So if we have n descriptive features,"
989.66,1019.92,"that we are using for classification. So we have n descriptive features. The hyperplane has n minus 1. And as I said, the problem is I can draw this still if n is 3. If n is 4, I can no longer draw it. But that's what you should try to do in your mind. And these SVMs that we talk about now are very well equipped of dealing with, let's say, very large dimensional spaces. Much better than regression."
1020.56,1047.7,"So let's become a bit more technical. So what is a hyperplane? A hyperplane can be described by, let's say, an expression like this, that I can also write like this. So this is the dot product between the weights and the, let's say, variables corresponding to the instance. So think of x."
1047.7,1074.26,"These are these dots that are here in this n-dimensional space. And a hyperplane is described, let's say, in this way. So it is constraining the values of x to the plane that I see here. This is the dot product, and I can also write this completely in full, like this."
1074.58,1104.34,"And the only constraint that we have for such a hyperplane is that at least one of these W's is not equal to zero. Because if they would all be equal to zero, then B has to be equal to zero and we have no constraint, we would simply have everything. But if one of these values is not equal to zero, then we have defined such a hyperplane in an n-dimensional space."
1104.34,1131.06,"And if we can define this plane using such an expression, we can basically talk about being above it or being below it. So such a plane is separating the points in this n-dimensional space in the ones that are bigger and the ones that are smaller. Clear? That's the basic idea."
1131.57,1159.55,"And I will describe this in a more technical sense in a minute. So what to do if things are not separable? Then there are two cases. It could be that we are dealing with an outlier. We need to deal with that, with a technique that we call soft margins, that I will explain. But the problem can also be structural. And then you need to use things like kernels."
1159.55,1188.75,"That's another technique that I will talk about later today. So here we see an example where I cannot draw, let's say, a line here in between them separating the yellow dots from the white dots. I cannot draw it. So if I look at this situation, the same story, there is no line that I can draw here. I can do this."
1188.75,1218.34,"But then I'm making errors with respect to these two instances and errors with respect to these two things. And so these would be cats that we classify as dogs or dogs that we classify as cats simply because they are too much looking like the other species. Right? And this can happen. So what we will do later, let's say in this lecture, is that we relax the problem. So we still try to inflate"
1218.34,1243.22,"this, let's say, safety margin. But at the same time, we have another force that is allowing us to have a few outliers. So then we look at the mixture of these two techniques. So on the one hand, we try to maximize this. On the other hand, we try to minimize, let's say, the points that are violating this constraint."
1244.43,1270.93,"will become clear. So we have two forces, and later you will see a formula where you can basically balance between these two forces. So this example is an example where you could argue, okay, we have like this one and this one, they're kind of outliers, and this one is an outlier. But if you look at this situation, that is no longer possible, right?"
1270.93,1299.47,"We can say, okay, the boundary is here or the boundary is here. But we are making a structural problem. If we put the boundary here, right, and we are classifying all of these points incorrectly. If we put the boundary here, we are classifying all these points as incorrectly. So here we have a structural problem. And the structural problem, so here you see the same thing. Like I cannot draw a meaningful line."
1299.47,1326.37,"separating the yellow and the white dots. So here we are not dealing with outliers, a few exceptions, dogs looking like cats, but we are still, we have a, let's say, a fundamental problem. At the same time, everybody can see, okay, if you would have like a circle, then suddenly you would be able to separate it. So we can no longer use a linear function."
1326.37,1356.77,"We need to address this more systematically. And when we talk about kernels, this is exactly the topic that we will do. So the way to think about this, if you have a problem like this, is that you try to lift the number of dimensions. So you can think of this if you have something like this. If there is a way to transform it in a two-dimensional space, then I can nicely separate it."
1356.77,1384.64,"So we increase the number of dimensions in order to still be able to do linear separation. Right? Simple. Right? Not really, because now I'm going to show you the formulas. Right? And we need now, like I assume that everybody has, let's say, linear algebra background, but I'm repeating some of the stuff so that..."
1384.64,1414.5,"If your knowledge is rusty in that area, that you're not immediately lost. I'm taking some of the, let's say, pictures that are in this book. Just like with the other books, you do not have to look at it, but you can look at it if you want to go deeper. So we need to talk about vectors. So this is a vector. And I think everybody, so Pythagoras, what is the length of this vector? It's equal to five."
1414.5,1442.05,"And this is the general formula. So if we have a vector like this, then this is the length. And I'm showing you that you understand this symbol. That's the length of a vector. Simply the basic things. You can normalize a vector. So if you divide x1 until xn by the length of the vector, you get a vector of length 1."
1442.05,1472.5,"As I said, standard linear algebra repetitions. So if I look at this vector where I divided it by the length, the length of this vector is equal to 1. So we can always normalize a vector. Something that is very important and that we will use multiple times today is if I have a vector, I can basically change this vector by multiplying it with the constant Q."
1472.5,1502.02,"And that basically means that I change all the different entries. And what you will see later is that if we talk about inflating this line, etc., that is often related to, let's say, multiplying a vector with a particular number to scale it. Then there is the dot product that we have already been using on the previous slides, but it's important to get the intuition."
1502.02,1528.93,"So the dot product of two vectors, we have seen it already when I talked about the separating plane. The dot product of two vectors is that we simply multiply x1 times y1, x2 times y2, etc. And until the end, xn times yn, and we sum up all of these numbers. So this is the dot product. Now what is very important for your intuition..."
1528.93,1557.87,"is that this dot product can be easily, let's say, computed. But you can also rewrite this dot product using another expression, and that is this expression. So the dot product that I just indicated is equal to the length of the first vector times the length of the second vector times the cosine of the, let's say, the angle between these two vectors. Right?"
1557.87,1582.61,"So this expression is the same as this expression. So later, we will talk, for example, about when a vector is orthogonal. And if a vector is orthogonal, then the cosine is equal to 0. So the dot product is also equal to 0. And here you can see some numbers."
1582.61,1610.85,"So if the angle is equal to 0, then the cosine is equal to 1. So we simply multiply the length of the vector. If it's 90 degrees, then the cosine is equal to 0. So we have this value. If it's 45 degrees, so it's a bit in the middle, then it's the product of the two lengths of the vectors times 0.7. If it's 180..."
1610.85,1640.05,"So it's pointing in completely the different direction. It is maximally negative. So this hopefully refreshes some of your knowledge. And here I show the graph. So we are looking at the blue dashed line. That is the cosine. If the angle is equal to 0, it is 1. If it's equal to 90 degrees, it is equal to 0. If it's equal to 180 degrees."
1640.05,1670.77,"So pointing in opposite directions, it is minus 1. That hopefully refreshes some things there. Then hyperplane. I already showed you this formula. Now in an n-dimensional space. So our instances look like this. So we have an n-dimensional vector. We have n descriptive features. And the separating plane is basically the line that is putting constraints on these numbers."
1670.77,1699.33,"And if this is equal to zero, we have a point that is on the separating line. If it's bigger than zero, it's on one side. If it's smaller than zero, it is on the other side. So these are the three variables. These weights kind of describe this, and we have this constant B, and this is defining a hyperplane."
1699.33,1729.04,"Instead of writing this, but you should know this, and I can also write this like this, that this would be the alternative for formulation. So a hyperplane, a line like this, is fully defined by this weight vector and this constant b. So whenever we have a weight vector and a constant b, we are completely defining a hyperplane."
1729.04,1759.87,"And here I'm showing it, let's say, for two dimensions, but this also holds in the n-dimensional space. So if I look at these values, so the w vector is 0.4 and 1, and b is equal to minus 9, we are describing this line. For example, take that x is equal to 0, right? So x is equal to 0."
1759.87,1790.34,"So this one is 0. Then to be on this separating hyperplane, this other variable, let's say x2, needs to be equal to 9. Because then we have precisely this value. So here I show you, let's say, again, what does it mean to be orthogonal. So a vector is orthogonal."
1790.34,1819.12,"to a line if the cosine is equal to zero, right? What I just indicated. And that corresponds to the dot product being equal to one. So if you think about this for a while, that if this is my separating plane, defined by this, let's say, weight vector w, then this weight vector w is orthogonal to the separating plane. And for the direction,"
1819.12,1847.6,"B doesn't matter. B just says where it is running, but it doesn't determine the angle. We will often use this, so we talk about the separating plane, and then we want to walk in an orthogonal direction, and this W that is explaining this direction is exactly the same W that is used to define the separating line. So there can be different hyperplanes."
1847.6,1876.74,here showing you in the two-dimensional space so this is one that looks good but for example you can see here there's another one this would make an error here there are some errors made here this is also let's say running okay yeah so again I'm repeating a bit what I'm saying and I think that no that's okay so here you can see that I scaled the weights
1876.74,1908.27,"compared to what I had before. An important thing that I indicated before, if this holds, then also this holds. So if I have an expression like the dot product of w and x plus b is equal to zero, then if I multiply this side with q, and I multiply this side with q, then this side remains zero. Zero times q is zero."
1908.27,1937.22,"But on this side it looks like this. So I can rewrite this into this direction. Even if Q is not equal to zero, then if this holds, then also that holds. Because it would correspond to dividing by Q, which is a non-zero number. So these two are equivalent. They describe exactly the same hyperplane."
1937.22,1968.18,"as long as Q is not equal to zero. Do we see this? Right? It's very important. I often use the term, it's above the separating plane or below the separating plane. This is highly confusing, right? I use this for your intuition, but it is very misleading because I can rewrite this expression with Q is minus one, right? And then I'm basically swapping the two."
1968.18,1994.32,"Right? So like above and below, you should take that with a grain of salt. That is just for your intuition. Right? Because by multiplying it with minus 1, right, the whole space kind of changes. Although the separation line does not change at all. So this is the goal that we have. So we want to find the separating plane in such a way that the crosses here are..."
1994.32,2022.3,"I now say it again, above the line, but you should take that with a grain of salt. And the red pluses are below the separating line. So that's our goal. So here I should show it again. So the points here should satisfy this expression, which I can write like this. And the points here should satisfy this expression. Again, that's why I put an explicit note here."
2022.3,2052.4,"Note that x is bigger than 0 is the same as minus x is smaller or equal to 0, right? So that means that if I use this q is equal to minus 1, this is equivalent to this. So just by negating all the numbers, I swap whether it's above or below, right? And that's why you should be careful if you interpret these things properly."
2052.43,2082.66,"So here you see a concrete example using the numbers that I had before. So based on these values, we get this line. And here, like we want these triangles to be above the line, and we want to be the stars be below the line. That's a basic idea. When we talked about logistic regression, we were using values 0 and 1. What is very common?"
2082.66,2111.42,"As you will see, let's say, later today, if you talk about SVMs, we typically talk about minus 1 and plus 1. And why that is the case will become clear, let's say, very soon. So that's what I show you now. So we use the convention that, let's say, the points that belong to one class are labeled minus 1. So this is..."
2111.42,2140.91,"These are the descriptive features. These are my target features of a certain instance i. So these are labeled minus 1, and the others are labeled plus 1. It's exactly the same idea as with regression. There we had 0 and 1. Here we have minus 1 and plus 1. And that has a mathematical reason that will become clear to you. So we want to find a hyperplane that separates the minus 1 from the plus 1 instances."
2141.01,2166.54,"And we will do that step by step. And what you will see is that I need, let's say, two or three attempts to get the problem into the way that we want to have it. But I'm doing it in multiple iterations in such a way that you can still follow it. If I would immediately show you the formula, you would be completely lost. So let's assume that we have something that is separable."
2166.74,2197.15,"where we have a line that is separating the instances that have a target feature minus 1 and 1. And then we can simply say, okay, if this is bigger than 0, we predict that the target feature is plus 1. If this is below 0, we predict the target feature to be minus 1. That's the convention that we are now using. So this is what I showed before."
2197.15,2226.62,"And you can see that this is good, this is good, this is not good, this is not good. Right? With the red and the yellow. So we could now, right, and now it's good that you, right, if you don't understand something here, it's very important that you ask, right? Otherwise you will be lost. So this is our input. We have I, so we have instances that are described."
2226.62,2256.59,"by a vector describing the descriptive features. And we have one target feature, Wi, that is equal to minus 101. And these other features, they are just numbers. They can be anything. And we have m instances. So this is our input data that we use to train it."
2256.75,2286.26,"And we would now, given this input, which is simply a table where you have columns describing, let's say, these things, and these are numbers. And the last column is a column where we just see values minus 1 and 1, because we are doing binary classification. And we have MROS in the table. So this is our input. And given this input, we would like to find a separating plane."
2286.26,2315.47,"a vector w and a constant b in such a way that this holds. So if this value is above zero, then it's plus one. If it's below zero, it's minus one. And that's what we would like to find. And we are assuming that that is possible, as you can see here. But there may be many lines. If you look at this, there are infinitely many separating planes. So which one do we take?"
2329.42,2355.31,"You don't understand it yet, why we are using the minus one and the one. It's just binary classification, and the convention is that we relabel the target feature. So the target could be cat or dog, and we now use the convention to call cat minus one and dog plus one. It's just a convention. Why we need it, you don't see that yet, but that will become clear."
2360.46,2390.26,"No, so in logistic regression, it was 0 or 1. It was also binary classification, it was 0 or 1. But there we had the convention for technical reasons that it was more like a probability. That was why it was between 0 and 1. Like the probability to belong to a particular class. Here, you don't understand why it's minus 1 and plus 1, but I promise you that will become clear."
2390.58,2420.48,"So we are now dealing with a problem. So if you would simply brute force, let's say, test all possible values of W and B, you would be able to find separating planes. There are many of them. And I hope that you understand there is precisely one that has a maximal safety margin that we get by inflating, let's say, this line in the middle. So we would like to..."
2420.48,2449.78,to create a separating plane where the distance from the hyperplane to the next point is as large as possible. So I can formulate this. So this is my point. This is an instance here. And this is my separating hyperplane. And we want this distance to be maximal.
2450.06,2479.17,"And if I want to walk from this point to the separating hyperplane, I need to use a vector that is orthogonal to this separating plane. And which vector is orthogonal? That is the vector w, as I showed you earlier. So we want to maximize this distance. And here I showed you, if I look at an instance here, x1,"
2479.17,2509.15,"and I want to walk to the closest point on the separating hyperplane, it will look like this. Because we have a vector that should be orthogonal to it. So we have this expression. This one is defined of taking this point and then walking using this orthogonal vector, lambda times w."
2509.15,2538.56,"this point x2, we know that it satisfies, let's say, this expression. So we have a point x2 that is on the line, so it should satisfy this expression. And we have a point x1 that is possibly not on the line. And this lambda times w is the way to walk from this point to that."
2538.56,2566.45,"And because it's orthogonal, we know that it is the shortest. So now I can simply, if I look here at this x2, I can replace this x2 by looking at this formula. So I bring this to the other side. So x2 is equal to x1 minus lambda w. So I simply fill out this x2 value here and I get this expression."
2566.8,2594.93,"The distance between this point and the line, the one that we would like to maximize, is equal to the length of this vector. Right? Is equal to that. Clear? So now we are just using mathematics, right? So we have this expression and we have this expression and we're going to play a bit with it. Right? So I copy this to the next slide. So we now have this and we have this."
2595.66,2626.51,"And the nasty thing that here is we have this value lambda, and we know that such a value exists, but we do not know which value it has. So the goal that we now have is to remove this lambda. That's our goal. We want to find an expression for this lambda. So if I look at this expression, I can bring this to the other side. So we get w times x1."
2626.51,2655.65,"The minus 1 goes to the other side. That becomes this expression. And the b stays on this side. Note that if I multiply, if I take the dot product of a vector with itself, they point in the same direction. So the cosine is equal to 1. So we simply get the product of the length of the vector. So this expression is very easy because it's just, let's say, the squared length."
2655.65,2682.91,"Right? That's what I did here. Right? So I'm rewriting this into this, and then we get this. So why did I move this to the other side? So now you can say lambda is equal to this divided by that. Right? So we have this. So I achieved my goal. I wanted to get rid, I wanted to compute this lambda, and I find an expression for the lambda. Right? So I achieved what I wanted to have."
2682.91,2711.86,Here I simply copied again what was on the previous slide. I did not do anything. But now I can compute the distance. Like it's lambda times w. I now know that lambda is this. Note that lambda is a number. W is a vector. But we look at the length of the vector. So I can simply... So here I fill in this lambda value. We get something like this. And I can simplify this.
2712.3,2743.15,"to this expression. I simply rewrite this as a note that these are just numbers. We have this squared, we have this here, so it can be simplified to this. Clear? So now we have an expression for d. The distance of this point x1 to the separating hyperplane."
2744.3,2770.29,And we would like to maximize the minimum distance. And note that this is a max-min problem. Do we see that? So we look at all the points and we would like to maximize the distance between the separating hydroplane and the point that is closest. Because that's our safety margin that we would like to maximize.
2770.77,2796.85,"So now we have, let's say, formulated this as a completely, let's say, mathematical problem. We want to find, let's say, this W vector and this B value in such a way that this is maximized under these constraints, where we are perfectly separating the points. Yeah? Unfortunately..."
2797.07,2824.85,"If you write it like this, you still need to use a kind of brute force approach. You cannot solve this problem very easily. So that's why we are going to do another round. To get to an expression that we would like to see. But I'm doing it in different steps in such a way that you can still track what I'm talking about. So here we have an if-then to two if-then statements."
2824.85,2852.46,"And in mathematical terms, you don't like if-then statements, right? Because that leads to a combinatorial explosion of possibilities. We also want to simplify this expression that you have here. So what are we going to do? We are going to make one assumption that makes suddenly all these expressions become very simple, right? And the assumption is that we assume..."
2852.46,2882.19,"that, let's say, the support vectors are on the line with the value 1 and the value minus 1. So we make this assumption. And why can we make this assumption? This is related to what I explained earlier, that we can multiply things by Q. If Q is, let's say, non-zero, we are not changing the problem at all. So in other words, we can basically use scaling."
2882.29,2910.99,"in such a way that we can use this assumption that we assume that the support vectors are on the line W times X plus B is equal to 1, and W times X plus B is equal to minus 1. So this is like a trick, right? But we can do it because we can multiply things by Q and it scales. So let's see what happens if we do this."
2911.18,2940.14,"So we simply make this assumption and we are allowed to make this assumption because we can always multiply, let's say, our separating hyperplane with the value q and that would not change the problem. That's why we can make this assumption. So I'm now writing down everything that we need to know. So all the red points are above this line corresponding to this expression."
2940.14,2969.25,"All the blue dots are below this line. This. And this area between these two lines is like a no man's land. There are no points there. Right? There are no things here. And like the distance is again, so the short distance from this support vector to the separating hyperplane is again equal to the same thing that we saw before. Right? And the reasoning is,"
2969.25,3000.0,"That from a point to go to the separating hyperplane, the shortest path is always an orthogonal path. That's orthogonal to the hyperplane. So this is now the alternative formulation. And I'm now going to again play with all of the formulas. So before we had this, right? So this was the original expression. And if you now see, I can now, before it says zero, right? But now it says bigger than one."
3000.0,3030.58,"and smaller than minus 1. And now you can see why I chose plus 1 and minus 1, because this can now be rewritten into one expression. Right? So this is, there was earlier a question, why is it plus 1 and why is it minus 1? Because now we can use this, let's say, convenient trick. And let me explain this. So if this value is bigger than 1, and this value is plus 1, if I multiply these two,"
3030.67,3059.87,"it is also at least 1. If my target feature is minus 1, and this is lower than minus 1, and I multiply these two, minus 1 times minus 1 is equal to 1. So this logical expression can be rewritten in this way. So suppose that we are now, let's say,"
3059.87,3082.48,"If this would be bigger than 1 and this would be minus 1, this would not hold, right? And vice versa. So from a logical point of view, these two if-then-else statements collapse into this expression. So that was the first goal that we had, right? That we wanted to get rid of these two if-then-else expressions."
3084.56,3111.98,"Now we also want to have a simpler expression for this distance notion. And again, here I didn't do anything. I just repeated everything that I said before. Just assuming that the support vectors are on the line 1 and the line equal to minus 1. So if I have a point here and I want to go to the closest point."
3111.98,3139.09,"On this other line that is corresponding to the support vectors, it has a length of 2d. And again, we need to walk orthogonal, let's say, to the separating hyperplane. So the distance between this line that contains one or more support vectors and this line that contains one or more support vectors is equal to 2 times d is equal to this. Right? That's it."
3140.3,3170.94,"This point on the other line, we can describe that. Again, the same trick. X2 is equal to X1 walking into this direction indicated by W. And the length is now 2 lambda. And why is it 2 lambda? Because we take this step and we take this step to get to this point. Clear? So now I simply fill out..."
3170.94,3199.86,"So we know that x2 is on this line, right? And we know that x2 is equal to this expression. So we simply replace x2 by this expression and then we get this. This support vector here is on this line that equals to minus 1. So we have this expression. Clear? This is what we can conclude from the pictures."
3200.08,3230.21,"And now the only thing that I have to do is that I need to manipulate these formulas. And remember, the goal is again to have like an expression for the distance. Right? For the distance d. So I simply copied the two expressions that we had before. Right? And I can simply rewrite this. So we get w times x1 plus b."
3230.21,3257.94,"And I do W times 2 times lambda W, that is equal to this. W times W is again very easy because it's the square of the length of the vector. So I can now, if you look at this part here, that is exactly the same as this part. So I can now fill in minus 1 here."
3258.93,3288.64,"And now we have an expression that just talks about W and lambda. And I can move the minus 1 to the other side and then rewrite it. So lambda is equal to 1 divided by the square of the length of the vector. And that was again our goal, to have an expression for lambda just in terms of W."
3288.64,3317.1,"the vector that we would like to do. So here I have an expression for lambda. We would like to maximize the minimal distance. So we like to maximize this expression. So lambda is equal to this. This is the thing that we would like to maximize. And now note, so here I filled out this, so the lambda I filled it out here, then we get something like this."
3317.1,3348.59,"So maximizing the distance corresponds to minimizing the length of vector w. Right? It has become very simple. Right? And it has become very simple because I simply made this assumption, let's now assume that it's on line 1 and line minus 1. Yeah? So this is what we now have. So our reformulated problem,"
3348.59,3378.77,"This is our input. This is our constraint. So the two if-then statements have collapsed into this single expression here. And we would like to minimize, let's say, this W, such that these constraints hold. Then, for technical reasons, but that's very easy to understand, if you want to minimize, let's say, the length W,"
3378.77,3407.39,"you can also rewrite this to minimize the square of W. Nothing changes. It's a positive value. So this problem that I show you here is equivalent to this problem. Nothing changes. Minimizing the square is the same as minimizing the value, so nothing changed. And this is now the final form."
3407.39,3438.98,"And this is a quadratic optimization problem where there are, let's say, solvers that are, let's say, tailored to solve this problem as efficient as possible. Yeah? Clear? So this is a formula that you should know and understand how it works. Right? So if you get like a small example, these are the things that you should be able to reason about. Yeah? So this is a convex quadratic optimization problem that can be solved."
3438.98,3466.77,"Let's say efficiently. And that's why I made all the constraints that I showed you before. So a summary. If we want to solve this problem, we just need to solve this optimization problem. And there are standard solvers to do that for you. Yeah? It's so beautiful it makes you cry. Doesn't it? Okay."
3467.09,3492.14,"But I've made one very important simplifying assumption. Everything that I talked about before assumes that there is a separating plane, right? But in any real-life application, that will not be the case, right? So we need to relax the problem a bit to make it work, right? So if instances are not separable..."
3492.14,3522.77,"then there can be these two reasons that I indicated before. There can be outliers or there can be a structural problem, right? And both issues we still need to address. So this data is nicely separable. If I now just put a white dot somewhere here, it is no longer separable. And everything that I talked about before would completely collapse, right? Because there is no solution for the optimization problem. So this one would destroy it."
3522.77,3553.39,"So the one that I show here makes it non-separable. So there is no solution. But also this one would not be, let's say, very good. Because this one point would influence completely where this line is running. Where you probably would like to have something that is more in the middle and accept this outlier. So now we are going to balance two forces."
3553.39,3583.62,"We allow points that are violating our constraint. So, in principle, all white dots should be below this line and all yellow dots should be above this line. But that's not the case. So, we now allow for this with a certain penalty. And we are solving an optimization problem where we have a goal function and we can simply manipulate the goal function"
3583.62,3613.62,"to allow for such, let's say, exceptional cases to some degree. And that's actually very simple. But please ask if things are not clear. So here, we had this constraint, should be at least one. But now we write here minus epsilon. And the goal is that epsilon is zero. An epsilon equal to zero."
3613.62,3641.71,"means that we can still do this. But we tolerate, let's say, that this constraint is being violated. But there is a penalty. So think of this epsilon as a penalty if we need to use it. And we are summing over all the epsilons for all the different instances. And we multiply this with a constant c. So this is the part that we would like to minimize before."
3642.86,3669.9,"Now we allow for, let's say, these imperfections, but these imperfections also have a penalty. And with this constant weight, we can indicate how we count this penalty. So if we choose C equal to infinite, right, then we basically get our original solution."
3670.29,3703.06,"If we would say C is equal to zero, which would be very stupid, right? Like any solution would work, right? So you need to play with this value of C to see how it happens. Yes? Epsilon is the penalty, but at the same time, it's both the penalty and it is to what degree you relax this expression for a particular instance. So it's a penalty, and at the same time, it allows you to violate things."
3705.71,3744.64,"So it's what I showed in this earlier picture. So this y dot should be here, right? And it goes here. So this is, let's say, the imperfection. And that is also the penalty. So you're basically counting how far off this dot is. Yeah? Yeah, yeah. We are trying to balance two forces. On the one hand..."
3744.64,3771.71,"we would like to have something with a big safety margin in between it. That's the first expression. And we want things to be separable. At the same time, if there are imperfections like this, we tolerate it. But by choosing the weight C here, we are balancing these two forces. So this force says it should be completely separable."
3771.71,3801.14,"And we want to maximize the safety margin. And this force is being, okay, we are allowing for some imperfections, but there is a penalty associated to it. Later you will see some pictures and then this interplay will become clearer. So it's already here. So this is again the expression. And this is if I say that C is equal to infinite."
3801.14,3833.01,"then even a small epsilon would already be lethal. So probably if we say C is equal to plus infinity, we try to go as long as we can for epsilon is equal to zero. And that means that we have the original situation. If we say C has a lower value, as you can see here, so what you now see is that this line has changed."
3833.49,3863.22,"Right? Because we have changed this optimization problem. So now the safety margin of this point is somehow less important than it was before. If we make C smaller and smaller and smaller, you can see that these other points start playing a role. So there's an interplay between both things. So here, this is a bit like in this setting, you would say, okay, why don't you take this line? This is what we want to have."
3863.6,3893.7,"But the reason that we do that is that we can also run into situations where there is one outlier or things are no longer separable. So here things are still separable, but think of this point as an outlier. Because this point is there, the separating plane is here and the safety margin is very small. So the line is here, safety margin is very small."
3893.7,3923.22,"That's because we want it to be completely separable. If I set C is equal to 1, now we have a line in the middle. We are tolerating that this blue instance is on the wrong side of the separating plane. But we have a higher safety margin if we look at the other points that you see here. So we now see this as an outlier. Just think of it removing it with a certain penalty."
3923.22,3959.97,"And then we get, let's say, a nice separating plane. If we make this penalty too small, so here I made it 100 times small, then you can see that we get a solution that is not very desirable. But here it's still separable. So here in principle you could say, okay, I have this line and I'm happy with it. Yeah? Epsilon values, you do not have to get them."
3959.97,3992.37,"Because we are solving an optimization problem. So we do not need to decide on epsilon. Epsilon is in the goal function, so we try to reduce it as much as possible. So we don't have to decide it. It's part of the optimization problem. So we say that the error is if we are using an epsilon that is not equal to zero."
3992.37,4029.5,And we are using this exceptional situation. And our goal function says we try to avoid using this exceptional situation. And we only use it if we really have to. So you do not need to define it. It's something that you would like to minimize. So it's not part of the input. It's part of the part that we would like to minimize. Just think a bit about it. I hope it gets clear.
4029.5,4058.35,So we do not need to define it. We just want to minimize it. So this was fully separable. This was also still fully separable. But here you can kind of see why we have this. Here we have something that is not separable. So our existing approach would not be able to handle this situation. The one without the epsilons.
4058.74,4085.87,"would simply say there is no solution for this optimization problem. But using, let's say, this relaxed problem, we are able to allow for something like this. And now you can see this is probably the separating plane that you would like to have, with this as an outlier. And you can see what the effect is of playing with this value C. So with this value C, we are balancing two forces."
4085.87,4116.18,"On the one hand, trying to maximize the safety margin. On the other hand, let's say, trying to avoid that we have exceptional situations. Yeah? So perhaps this helps for you that most of the epsilons will be equal to zero. Right? So if they can be equal to zero, they should be set equal to zero. But once we need to use these exceptions, like, for example, for this one, that would be the case."
4116.18,4142.1,"So in this example, all the epsilons of all the instances would be equal to zero. Just for this blue one, we would have an epsilon value. And the epsilon value would be the distance from this point to the other side of the, let's say, minus one line."
4142.1,4169.58,"If in this situation I would say C is equal to 2 plus infinity, we have something that doesn't have a solution. That is how we can play with it. Our fourth question is what is now a value for C? It's a balance between two forces. Typical values are between 1 and 10. But you can play with this value, as it said here."
4169.58,4200.22,The best way to do it is that you train your model using a certain C value and then you apply it to unseen data what the performance of your technique is. And that would then help you to parameterize this. So you can play with multiple values and then see which is the one that works best. So this was the problem if we have a few outliers. But there are also structural problems.
4200.22,4230.61,"If you look at this, I think for everybody it's clear you would like to see a line that runs like this. But if I try to draw a straight line, I'm in problem. So I cannot have a linear separating plane behind these things. I can make many attempts, but it will always be, let's say, something bad. So we would like to have this. And the question is how to do this."
4230.61,4257.17,"And that is lifting the number of dimensions. That's the solution. So also this, so the idea is to get this, we still, so all of our machinery is highly linear, if you see that, right? We are multiplying vectors, it's all linear. So the way to deal with these nonlinear issues is to increase the number of dimensions."
4257.9,4287.84,And I hope that everybody remembers this example. Right? So this was where we were trying to predict the growth in terms of the rainfall. And there is no linear function that is describing the relationship between growth and rain in a kind of in a straight line. There is no line that you can do. But then we had a simple trick.
4287.84,4318.74,"what you could see, let's say, in the lecture last week, is where we simply say, okay, we also incorporate rain to the power 2. And if we kind of transform rain into three variables, let's say a constant, you can ignore this, but rain and rain to the power 2, then you could get an expression like this. So we have one input feature that is lifted."
4318.74,4345.14,"to be able to describe something like this. And if you understand this, then you also understand, okay, if we want to have a separating hyperplane that looks like this, we also need to lift a number of dimensions, just as we did here. So this is a kind of a visualization. Suppose that we have this problem. There is no separating hyperplane here."
4345.14,4370.77,"but we lift it from two to three dimensions, and if we are able to transform it in such a way, we now have a kind of linear separating hyperplane, separating the red dots from the blue dots. So that's what we would like to do. That's the idea. And here it is very problem-specific, like what kind of transformation that you need."
4370.77,4400.05,"That's the big advantage of neural networks that we will start talking about later this week. If you are using neural networks, to some degree, you do not need this domain knowledge. The neural network will somehow sort it out. Here, it's problem-specific. If something doesn't work, you have certain things that you can use to lift a number of dimensions. So this is the expression that we had earlier."
4400.11,4428.08,"with the soft margins, as you can see here. And we now simply do the same as in the rainfall example. We take our vector with the descriptive features and we lift the number of dimensions. So we transform this n-dimensional vector xi to a q-dimensional vector in such a way that q is bigger than n."
4428.37,4457.97,"And please think about the rainfall example. We translated rain, one variable rain, into two variables, rain and rain to the power 2. Something similar could be done here. We take one feature in Xi and we just look at that feature and the product of the feature. The squared feature or the root or whatever, or the sinus or whatever transformation you can think of."
4458.48,4489.81,"So if you look at this situation, so we have this using this expression here, which looks very strange for you now. We have x1 and x2, and we translate it into this. And by doing this, we get something that looks like this. Now I hope that you think, why is there this strange expression? It could in principle be anything."
4489.84,4519.98,"But we want to have expressions that we can compute very efficiently, right? And that's where you, perhaps, who has heard of the kernel trick? Does the name, so a few people raise their hand, so that's very good. So this is the kernel trick, where you lift a number of dimensions without actually doing the computation, right? That's the reason it's a trick."
4519.98,4545.52,kind of do not expect you to fully understand it's also not important but it's important for the motivation is that this problem can again be rewritten in another problem that you do not need to understand that is rewritten in such a way for efficiency reasons right so this problem is the same as this problem
4545.52,4573.62,"And you do not need to understand it, right? It would take multiple lectures to explain this, how this would exactly work. The only thing that you should know that this is possible, and that the thing that is time-consuming if you are computing this, is this multiplication of these two vectors. So this dot product is the part that is expensive, that takes time."
4573.78,4609.3,"So again, you do not need to understand how this works. But this problem that we have been talking about can be transformed into this problem. And then there is this part that is time-consuming. So we are taking the dot product of two vectors. So xi and xj are the input features. It's part of the input. So the problem is transformed into a problem that looks completely different."
4609.3,4637.65,"but is solving exactly the same problem. And you do not need to understand how this works. The only thing that you need to get from this is this is the part that is expensive to compute. Because you are looking at all instances, so all combined instances, and you take the dot product. So you need to do this many times."
4637.65,4666.99,"And that's in the goal function, so that's super expensive. So the goal is to not make this explode. And, of course, you can understand if this vector, if the vector initially has 50 dimensions, right, and you lift it to 1,000, that this becomes very, very expensive. Right? That's the thing. So we are not, so if we are,"
4667.25,4695.68,"So using this lifting the number of dimensions from n dimensions to Q, we need to compute this. So these were n-dimensional vectors where we compute the dot product. And we need to do it for every combination. This is lifted to Q. So if Q is much bigger than n, this will take a lot of time."
4695.68,4726.42,"So our goal is now to compute this expression efficiently. And that is where kernel functions come in. So a kernel function is a function that computes this value, the dot product of these two vectors that have been made bigger, in an efficient way. So the idea is that you can compute this"
4726.45,4756.45,"without actually computing it. That's the goal. And that is possible for specific functions. So here, like I showed you earlier, this picture of going from two to three dimensions to separate this line. So I was using this. So we go from two dimensions to three dimensions. And I can do that for my Xi."
4756.45,4785.74,"instance, and I can do that for my instance j. So this is still very simple. I go from two to three dimensions, not at all dramatic, but you should think of this at a much bigger scale. So now we are interested in computing this time this. So we need to take the dot product of this vector and this vector."
4786.42,4815.23,"And if we would not have, let's say, a shortcut, that would mean that we first need to lift this to this higher dimension, and then multiply it in this higher dimension. And this is something that we would like to avoid, and that is actually, for certain functions, this is actually possible. So what you see here is that I'm, like xi and xj are two original input vectors. I lift them."
4815.23,4846.29,"from n dimensions to q dimensions. In this example, I lift them from two dimensions to three dimensions. I take the dot product. So here in green you can see what originates from this. In blue you see what originates from this. So that looks like this. And if I look at this expression, I can rewrite it into this expression. And what you now hopefully see is that I can compute this k value"
4846.29,4876.22,"Without actually computing, let's say, these two other values, I can do it directly. And this is, as I said, a bit foolish because we are just looking at going from two to three dimensions, not dramatic at all. But if you look at higher dimensions, this can actually save a lot of time. So we are computing in a higher dimensional space without actually spending the effort. And doing it directly."
4876.22,4906.06,"in a direct way. So there are many functions that have these types of properties that you can do. So they typically have, let's say, for example, this form, where you multiply these two vectors plus c to the power d, where I do not describe what these two functions look like. But these are the kernels that you have, which you can compute directly."
4906.38,4932.69,"Which is very nice. And now I show some pictures for you to get an idea of what this means. So, for example, if I take this separating line, that can be achieved by using this kernel function. Jeff here. So I'm able to separate this. If I would... Oops."
4933.26,4961.92,"If I want this dimension to be equal to 1, I cannot separate it. I can have the original problem. If I make this parameter very high, I can get very strange lanes that may start overfitting the data that you have. So again, just like the C in the soft margin, you can play with these dimensions to decide."
4961.92,4992.45,"Let's say what works best for your example. And you typically train such a model on part of the data. And then in the end, there's part of the data that you do not use. And you test, let's say, different parameters of C and different parameters related to dimensions that you have here. Of course, there are still certain things that you cannot capture. So if you look at this, where you would like to have a circle, there is no polynomial kernel."
4992.45,5016.98,"So there's no kernel that looks like this, as you can see here, that would be able to capture this shape, right? But there are other, let's say, kernel functions that you can use that would capture such shapes. So if I would do this here, like, and I would try to do it with a polynomial kernel, I would get something like this. But I cannot get the circle, right?"
5017.87,5048.35,"But there are other kernels that would be able to do this and would exactly capture what you would like to have. Clear? Let me come to a conclusion. So I hope that you got an idea of SVMs, like how that worked. I know that I showed you a lot of formulas, but I hope that every individual step was kind of traceable, like how it worked from one step to another. And I think that's important if you apply these things."
5048.35,5078.27,"The key insight is that you are focusing not on the instances that are easy to classify, but you try to focus on instances that are difficult to classify. You put most of the energy in that. And what I also showed is one needs to be able to deal with outliers, and one needs to be able, sometimes things are not separable in, let's say, just two or three dimensions, and then you need to lift the number of dimensions."
5078.27,5103.82,"And if you think of, let's say, neural networks, neural networks also have multiple layers, and the goal of these multiple layers is kind of the same as these dimensions that I talk about here. But then instead of making a smart choice, I now use this polynomial kernel, the goal is that the neural network kind of sorts it out himself, right? That's the idea."
5103.82,5119.95,That's what I just said. Here you see the overview so later this week we will start with neural networks and the exercise this week is about SVMs. See you on Wednesday.
