start,end,text
4.4,27.81,"Okay, let's get started. So welcome to lecture number six. So today we start with neural networks and we will continue with that topic next week. Right, and the goal of today is to basically explain how a neural network works, what the basic elements are."
27.81,54.8,"And the lecture on Monday will in detail explain to you how a standard neural network is actually learning, right, how that works. And on Monday I will do exactly the same as I did for the support vector machines, that I walk step for step in such a way that you really hopefully understand the formulas, how it actually works, rather than looking at this from a very distance."
55.12,81.09,"So today will be a bit less formal, a bit less technical. The lecture on Monday will be more and more technical because I just introduced the concepts. And because I have a bit of time, there is also like another topic that is very compact to explain, that is Bayesian classification, right? And I will also explain that. So around, I don't know."
81.09,109.1,"One fifth or something like that will be devoted to this, but most of the time I will talk about neural networks. Let's take a step back. We have seen, let's say already, like in the last couple of lectures, we have seen various supervised learning methods. And the goal of a supervised learning method is that you are interested in a target feature, so you know what you are interested in. We will also have many lectures."
109.1,137.84,"devoted to unsupervised learning, where it's not so clear, where you do not have a target feature. But we've always been looking at problems, if you go to decision trees, if you go to regression, if you go to SVMs, where it is very clear what the target feature is. And there, what we will talk about later, it is crucial to decide how good these models are. And a central team,"
137.84,166.78,"that we do not talk about now, but that you will see later, is that you can only evaluate how good a supervised learning technique is if you apply it to unseen data. Because it's very easy to build a model that performs perfectly on the test data, but due to overfitting, would perform very badly, let's say, in the general setting. So as I said, let's take a brief step back and also reflect a bit on the things that we have seen."
166.78,195.98,"So it can be that your target feature is numerical. And then I can show a picture like this. So the goal is to learn a function where we start with an input feature. It can also be multiple. But here is just like an x-coordinate. And we would like to learn, let's say, what the corresponding y-feature is. So we look at input."
195.98,225.26,"And given the input, we try to predict what the output would be. So this is, let's say, finding out how this line is running. So we only have the green dots. And the idea is that we learn a function in such a way that if we see a new green dot, a new observation, that would be precisely on that line. Right? That's the way it works with the numerical target feature. And for example, when we talked about regression, this is what you saw."
225.49,253.81,"We also looked at, for example, in the last lecture we spoke about SVMs. SVMs try to find a separating plane. A separating plane between instances that are classified differently. So in these situations, our target feature is categorical. And we would like to predict for, let's say, unseen instances, given the X."
253.81,280.77,"which is now like a point in this two-dimensional plane, given an X1 and an X2, we would like to predict, is the result red or green? So in essence, the only thing that supervised learning is doing, based on examples, it tries to learn a function. And a neural network, that's why I'm explaining it in this way,"
280.77,311.17,"A neural network can be just seen as a function, right? There's nothing more, there's nothing less. So what have we seen? We have seen decision trees. A decision tree can also be seen as a function. Decision trees lend themselves very well for categorical variables, but we saw extensions towards numerical. Then we went to regression, and if you look at regression, first it is kind of tailored towards numerical data, as I just indicated."
311.17,338.67,"But we have seen, okay, we can also do logistic regression, and then we turn this machinery into something that works on categorical. SVMs can be seen as an extension of, let's say, logistic regression, and we only discussed it where the target feature was categorical. Actually, in my whole lecture that I talked about on Monday, we just assumed the binary."
338.67,365.66,"target feature, right? It was above the line or below the line, right? That was it. So, right, if we measure the error, right, we can look at the sum of square errors, we can look at the percentage of instances that we misclassified, but in the end, to really evaluate it, we always need test data, right? So we try to"
365.66,395.65,"to train it on training data, but we should make sure not to overfit, let's say, our model. So just some pictures of things that you've seen before, right? So this was logistic regression. So you're trying to separate the plusses from the triangles. Often it is not so simple as I'm showing you here, where it is able, where the separating plane is something that is very linear."
395.65,422.18,"We have seen also in regression that the function may look like this. And we also talked about, let's say, the kernel trick, where we lift a number of dimensions to be able to fit, let's say, shapes that look like this. So this is what has happened. And all of these things have in common if your separating plane is not linear."
422.18,449.06,"that you need to have lots of domain knowledge. So how are you kind of lifting this from a lower dimensional space to a higher dimensional space requires domain knowledge in order to be able to do this. So if I try to take a step back and going slowly towards, let's say, what the neural network is, I can basically summarize what we have seen so far."
449.06,476.59,"specifically a bit SVM-like and logistic regression-like. As we take an input vector x, we use this function to lift the number of dimensions. So this is, for example, if the separating plane would not be, let's say, linear, right? That it is something that is curved. Then we multiply this with a weight vector."
476.59,503.79,"So x is a vector, then we lift the number of dimensions, and then we have the j-th element that results from this. We multiply that with the weight j, then we take the sum, and then we apply a function to it. So this is like a kind of semi-generic representation of what a function could be, what is describing a class of functions."
503.79,533.57,"And it is mainly, let's say, characterized by this weight WJ. So that's what we have seen. And here I'm describing the things. So the letter M here is the number of descriptive features. You will see that the notation will be constantly different. So when I was using this book about SVMs, then a vector was indicated with an arrow above it, etc., etc. So there are constantly..."
533.57,561.78,"notational variations because of the different sources of information that we use, but it is also something that you have to see through, that you see this is basically all the same. So taking a step back, we have an input that is a bunch of instances, and you can think of an instance as a vector, a vector of fixed length. Why a vector of fixed length? Because"
561.78,591.74,"the number of features that we use as input is fixed. Then we have a model that is characterized by a bunch of weights. And we have very clearly seen that in logistic regression. We have very clearly seen that in SVMs. We have a bunch of weights. Also in the context of regression, it was exactly the same. So we have a bunch of weights. And we try to predict..."
591.74,620.53,a target feature and there are mechanisms that we are able based on our training data we feed it with inputs we have these weights initially these weights are completely random then we make a prediction and we can measure what the error is right so we have input data training data we apply a model with random weights we get a certain performance
620.59,647.17,"And now the key idea of, let's say, all the techniques that we have been talking about is that based on the specific mathematical choices that are being made, whenever we have an error, we know in which way we have to walk to reduce the error, right? And I think in logistic regression, that was the easiest, right? So you have an error function."
647.17,673.55,"And you just look at the derivative, and you know if your error is not at the lowest point, you know in which direction to walk to make it low. So we update the weight. We typically call it gradient descent. We know which direction to walk, and we continue going through the cycle until we reach a point where the derivative is zero, right? So that we have reached the minimum."
673.55,701.52,"So this can be visualized as this. So we start at a random point, and then we walk through this multidimensional space where we have all the different weights until we hit, let's say, a point where the derivatives are equal to zero. We can no longer reduce it further. Clear? That's the whole idea. And if we are now switching to neural networks, it's exactly the same."
701.52,731.1,"But you should see it's a paradigm that is in a way constant to do this. Of course, things like regression, etc. work very well for tabular data. It's very easy. These methods, of course, completely fail the moment that your input is much more complicated. And neural networks are particularly suitable in situations where the data is kind of unstructured."
731.1,761.25,"like text or images or sound, et cetera, et cetera. Right, so a neural network, given a bunch of training data, then it gets a new picture of our dog Miffy, and it is able to classify this as a dog successfully. Right, that's the idea. So a child can do that, and that is because the way that our brain works, and neural networks try to imitate what our brain is doing. Right, you train it with many examples,"
761.25,787.86,"and then you learn how this actually works. So if we look at SVMs, there we had this problem that I ended with the kernel trick, where with some voodoo I had images which you clearly could not separate, let's say for example in two dimensions."
787.86,815.06,"But if you then would lift it to three dimensions, you suddenly could separate it very clearly, right? So that was a kind of trick where I kind of know what kind of problems I'm facing, and therefore I'm using this function to lift the number of dimensions in such a way that it becomes separable, right? So we had things that were non-separable, and by doing this, lifting the number of dimensions, things become suddenly separable."
815.06,839.97,"But there I need to use domain knowledge, right, in order to be able to do that or try many different functions. And you can think of a neural network as doing this automatically, right? So you're basically creating multiple layers that serve a bit as what this function was doing, right? And that's the basic idea."
839.97,865.84,"So when we talked about SVMs and logistic regression, we had expressions that look like this. And now you can think of this expression here as just something that is repeating. And if I go to this one, I could go one level deeper, et cetera, et cetera. So we are basically concatenating multiple learnable functions after each other."
865.84,897.25,"And the trick is that we can still apply this gradient descent idea, right? That's the intuition that you should have. So, as I said, these standard techniques work well on tabular data, but if you go to images, et cetera, you need to have something like this. Just to make you realize that if you look at the dog picture, right, as a human you interpret this in a certain way, the neural network,"
897.25,927.74,"sees it like this, right? It just sees an image as a kind of vector representation, right? Also, if we would look at sound or text or whatever, it's turned into, let's say, numbers, right? And the neural network is dealing with these numbers. So just to indicate that a bit, so this is like a training data set where you have lots of different symbols."
927.74,958.78,"You as a human can see what it is. So for example, this is a 6. So this image that you see here is labeled as a 6. This image here is labeled like an 8. And for example, if I look at an example image 8, then it may, like if I describe all the pixels, then it looks something like this. So what is our input? This vector are our descriptive features, the sequence of numbers, and our target feature,"
958.78,985.39,"is the number eight, right? And that's what we would like to learn. So, and in a way, now we just want to learn a function that given such a representation is then able to return the number eight, right? And we train it on many examples and then hope that it works. And this can also be applied to sound and all kinds of other things that are like this."
985.39,1014.78,"So the way that it works, as I just said, a child can easily recognize what the dog is, etc. So a neural network is inspired by what happens in our brain. Although the similarities are there, but it's of course not working exactly the same. Sometimes I think that is a bit exaggerated, as I write here at the bottom. But that's the basic idea."
1014.78,1044.62,"So a brain has a large number of neurons that are connected, trained based on data. So to take a historical perspective, right, this is not a very new idea, right? This idea exists already for over 80 years. So this was probably the first paper describing stuff in these directions, which were describing these artificial neurons and how you could connect them to each other."
1045.07,1072.62,"So if you look at the field of AI, today, if we typically talk about AI, we are talking about this, let's say, biologically inspired form of learning. Deep neural networks, things like chat GPT, et cetera, et cetera, are very much, let's say, in this space. However, there is also this other, let's say, area which is more based on reasoning."
1072.62,1101.79,"So you can think of, for example, topics like AI planning, I don't know, using prolog or something like that, dealing with logical expressions, dealing with rules, et cetera, et cetera. That is not, let's say, bio-inspired. That is much, let's say, more rule-based. And this is sometimes also called good old-fashioned artificial intelligence, right? So there are these two camps. This was a very obscure area for a very..."
1101.79,1130.77,long time but in recent years these areas become much more visible and much more important than the other part and so for a for a long time this worked much better than this and there were also many AI winters right where there were like high expectations but since around 2010 suddenly this started to work much better than the thing at the top
1130.83,1155.5,"And why was the reason that it started to work better? It's mostly because the compute resources were there to kind of mimic what is happening in our brain, before the scale was too small. So this is showing the timeline. As I said, around 2010, these types of techniques started to outperform, let's say, more classical techniques."
1155.5,1185.04,"But there is this whole history of, let's say, all attempts to kind of build these things that today we would call a neural network. And I will revisit some of these things. This is now highly visible. So the Turing Award, the Nobel Prize for computer science, was handed out there. Also the Nobel Prize in physics was also attributed to basically neural networks."
1185.26,1210.62,"What is important to see is that this is not a single idea, but this is 80 years of history where lots and lots and lots of engineering are there, right? So it is impossible to explain to you how ChatGPT is working, right? One can give a basic idea how it is working, but there is so much engineering behind it that it is not a single concept. It's a combination of..."
1210.62,1238.08,"of let's say thousands of things, let's say put together, where images are treated completely different than text, time series are treated differently than, I don't know, events, etc. There are also quite some controversies, so when this, let's say last year, these two persons won the Nobel Prize in Physics, there was quite some opposition, because..."
1238.08,1267.39,like if you look at history there were many people that had very similar ideas let's say much earlier right so if you if you're interested in this like this is a kind of very critical uh let's say review of let's say many inventions that were made and that were then completely forgotten and then reinvented later with people that would take the fame rather than the people that would invent these things at the house so
1267.39,1292.02,"What are the advantages of this? We can model much more complicated functions. Just think of, I don't know, regression. You have a number of weights, and you can just describe a linear function. So that is very limited. With a neural network, you can describe functions that have, let's say, very flexible shapes. So it's generic."
1292.5,1321.18,"So with SVMs we need to really think how we are going to lift a number of dimensions in neural network. The hope is that the network takes care of this. It can deal with noisy situation. It can also deal with images, sound, text, video, etc. And of course training a neural network is very expensive. You need to have many examples. But once it is trained, the application is just applying a function."
1321.18,1350.93,"And it's also, if people talk about, let's say, foundation models, like they are used in LLMs, to train a foundation model takes a very long time. I think in my first lecture, I said that if you would use my laptop to create Gemini, you would need to have more than 600,000 years to do it. So it's very expensive, but the moment that you have it, to apply it is relatively cheap."
1350.93,1379.95,right that's the idea so these are the advantages the disadvantages is that you you typically need to have many training examples so this works very well uh that's also why chat gpt is is doing so well there is lots of text available on the internet right so there's lots of free training material available if you are dealing with the setting where you're not dealing with standard text or something like that
1379.95,1409.97,"something like event data in process mining, this data is not publicly available, right? It's the secret of companies that they do not want to share and it may be specific for a company, right? So then these things do not work so well because you do not have many training examples available. As I said, training may be time and resource consuming. It is like something happens and I will elaborate on this more and it is not really"
1409.97,1439.49,"You cannot really say why a certain result comes out. So if you look at the decision tree and the decision tree says this person will fail or this claim will be rejected, you can kind of logically walk through the decision tree and understand why that decision is being made. If it's a neural network and that gives a certain outcome, you have no idea what it is. The risk of overfitting. So you need to have many training examples to avoid overfitting."
1439.49,1468.37,"So what you can understand, if you have many possible parameters and just a few examples, there are infinitely many solutions of all of these parameters that would give exactly the same result on this test data. So that is clearly a problem. It performs typically worse. That's very surprising on very well-defined problems. And it can be hacked. I have some images to show this."
1468.69,1497.2,"So these images are a bit old, but exactly the same principles would apply to, let's say, newer techniques. Because there is a foundational problem here. So this image looks like a pig. I now add some random noise to it. You cannot see it in the picture. So for you, this looks exactly the same. And the classifier will say this is an airliner. So that is..."
1497.74,1525.04,"You as a human, it's completely clear, but the neural network is doing this. Here are some more examples. And as I said, some of these references are a bit old, but exactly the same applies to the newer techniques. You just need to search harder to find these things. So this is a zero. This is a five. You could still do something, but this I think is very convincing. This is three and this is eight."
1525.04,1554.43,"And just by making minimal changes to the image, the class changes from 3 to 8. Here the class ranges from, this is forbidden for trucks, to you have priority, right? And for you it looks exactly the same, and the same with the other examples. Right? So this is the danger of a neural network. It's very sophisticated that it can deal with this noisy..."
1554.43,1581.98,"that's a very unstructured data, but at the same time it can classify things wrong. I think this example shows it even better. So this was at the time the leading, let's say, technique to detect in images whether the traffic light is red or green. So in this data set, and you take the best model doing"
1581.98,1612.7,performing the best on this classification task so the traffic light is green or the traffic light is red if you look at the best classifier on average you only need to change three pixels to make it swap the other direction right so on the test set it always works very well but you take an image and on average you only need to change three pixels to make it swap right and I think that this shows
1612.7,1641.97,"Let's say there is. So here these are numbers and all of these images by really testing it on extreme examples. So all of these things will be classified as zero. All of these things will be classified as one based on the type of data that I showed you earlier. You can also manipulate sound. So you as a human, you hear exactly the same sentence."
1641.97,1669.47,"but an AI would say, okay, fire the nuclear missiles or something like that. That would be possible. This tries to visualize this, but this is very difficult to explain this because we are in a high-dimensional space. So if you have a neural network with a million parameters, you're looking at the space that has a million dimensions. And the idea is..."
1669.47,1697.17,"That if you have a million dimensions and you have something that is classified correctly, by moving in the right direction in this one million dimensional space, you're typically very close to a point where it changes to something different. I cannot show that in a million dimensional space, but here you see it in a two dimensional space. So all of these things are mapped onto an eight."
1697.71,1723.25,"But you can see that there are these values here where I only need to change a bit, and then it swaps to the other one. And that's the basic idea. So this 2 can be easily swapped into an 8. So that's, let's say, the disturbing part, and it's like a foundational problem."
1723.25,1750.96,"And so although people talk about guardrails, people talk about, I don't know, explainable AI, et cetera, et cetera, it will always have this risk, right? Because that's the cost of the flexibility that one has. One has no guarantees. Just like people can sometimes do strange things, right? It becomes more like a human. Very briefly to..."
1750.96,1777.76,"to kind of make a bit a link to how our brain is working. So our brain has, let's say, 100 billion neurons, depends on how much you drink, what remains, and these neurons are connected. And you can think of this as a network with weights between the different nodes, and by training you..."
1777.76,1805.62,"these weights are being changed. So this shows an image, and I do not want to talk about this in detail, but you have, let's say, these neurons. That's why we say a neural network. These have these connections, and there is like an electrical signal is being passed. At the same time, there are substances here."
1805.62,1835.22,"And when the substances come above a certain threshold, like it's a chemical process, if the substances come above a certain threshold, then the neuron will fire and it will send again a signal to the next one. So that's the intuitive idea that is behind this. So if we try to describe this a bit more mathematically, so you have these different neurons. These neurons can fire if..."
1835.7,1864.58,"Due to these electric signals, a chemical substance reaches a certain threshold, and then it fires. And this idea is that you have multiple inputs, let's say from other neurons, and then these things have weights, you add them up, that's like the chemical substance is increasing, I don't know, in volume or whatever, and then above a certain threshold it fires."
1864.58,1892.18,and it sends a particular output. So that's the idea and this kind of what I'm showing you here you will recognize later when I become more technical. So how does this now work? We first we create a network and in the network the only let's say things that we can change are the weights between the different neurons. That's the only thing that we do.
1892.18,1921.86,"And we use backpropagation to, let's say, learn these weights. This will be very much detailed in the lecture on Monday, but I say I will be much more technical. But the basic idea is that you have a bunch of weights, you have a certain output, and you know if your output is not, let's say, optimal, you know in which direction you need to modify the different weights in order to reduce the error."
1921.86,1952.67,"So you know how to update the weights. And that is, let's say, the gradient descent at a much larger scale than things that we have been talking about. So now you can see that the notation already becomes a bit more mathematical. So if you look at the cell, then there are these inputs that are being given. So x0, x1, x2, you can think of that as the output of a previous..."
1952.67,1984.02,"neuron, and this is multiplied with the weights, right? W0, W1, W2. These are summed up, and then a function is being applied to this sum, and that is again the output, right? And I hope that you recognize the things that we also talked about in regression, et cetera, et cetera, right? So this is the same process in the small."
1984.27,2010.9,"What will be confusing, right? And something that was also pointed out in the lecture on regression. Here I start counting with W0, W1, W2. Right? So you have these weights. But there is also this constant B. So here B is listed separately. Right? And then that..."
2010.9,2039.46,"Like in some books that is being done, that's also how you mostly saw it when we talked about regression. So here B is done separately. Also in SVMs, we always had like a constant factor there. And then you take, let's say, the weighted sum of all the other inputs. This is exactly the same as saying that B is equal to W0 times X0."
2039.46,2070.29,"where you make the assumption that this signal has an input of 1. So why am I elaborating on that? In different articles, in different books, you will see different notations. As I said, for SVMs, there was an explicit B. If we go to talking more formal about neural networks, there will not be a B. But there will be a W0 that we assume that this input, this X0, is a constant, is always 1. Yeah?"
2070.42,2098.66,"Just to avoid that you get confused, let's say, later. So we have inputs, we have weights, we have the somebody that is doing the summation, and then we have an activation function. So these are the different ingredients. So then the first attempt at building such a neural network, they would just use one layer, and that is called the perceptron."
2098.66,2127.5,"And the capability of the perceptron are very much similar to, let's say, SVMs and regression type of techniques. And what you will see is that this is not enough. So here we have just one cell. And this is already very powerful. Everything that we were talking about related to regression, et cetera, I can apply here. It's like this, very compact, but it is not enough. And I will show you."
2127.6,2155.07,"an example where this is the case. But before doing this, let's try this to make this a bit more operational. So we have this activation function. There are different inputs, one, two, and one. So as I said, we often assume that the first one has a weight of equal to one, right? So this times this is our B."
2155.07,2183.98,"value in the previous notation. So we just then multiply these things, and this is what comes out of this. So we have a certain output, for example 0.3. If we multiply, so if we take the weighted sum of these inputs, I get 0.3. And then we have this function f that needs to translate this 0.3 into an output."
2184.02,2213.46,"Just think about logistic regression, right? Where we are using the sigmoid function, and in the end we would like to translate things to either true or false. Let's say 1 or 0. And there we typically did, if it was below half, then it would be equal to 0, for example. But we can choose this activation function here. Here again is this comment."
2213.46,2243.95,"to make sure that you don't get confused there. I already mentioned the sigmoid function. So this function f that we saw, like in this lecture, we will typically assume that this function f is the sigmoid function, right? And why do we assume this? Because the sigmoid function has a bunch of very interesting mathematical properties. First of all, whatever the input is,"
2243.95,2272.58,"So I have an input that can range from minus infinity to plus infinity, so basically any number, and I always squeeze it into this interval between 0 and 1. So given a certain x value, it can be plus infinity or minus infinity, we squash it between 0 and 1. That's a nice property. Another very nice property is that if we look at the derivative,"
2272.58,2301.78,"of the sigmoid function, the derivative of this is, let's say, sigmoid x times 1 minus sigmoid x. You have no clue why this is now relevant for you, but if I show you the formulas on Monday, you will realize that we are using this, right? And again, we want to do gradient descent, so if we want to do gradient descent, it is very helpful that our derivative looks very nice."
2301.84,2329.23,"is the same as that we often take the error to the power 2. Why do we do that? Because then the derivative, if you have x to the power 2, the derivative, better I should say better, if I take half times x to the power 2, and I take the derivative of this, it's simply x, right? And that's something that we can easily compute with. So this is what we will actually use."
2329.3,2358.1,"But to do, let's say, pen and paper examples, we typically assume a simpler function, in such a way that we do not have to compute with, let's say, very complicated numbers, where we simply say, if it's below zero, we say zero. If it's above zero, we just say one. So we make it like a function that is easy with pen and paper. Let's now try to implement some logical formulas."
2358.35,2384.82,"So if we look at the OR, this is the truth table where 0 is false and 1 is true. If we look at this true table, then false or false is false. False or true is true. True or false is true. True or true is equal to true. So this is the truth table that you all know."
2384.82,2415.87,"And here what I do is that I create a small, let's say, perceptron. So a very simple neural network that is able to compute this. And I'm using these weights. And I'm using, let's say, this function that I explained on the previous slide to make it very easy. Right? So, and again, for this tiny example, it is evil to write down. So we are now looking at a function that has this shape."
2415.87,2445.23,"So we take the weighted input and note that we are making this assumption W0 times 1 is equal to the B that we had in other expressions. So there is always a 1 here. It doesn't come from the previous layer or it doesn't come from the input. It's fixed. But we have X1 and X2 as output. So that leads to this formula. And F has this shape. So with these weights,"
2445.23,2474.83,"we get this expression. So x1 plus x2 minus half gets a sum. And if the sum is negative, we say false. If the sum is positive, we say it's true. And like if I now draw this line in this two-dimensional place, we are basically saying for these three points, we say true. And for this point, we say false. So we have implemented an R."
2474.83,2501.39,"in terms of a neural network. Yeah, clear? That's the basic idea. So this is an or, and now I'll try to, so these are the types of questions that we could also ask, let's say, at a written exam. Now implement, let's say, a particular logical formula, right? Because this is the or. If you look at the end, it is this, right? We just need to move that line."
2501.62,2532.35,"So now this was minus half before, and this is, let's say, now minus one and a half. So they both need to be one in order to return a value above zero. Note that there are, of course, many different lines that would have the desired outcome for these, let's say, four training instances. And it also shows the flexibility, but also the danger."
2532.35,2562.38,"So we have seen R, we have seen AND, and now the question is, can we implement XR? And now we have the following problem. So line 1 was the line that we saw for the R. Line 2 was the line that we saw for the AND. But now we have the complication that we want these points to be equal to 1."
2562.38,2592.1,"And we want this point and this point to be equal to zero. And now, think of all the kind of linear machinery that we have explained before. This is impossible. I cannot draw a separating plane in this two-dimensional space that would have this property. So if we would now go to SVMs or whatever, we would need to lift the number of dimensions. We need to now add something that these two points, I don't know, go up and the other points go down."
2592.1,2623.49,"or something like that. So you can prove that a simple neural network, a perceptron, cannot implement an XR. And it shows immediately the limitations of things. And I just wrote down the formulas. And so we know that if X1 is false, X2 is false, then the output should also be false. So we know that this weighted sum,"
2623.49,2652.11,"should be smaller than 0 if x1 is 0 and x2 is 0. Also, to take a look at 1, if, I don't know, x1 is 0 and x2 is equal to 1, right, so we are looking at the second row, then the output should be 1, so you should have a value of at least 0. Do we all see that these equations should hold, right?"
2652.66,2680.83,"This was under the condition x1 is equal to 0 and x2 is equal to 0. So if I look at this first line, you can basically remove these terms. So I know that w0 is smaller than 1. I can also fill in these values and I know that w0 plus w1 plus w2 should be smaller than 0. So I can simply fill out."
2680.83,2709.36,"the X1 and X2 values, and then I get this. Right, again, I just look at these expressions and I just fill in the X1 and the X2 values in every row, and then this is what I get. And now you can very easily see that this leads to a contradiction, so you can add up the first two expressions, and you can add up the second two in equations, and"
2709.36,2739.22,"Of course, the sum of two negative values is negative. The sum of two positive values is positive. And now you can see that there is a conflict. So this is a mathematical proof showing that the perceptron cannot implement something as simple as an XR. And it shows why in a neural network we need to have multiple layers. And it also shows immediately where, I don't know, SVMs and logistic regression, et cetera, et cetera, would fail. Without lifting the number of dimensions."
2740.21,2770.1,"So the conclusion of this is we need to have more layers in order to support this. And in a minute I will show you, let's say, a two-layer neural network that is able to implement this XR function. So this is like if we zoom out, because of course an XR is very similar, so in all the situations where there is not like a separating, let's say, linear hyperplane,"
2770.1,2798.24,"Like in all of these situations, we can kind of address this problem by adding more layers, right? That we can fit shapes that you could otherwise not fit. There are many, let's say, neural network structures that you talk about. I will only talk about feed-forward networks, right? Because for that I will explain things in detail. There are lots of, let's say, special neural networks."
2798.24,2827.98,"dedicated for specific tasks. So the two ones that are best known are, for example, convolutional neural networks to deal with images, right? You can imagine images are very special, right? Because you have pixels that are in an X and Y direction. You can talk about how close two pixels are to each other, et cetera, et cetera. So you can exploit these properties and use special network architectures for that."
2827.98,2855.63,"Also think of a picture of a dog, and you would rotate the picture of a dog a bit, like all the pixels will have different values, right? And you want it to be robust to things like rotation, etc. And that's what these more complicated architectures do. Another, let's say, very well-known architecture is, let's say, called LSTM, so long, short-term memory."
2855.73,2885.44,"And the reason behind this is that everything that we have looked at so far, the input size of our data is fixed. Right? We have a number of descriptive features, and we try to predict the target feature. But the number of columns is fixed. Of course, this immediately fails the moment that we are dealing with sequences. Because if we are dealing with sequences, then it is not fixed."
2885.44,2914.18,"You can start with the short sequence, and the sequence can grow, let's say, infinitely long. And also, the order matters. If I'm training a decision tree, if I would swap the position of the columns, it would have no effect on the result. But in sequences, things orderly matter. This is where these LSTM..."
2914.18,2942.66,"networks play a role, and they basically have a memory that, let's say, you remember what has happened in the past, right? And if you then go to the attention mechanism that we'll talk about in one of the later lectures, this is even more sophisticated, right? But it tries to capture, let's say, what is important of the things that have happened before. As I said, forget about all of this. It's complicated enough as it is."
2942.66,2970.64,"We just look at feed-forward network that looks like this, right? And all the connections, they go from one layer to the next layer, from one layer to the next layer, etc. And if we train it, we measure the error here, and then we propagate backwards this error, and we know how to update the weights. So that's the basic architecture that we will focus on."
2970.77,2999.63,"And we choose the activation functions in such a way that we can actually backpropagate, let's say, the error. That we know what we should change. So this would be a one-layer network. We can implement an R. We can implement an AND. This is a two-layer network which allows us to implement an XR. Something that was impossible by doing this."
3000.56,3030.51,"So this is a repetition just to keep things together. So this was the single layer R. We've seen that before. So the weights are minus half 1,1, what we have seen before. This was the single layer end. Also something that we have seen before. I'm just repeating things. You can also implement a knot. That's the one that is being shown here. So like this is half."
3031.02,3058.59,"minus x1. So if you take half minus x1, we get a negation, right? Because if it's positive, it becomes negative. If it was negative, it will become positive, right? So again, like here, you see the full expression, what is being represented here. So minus x1 plus half is the sum, and then we apply this function."
3058.59,3084.62,"So if x1 is equal to, let's say, 0, it remains positive. If it's equal to 1, it becomes negative. So we swap the sign. So if you look at logic, like if you have a bunch of these basic operators, you can combine them to have arbitrarily complicated logical expressions."
3084.62,3115.57,"With these building blocks, we can build more complicated functions, including the XR. So what was the XR? So it has these values in the middle that are 1, but if everything is true or everything is false, we should return false. And this is the network. So you would like to build a network that looks like this. It's now a two-layer neural network."
3115.57,3143.22,"Where, like, this is what comes out of this neuron, Y1. This is the expression what comes out of Y2. And this is the expression what comes out at the end. And our activation function is the same in all the cases. So now we have, how many are there? We have nine weights."
3143.54,3172.42,"And now we need to find the weights in such a way that this truth table holds. Right? And again, there are many possibilities to do so. So here you see, let's say, a possible, let's say, assignment of the different weights in such a way that we implemented this formula. Let me try to explain. So if we have here minus half..."
3172.42,3201.49,"and here we have 1, and here we have 1, then I hope that you remember this was the inclusive OR. We have seen that before. If we have 1, 1, and we have this constant 1 times minus 1 and a half, this was the end. So we have seen these two neurons before. And now we use this one, that looks a bit, let's say, different of what we had before, but still something that we can express."
3201.49,3230.9,"So this y1 corresponds to x1 or x2. This y2 corresponds to x1 and x2. And now we take this should hold and not that should hold. And if you do that, then you get the result. So please apply the table on the previous slide. And you will see that this actually gives you the right result."
3231.28,3260.91,"So for example, try to fill in zero here and zero here, and then you'll see that zero comes out at the end. If we insert one here and one here, then also zero will come out, right? Because of the knot that is there. Yeah, so this is the general idea. So we will often look at pictures that look like this, which show you the different layers."
3261.1,3288.53,"that are connected, let's say, in this feed-forward sense. And like we often number, let's say, the weights within what layer they are and from which neuron in the previous layer, they go to the neuron in the next layer. So that's the way that looks. If we just look at two layers, we again can write this down as a formula."
3288.53,3316.43,"What I'm showing you here, this network can be written down as a formula that looks like this. We first go through the first layer and we compute these things and the output of what happens in the first layer is passed on to the second layer, multiplied by these weights and in the end we get this output."
3316.5,3345.62,"Like here, the number of output neurons is the k value between 1 and n. And we can see that the number of inputs is equal to d and the number of nodes in the hidden layer is equal to m. Right? And like if you compare this picture with the previous picture, that should become clear. And so here you can see that next to each other, just let's say for your convenience. So this k."
3345.62,3375.6,"is referring to which of the output neurons we look at. Then we have these different activation functions, so I'm assuming that here an activation function H is being used, here an activation function F is being used. It could also be all F, right? Just to make you point out what the references are. So each of these nodes that is labeled here with an H,"
3375.6,3403.26,"named after the activation function, we take the weighted sum of the inputs. And these are the inputs going from I to J, right? So this is the weighted input of a neuron J in this hidden layer. And to this weighted sum, so assume that this would be neuron J, right? Then we take the weighted sum, we apply F."
3403.26,3434.45,"And then this is kind of broadcasted to all the neurons in the next layer, multiplied by this weight. So all these values are corresponding to the weights that we have here. And again here, for every node, we look at the weighted sum of these different inputs and then apply the activation function to it. And I show it now for two layers. So we have just one hidden layer."
3434.45,3471.25,"But I could, I don't know, also extend this that we would have 10 or 100 layers. Is this clear? Right? So this is, yeah. So I'm using here this, let's say, superscript to indicate this. And I'm using this to number things in the different layer. But of course, like, the number one appears here, but the number one appears also here. Right?"
3471.25,3505.71,"to see how you need to interpret this depends on which connecting layer you are. Right? So if I look here, for example at this one, this one is referring to this one, and this one, the second one here, is referring to this one. That's the first in that layer. Because if I would give them all unique names, then it would be highly confusing. So this one is, this is the first one, this is the first one in the hidden layer."
3505.78,3543.68,"And this is the first one in the output layer. Because if I would give them all different names, it would be highly confusing. So here, if we have here M neurons, and here we have N neurons, we get M times N, sorry, M times N, a different weight. And that's what you see here. We see all the combinations of all numbers."
3543.68,3576.42,"They are completely random, right? And you will often see that. Of course, you can make a sophisticated choice, but in principle, it's okay to make everything zero. Or everything one, or everything 100, right? That would not matter. Sorry. So, I'm using, I'm reserving here the zero to refer to this special thing that has a constant input one. So, every layer..."
3576.42,3606.77,"also has a 0, 1, 0, 2, until 0, m, where there is this constant. So this is this b value that you saw earlier. Right, so each neuron has a constant associated with it, and the constant is fully defined by this weight factor. And every neuron that you see here has such a constant."
3607.38,3647.17,"So you could think of this as b1, b2 until bm. No, no. What is random are the weights. What x1, x2 and xd is our training data that goes in there. So x1, x2 and xd until xd are not random. We feed our training examples in there. But the weights can be chosen random."
3647.17,3680.46,"We just randomize all the different weights. Then we see what happens, and we measure a certain error, and we know how to improve it. That's the idea. There is no superscript zero, because in a way, this is the input. It's a bit confusing, but also some people would say this is, let's say, a one-layer neural network. Other people would say it's a two-layer network."
3680.46,3703.62,"And it depends on whether you count the output layer as a layer, right? The same you could argue, yeah, is this now an input layer or is it not a layer? This is just the input. There are no neurons here. There are only neurons here and neurons here, right? Just for conventions, right?"
3703.62,3734.53,"It is good to have this question so that you let it sink in, because on Monday you will often look at pictures like this, and then it's important that you have an intuitive understanding how that works. So we often use as a shorthand, it's not so relevant now, where we kind of, what is indicated here in red, we call the ZJ. So this is after we have applied the activation function, and this is again the input for the next layer."
3734.53,3762.77,"That's where we often use this set function. Let me now explain the basic idea of backpropagation. As I said, I will provide you very detailed with the formulas on Monday how this actually works. This is our neural network. It is fully defined by all the weights that you see."
3763.02,3792.75,"And we have an error function, and the error function looks exactly the same as what you have seen before. And this is done for technical reasons. So we take the error squared, and we do this in such a way that the derivative looks very nice. That's the idea. And now when we look at that error, we try to minimize it."
3792.75,3820.46,"We start by just randomizing the initial weights. We just take random numbers. And if we then apply this to a training example, so for a training example, we have concrete values of x1 until xd. So these are the columns in our table. And for one row, we simply insert the values. And then the neural network, based on these random weights,"
3820.46,3851.09,"is giving these values, right? So this is what the neural network is producing based on completely randomly chosen weights. And we know that corresponding to this input, we want the target feature to say that it is class two, right? So I assume that this is a classification problem. It would not matter if, like here, my variable is categorical."
3851.09,3880.24,"So all of the output cells should return a value 0 except one that has a value 1. This is a classification problem. So this is what it computes and this is what ideally it would compute. Ideally, based on this input, it should say it belongs to class 2. And now we can simply, let's say, look at the difference between the predicted value and the..."
3880.34,3910.8,"the real target feature that would like to come out, and then we see all of these errors, and we see the error is 16.32. It doesn't say anything intuitively, right? But because of gradient descent, based on, let's say, this backpropagation idea, we now know how we should change the weights in such a way that this error goes down."
3910.8,3939.02,"initially our weights were completely random, and now we know how to update the weights in such a way that it gets better, right? Because we know the derivative of the error function. And by doing this, we for example, as I now explain it very simplistically, in reality all the weights are updated, but assume that this weight is increased, that by increasing this weight we now get this."
3939.18,3966.34,"and we know how to nudge these weights in such a way that the error goes down. And then I apply it again, and then we have this error function. And now, if the gradient CSAN says, okay, we can still reduce that error, we again update the weights, and we again update the weights, et cetera, et cetera, until we reach a minimum. Yeah? That's the whole idea. I'm showing you that for one instance."
3966.34,3996.08,"But you can imagine that you should sum that over all the instances that are in your input set. And then you also realize why this is a very costly process, training these weights. Is that clear? As I said, on Monday I will explain this with formulas, etc., etc., but it's important that the idea sinks in. And we have this idea of, let's say, the moment that we know in which way we should walk to reduce the error."
3996.08,4026.06,"then we can, let's say, solve this problem. And then you can apply this to image data, and then if it's a classification problem, something like this would come out, and you would choose the majority class, and the neural network trains on lots of pictures, of dog and cat pictures, which of this picture say, this is a dog. Although it has never seen this picture before. That's the ultimate goal. In later lectures,"
4026.06,4052.27,uh also when we go into the direction of text mining and transformers we use a very interesting concept and that is the concept of an auto encoder right and the problem is that i hope that that you can get an idea if you look at an image you're looking at a high dimensional space right also if i would like to to encode words right
4052.27,4077.98,"So I think in the English dictionary there are 800,000 words or something like that. So if you think of this as a vector, one-hot encoding of all the words in a dictionary, I have a vector of dimension 800,000. That's never going to work. I hope that everybody has an intuition that that doesn't work. So you want to reduce the number of dimensions because..."
4077.98,4101.39,"If you look at the one hot encoded vector, you're not actually using, let's say, the data space, right? One is one and all the others are zero, right? So that's where an autoencoder comes in, and I will explain you, let's say, later how this works. So you have this, let's say, high dimensional data."
4101.46,4132.26,"and you reduce it to something low-dimensional with the goal of producing exactly the same output. And how to train such a neural network? You make sure that this layer in the middle is very small, and you just train it. I feed it with this picture, and I want it to reproduce the picture. And this could also be the picture of a dog or whatever. So I take this high-dimensional dog picture,"
4132.26,4156.37,"to simply reproduce the dark picture, squeezing it through a keyhole in such a way that we get a very compact representation. And this is, let's say, essential that we do this. But when we talk about tax mining, we will talk about this in detail. These are the formulas that I will explain to you on Monday."
4156.37,4184.72,"I will show you how to derive them, and these are the things that you should also be able to, when you get the pen and paper exercise, how you should apply that, so you understand how this works. To conclude, as I said, I just talked about it in a rather informal way today. On Monday, I will go in more details, just focusing on this backpropagation problem."
4184.72,4213.6,"That leaves me with some space to talk about another, let's say, supervised learning technique that is naive Bayesian classification. And this is a smaller topic, so that kind of still fits into this lecture. I hope that everybody has heard about Bayes' theorem before, that you probably see in your basic statistics course. And Bayes' theorem can also be used..."
4213.6,4242.1,"when you are dealing with data science classification, et cetera, et cetera. And I will explain to you how this works. So the basic idea of Bayesian statistics is that you have like a prior distribution, your beliefs about the world, then you look at particular evidence, and then you update your beliefs. Right? That's the basic idea. And in order to explain this, I need to repeat some basic statistics."
4242.1,4267.47,Just for you to get into it again. So this is the probability that X holds. This is the probability that X and Y holds. This is the conditional probability. So the probability that X holds under the condition that Y holds. There is this product rule that always holds. So the probability that X and Y hold.
4267.86,4296.59,"is always equal to the probability that X holds under the condition that Y holds, times the probability that Y holds. So this is always true. And I can repeatedly apply this rule. That's something that we will often use. And so here I, for example, show it for three elements. So this probability is equal to the probability that X holds under the condition that Y holds."
4296.59,4322.56,"Y and Z hold times the probability that Y and Z holds. Right? I'm just applying this rule where Y is equal to Y plus Z. Y and Z in this example. Now I can, let's say, look at this expression. And I can again apply this rule. And then I get this. Right? And this is the chain rule. So if I have, let's say, P."
4322.56,4350.58,"A and B, etc., etc., until Z holds is equal to this probability. So you can basically unfold things. It's something that we will use multiple times, including now a bit, but we will return to that later. Then again, some basic things. So the probability that X does not hold is 1 minus the probability that X holds. The probability that..."
4350.58,4382.29,X does not hold under the condition that Y holds is of course this. And there is this rule that the probability that Y holds is equal to the probability that Y holds under the condition X times the probability X plus this expression. I hope that you all have seen this before and I'm just showing you this to refresh.
4382.29,4408.94,"post, right? All the things that I wrote here and also rules like the ones that you see here, they always hold. The thing that does not always hold is this expression that the probability that A and B, et cetera, until Z holds is equal to the probability that A times the probability that B, et cetera, et cetera. So this only holds if"
4408.94,4435.09,"these different random variables are actually independent of each other. So if we assume independence, it holds. In general, it does not hold. Clear? I hope I'm surprising nobody. Now, I can just use the things that I have. So we have seen that this is equal to that, and this is equal to that. I just swap the role of x and y."
4435.18,4465.18,"And now I can rewrite this. So I take this, I move this to the other side, and then we get this expression. It's super simple, right? So the guy became world famous by it, but it's not very complicated. So the probability that X holds under the condition that Y holds is equal to the probability that Y holds under the condition that X holds times the probability that X holds."
4465.18,4493.81,"divided by the probability that Y holds. So why is this used? What you should note is that we are swapping here the position of X and Y, right? And that's the key idea, right? And this is, again, an expression that always holds, right? That if you have these probabilities, it is destined to hold. So let me show you a small example."
4494.8,4523.41,"So let's look at the random variable X is that somebody has a disease, I don't know, COVID or whatever, and you do a test, and the test can be positive or negative. So random variable X is, so if we talk about the probability of X, what is the probability that somebody has COVID? 5%. This is the probability, okay, if somebody has COVID,"
4523.63,4554.24,"and we apply the test, what is the probability that the test is positive? It's 99%. So in 1 out of 100 cases, the test will give the wrong result, if you have COVID. If you do not have COVID, so not X means that you do not have the disease, if you do not have COVID, the probability that the test is positive is 1%. So in both, if you have COVID,"
4554.24,4584.24,"The test has an error rate of 1%. And if you do not have COVID, right, you can get a false positive in one out of 100 cases, right? So suppose that you are provided with this data. Can you now compute the probability? So now you did the test. The test is positive. What is the probability that you have COVID, right?"
4584.24,4613.89,"If you would not be using the Bayesian statistics, you would simply say, okay, the test was positive, so I have COVID, right? But intuitively, you may also say, okay, like I look at these percentages that I have here, like if the test is positive, I don't know, it's extremely likely that I have COVID, right? But let's compute this, and we can compute this."
4613.89,4643.38,"in an exact way using Bayesian statistics. So again, these are the numbers that were on the previous slide. So we first compute what is the probability that the test is positive. And we simply do that by, okay, we have the cases where people have COVID and the cases where people do not have COVID. And we know what the percentages are. Right?"
4643.38,4673.71,"If people have COVID, then in 99% of the cases the test is positive. If people do not have COVID, in 1% of the cases the test is positive. So the probability that the test is positive is equal to this. Right? It's something that you can simply derive. And now I have all the expressions that we had before. And we find that the probability that if..."
4673.71,4703.82,"the test is positive, you have COVID, is 0.83. And it's not 0.99 or something like that. And these are, if you assume that these probabilities that I gave you as input are correct, this is the probability. You do not need to make additional assumptions for this. So this is a way that we are basically given with something where we have"
4703.86,4734.58,"y conditional on x, that we swap it into something where it is the other way around. This is something that we can use when we deal with classification. So how do we apply this to classification? And later I will explain you why this is important. So t is again referring to our target feature, and q1 until qm are corresponding to the input features."
4734.58,4764.24,"I'm sorry for the different notation. I don't know any book, any article will have different notation, so I just want you to be used to this. So this is the probability that the target feature is equal to L under the condition that the descriptive features have these values. And that's exactly what we are interested in. So given this input vector, given these descriptive features, we want to predict the target feature."
4764.24,4795.66,"So this is the probability. And I can rewrite this probability using Bayes' rule in something like this. So what you can again see is that we are swapping, let's say here, these two sides. And these are the probabilities that we have. So let me go back one here. So we can compute, given some input, we can compute the probability."
4795.66,4824.67,"that the output is equal to L. So if we are looking at classification, so we have different possible target features, it could be true or false, it could be red or green, and we can compute the probabilities of these target features given this particular input. If we are able to do that, our response is that we would like to return as a prediction, as a function that we learn,"
4824.67,4852.03,"the one with the highest value. So we are interested in the value of L. And again, L could be true or false. It could be red or green, whatever. We are interested in the value L that maximizes this probability. And if I have a classification where, for example, I have five different values, then there could be a value 0.3. It's less than half."
4852.03,4882.19,But still it's the one that has the highest probability of all the possible values. Right? That's still the case. So we are interested in the L that maximizes this probability. Then I can replace this using Bayes' theorem. Let's say with this expression. And I hope that you see that I divide that by this expression. But that expression doesn't matter. Because it's constant. Right?
4882.19,4911.49,"These input things, they are the same, right? Because that's part of our input. So we can simply remove this line here. So maximizing, finding the L where this expression is maximal is the same as this one where we simply remove the one at the bottom. Do you see that? We want to maximize something. And if we have a term that is divided by a constant, we can also leave out that constant."
4911.49,4942.98,"It is not needed. So this is, let's say, the best prediction that we can make given the data. So that is the case. And note that I have swapped, let's say, these columns. And why is this extremely relevant? And if I show you an example, that will become clearer."
4942.98,4970.67,"Typically our data is very sparse, right? So there may be many different input vectors and not all combinations of input vectors will happen together, right? There may be many combinations that we did not see. Or there may be just one, right? And then based on this one of our whole data set, we look at this one row and that row would determine what we predict."
4970.83,5000.75,"That doesn't sound like a good idea, right? If we have lots of data, it's the best idea. But if our data is very sparse, we just have examples, it's a very bad idea. So what we can do is we can assume that things are independent. And if we assume independence, then this probability here can be rewritten of, let's say, the product of all of these probabilities. And then we get this one."
5000.75,5030.9,"So now, assuming independence, we get the product of all of these conditional probabilities where things have been reversed. And I could only do this because I applied Bayes' rule before. So these values that you see here are input values. They have been moved to the front. And they were first at the back."
5030.9,5060.83,"Then we get this expression, assuming independence, this is the correct expression, where we get the best classifier, and then we can apply it. And just as a motivating example, this is taken from the book. So here we have credit history. This can be grantor, co-applicant of none. This can be accommodation, can be owned or rent."
5060.83,5089.78,"let's say 4 times 3 times 3, why do I have this other one? Plus the 2 for fraud, we have 72 combinations. So if you look at this table, there are 70 possible combinations of values. And if we have a table with just 20, then by definition, like we could never cover all possibilities."
5089.94,5120.67,"Even if the result would be completely deterministic, we would not be able to cover this, right? Because we'd still have half of 72. So, for example, if I look at row, sorry, I now have a data point for which I would like to make a prediction, and credit history is equal to paid, the second value is equal to none, and the third value is equal to rent, I can look into the table,"
5120.67,5151.18,"but I've never seen any instance that was the combination of paid, non, and rent. And if we have many different features, then that is very common. Most of the combinations we will not see, because there will be an exponential blow-up of all things. So we would still like to make a prediction. And there is, by assuming this independence thing,"
5151.18,5181.65,"we can make, let's say, a prediction for this value. So here I write down all the, let's say, probabilities, and you can simply do that by computing the fraction. So there are 20 rows, and for example, 6 of the rows, if I go back, so in 6 of the rows, fraud is equal to true. In 14 of the 20 rows, there is no fraud."
5181.65,5212.8,"So that leads to the probability of fraud is equal to 0.3. The probability of not fraud is equal to 0.7. I can also compute the conditional probabilities. And for most of the combinations, I have values to compute it based on. So for example, if I look at, I don't know, this column being none, so of the six rows,"
5212.8,5243.22,"There are rows that are like, I think it's one, right? There was one row where given fraud, that was none. So that is how we get this probability. Yeah? So although many combinations did not appear, we can still compute these more local probabilities here. Do you see this? Right? So this is just looking at the rows that are fraud and then looking at the percentage that has, let's say, did this particular target feature."
5243.57,5272.43,"So now we would like to predict for this kind of combination that we have never seen before, is it fraudulent or not? And we want to come up with the value, true or false, right? So we are forced to make a choice. And we pick the one with the maximal, let's say, value using, let's say, Bayesian classification. So here I repeat all the different numbers."
5273.26,5301.39,"And now I apply, let's say, this rule that I explained before. So we have the possibility now to say it is fraudulent or it is not fraudulent. And we can compute this product that you see here. I'm just multiplying these four numbers here. And then we get this value. I multiply these four numbers here. And then we get this value."
5302.0,5332.42,"And now our prediction is to take the one with the highest value, which is clearly this row. So although we have never seen this combination of values, we are still able to make a prediction. Right? And the prediction would be fraudulent. And this is based on the assumption of independence of features. Because if we would not assume independence, like the best that we could say is that we do not know because we have never seen it."
5332.42,5359.31,And we still would like to make a prediction. So assuming independence is better than having no data. That's a basic idea. Is this clear how this works? Note that these values are difficult to interpret because we are looking at the maximum value and I removed this fraction where we are dividing by because that's a constant and it doesn't matter.
5359.31,5389.23,"We have now seen many different types of, let's say, supervised learning. We will continue with neural networks on, let me just show you the schedule. So we are now here. So the instruction tomorrow will be devoted to, let's say, SVMs, that you have hands-on examples here. Then on Monday, there will be the lecture on neural networks, and you will train with that afterwards. See you on Monday."
